<html><body><div><div class="content html_format"><p>
      Привет, хабр! </p>

<img src="https://habrastorage.org/files/e2c/a40/b3f/e2ca40b3ffa74c5d93db82520f225fc5.png"/>
<p>
Меня зовут </p><a href="http://mlclass.ru">Александр</a><p>, я занимаюсь машинным обучением и анализом веб-графов (</p><a href="https://events.yandex.ru/lib/talks/1919/">в основном — теоретическим</a><p>), а также разработкой Big Data продуктов в одном из операторов Большой Тройки. Это мой первый пост — прошу, не судите строго!) 
</p><p>
В последнее время ко мне все чаще стали обращаться люди, которые хотят научиться разрабатывать эффективные алгоритмы и участвовать в соревнованиях по машинному обучению с вопросом: «С чего начать?». Некоторое время назад я руководил разработкой инструментов Big Data для анализа медиа и социальных сетей в </p><a href="http://www.ac.gov.ru">одном из учреждений Правительства РФ</a><p>, и у меня остался некоторый материал, по которому обучалась моя команда и которым можно поделиться. Предполагается, что у читателя есть хорошее знание математики и машинного обучения (в команде были в основном выпускники МФТИ и студенты </p><a href="http://shad.yandex.ru/">Школы Анализа Данных</a><p>).
</p><a name="habracut"/><p>
По-сути это было введение в </p><b>Data Science</b><p>. В последнее время эта наука стала довольно популярна. Все чаще проводятся соревнования по машинному обучению (например, </p><a href="http://www.kaggle.com/">Kaggle</a><p>, </p><a href="http://tunedit.org">TudedIT</a><p>), зачастую с немалым бюджетом. Целью данной статьи является дать читателю быстрое введение инструменты машинного обучения, чтобы он мог как можно скорее участвовать в соревнованиях.
</p><p>
Наиболее распространенными инструментами Data Scientist'а на сегодняшний день являются </p><b>R</b><p> и </p><b>Python</b><p>. У каждого инструмента есть свои плюсы и минусы, однако, в последнее время по всем параметрам выигрывает Python (это исключительно мнение автора, к тому же пользующегося одновременно и тем и другим). Это стало после того, как появилась отлично документированная библиотека </p><b>Scikit-Learn</b><p>, в которой реализовано большое количество алгоритмов машинного обучения.
</p><p>
Сразу отметим, что в статье мы остановимся именно на алгоритмах Machine Learning. Первичный анализ данных лучше обычно проводится средствами пакета </p><b>Pandas</b><p>, разобраться с которым можно самостоятельно. Итак, сосредоточимся на реализации, для определенности полагая, что на входе у нас есть матрица обьект-признак, хранящаяюся в файле с расширением </p><b>*.csv</b>

<h3><b>Загрузка данных</b></h3><p>
В первую очередь данные необходимо загрузить в оперативную память, чтобы мы имели возможность работать с ними. Сама библиотека Scikit-Learn использует в своей реализации NumPy массивы, поэтому будем загружать *.csv файлы средствами NumPy. Загрузим один из датасетов из репозитория </p><a href="http://archive.ics.uci.edu/ml/">UCI Machine Learning Repository</a><p>:

</p><pre><code class="python">import numpy as np
import urllib
# url with dataset
url = "http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data"
# download the file
raw_data = urllib.urlopen(url)
# load the CSV file as a numpy matrix
dataset = np.loadtxt(raw_data, delimiter=",")
# separate the data from the target attributes
X = dataset[:,0:7]
y = dataset[:,8]
</code></pre><p>
Далее во всех примерах будем работать с этим набором данных, а именно с матрицей обьект-признак </p><b>X</b><p> и значениями целевой переменной </p><b>y</b><p>.

</p><h3><b>Нормализация данных</b></h3><p>
Всем хорошо знакомо, что большинство градиентных методов (на которых по-сути и основаны почти все алгоритмы машинного обучения) сильно чуствительны к шкалированию данных. Поэтому перед запуском алгоритмов чаще всего делается либо </p><b>нормализация</b><p>, либо так называемая </p><b>стандартизация</b><p>. Нормализация предполагает замену номинальных признаков так, чтобы каждый из них лежал в диапазоне от 0 до 1. Стандартизация же подразумевает такую предобработку данных, после которой каждый признак имеет среднее 0 и дисперсию 1. В Scikit-Learn уже есть готовые для этого функции:

</p><pre><code class="python">from sklearn import preprocessing
# normalize the data attributes
normalized_X = preprocessing.normalize(X)
# standardize the data attributes
standardized_X = preprocessing.scale(X)
</code></pre>

<h3><b>Отбор признаков</b></h3><p>
Не секрет, что зачастую самым важным при решении задачи является умение правильно отобрать и даже создать признаки. В англоязычной литературе это называется </p><b>Feature Selection</b><p> и </p><b>Feature Engineering</b><p>. В то время как Future Engineering довольно творческий процесс и полагается больше на интуицию и экспертные знания, для Feature Selection есть уже большое количество готовых алгоритмов. «Древесные» алгоритмы допускают расчета информативности признаков:

</p><pre><code class="python">from sklearn import metrics
from sklearn.ensemble import ExtraTreesClassifier
model = ExtraTreesClassifier()
model.fit(X, y)
# display the relative importance of each attribute
print(model.feature_importances_)
</code></pre><p>
Все остальные методы так или иначе основаны на эффективном переборе подмножеств признаков с целью найти наилучшее подмножество, на которых построенная модель дает наилучшее качество. Одним из таких алгоритмов перебора является Recursive Feature Elimination алгоритм, который также доступен в библиотеке Scikit-Learn:

</p><pre><code class="python">from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
# create the RFE model and select 3 attributes
rfe = RFE(model, 3)
rfe = rfe.fit(X, y)
# summarize the selection of the attributes
print(rfe.support_)
print(rfe.ranking_)
</code></pre>

<h3><b>Построение алгоритма</b></h3><p>
Как уже было отмечено, в Scikit-Learn реализованы все основные алгоритмы машинного обучения. Рассмотрим некоторые из них.

</p><h4><b>Логистическая регрессия</b></h4><p>
Чаще всего используется для решения задач классификации (бинарной), но допускается и многоклассовая классификация (так называемый one-vs-all метод). Достоинством этого алгоритма являеся то, что на выходе для каждого обьекта мы имеем вероятсность принадлежности классу

</p><pre><code class="python">from sklearn import metrics
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X, y)
print(model)
# make predictions
expected = y
predicted = model.predict(X)
# summarize the fit of the model
print(metrics.classification_report(expected, predicted))
print(metrics.confusion_matrix(expected, predicted))
</code></pre>

<h4><b>Наивный Байес</b></h4><p>
Также является одним из самых известных алгоритмов машинного обучения, основной задачей которого является восстановление плотностей распределения данных обучающей выборки. Зачастую этот метод дает хорошее качество в задачах именно многоклассовой классификации. 

</p><pre><code class="python">from sklearn import metrics
from sklearn.naive_bayes import GaussianNB
model = GaussianNB()
model.fit(X, y)
print(model)
# make predictions
expected = y
predicted = model.predict(X)
# summarize the fit of the model
print(metrics.classification_report(expected, predicted))
print(metrics.confusion_matrix(expected, predicted))
</code></pre>

<h4><b>K-ближайших соседей</b></h4><p>
Метод </p><b>kNN (k-Nearest Neighbors)</b><p> часто используется как составная часть более сложного алгоритма классификации. Например, его оценку можно использовать как признак для обьекта. А иногда, простой kNN на хорошо подобранных признаках дает отличное качество. При грамотной настройке параметров (в основном — метрики) алгоритм дает зачастую хорошее качество в задачах регрессии 

</p><pre><code class="python">from sklearn import metrics
from sklearn.neighbors import KNeighborsClassifier
# fit a k-nearest neighbor model to the data
model = KNeighborsClassifier()
model.fit(X, y)
print(model)
# make predictions
expected = y
predicted = model.predict(X)
# summarize the fit of the model
print(metrics.classification_report(expected, predicted))
print(metrics.confusion_matrix(expected, predicted))
</code></pre>

<h4><b>Деревья решений</b></h4>
<b>Classification and Regression Trees (CART)</b><p> часто используются в задачах, в которых обьекты имеют категориальные признаки и используется для задач регресии и классификации. Очень хорошо деревья подходят для многоклассовой классификации

</p><pre><code class="python">from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier
# fit a CART model to the data
model = DecisionTreeClassifier()
model.fit(X, y)
print(model)
# make predictions
expected = y
predicted = model.predict(X)
# summarize the fit of the model
print(metrics.classification_report(expected, predicted))
print(metrics.confusion_matrix(expected, predicted))
</code></pre>

<h4><b>Метод опорных векторов</b></h4>
<b>SVM (Support Vector Machines)</b><p> является одним из самых известных алгоритмов машинного обучения, применяемых в основном для задачи классификации. Также как и логистическая регрессия, SVM допускает многоклассовую классификацию методом one-vs-all.

</p><pre><code class="python">from sklearn import metrics
from sklearn.svm import SVC
# fit a SVM model to the data
model = SVC()
model.fit(X, y)
print(model)
# make predictions
expected = y
predicted = model.predict(X)
# summarize the fit of the model
print(metrics.classification_report(expected, predicted))
print(metrics.confusion_matrix(expected, predicted))
</code></pre><p>
Помимо алгоритмов классификации и регрессии, в Scikit-Learn имеется огромное количество более сложных алгоритмов, в том числе кластеризации, а также реализованные техники построения композиций алгоритмов, в том числе </p><b>Bagging</b><p> и </p><b>Boosting</b><p>.

</p><h4><b>Оптимизация параметров алгоритма</b></h4><p>
Одним из самых сложных этапов в построении действительно эффективных алгоритмов является выбор правильных параметров. Обычно, это делается легче с опытом, но так или иначе приходится делать перебор. К счастью, в Scikit-Learn уже есть немало реализованных для этого функций
</p><p>
Для примера посмотрим на подбор параметра регуляризации, в котором мы по очереди перебирают несколько значений:

</p><pre><code class="python">import numpy as np
from sklearn.linear_model import Ridge
from sklearn.grid_search import GridSearchCV
# prepare a range of alpha values to test
alphas = np.array([1,0.1,0.01,0.001,0.0001,0])
# create and fit a ridge regression model, testing each alpha
model = Ridge()
grid = GridSearchCV(estimator=model, param_grid=dict(alpha=alphas))
grid.fit(X, y)
print(grid)
# summarize the results of the grid search
print(grid.best_score_)
print(grid.best_estimator_.alpha)
</code></pre><p>
Иногда более эффективным оказывается много раз выбрать случайно параметр из данного отрезка, померить качество алгоритма при данном параметре и выбрать тем самым луйший:

</p><pre><code class="python">import numpy as np
from scipy.stats import uniform as sp_rand
from sklearn.linear_model import Ridge
from sklearn.grid_search import RandomizedSearchCV
# prepare a uniform distribution to sample for the alpha parameter
param_grid = {'alpha': sp_rand()}
# create and fit a ridge regression model, testing random alpha values
model = Ridge()
rsearch = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100)
rsearch.fit(X, y)
print(rsearch)
# summarize the results of the random parameter search
print(rsearch.best_score_)
print(rsearch.best_estimator_.alpha)
</code></pre><p>
Мы рассмотрели весь процесс работы с библиотекой Scikit-Learn за исключением вывода результатов обратно в файл, что предлагается сделать читателю в качестве упражнения, потому как одним из достоинств Python (и самой библиотеки Scikit-Learn) по-сравнению с R является отличная документация. В следующих частях мы рассмотрим подробно каждый из разделов, в частности, затронем такую важную вещь как </p><b>Feauture Engineering</b><p>.
</p><p>
Я очень надеюсь, что данный материал поможет начинающим Data Scientist'ам как можно скорее приступить к решению задач машинного обучения на практике. В заключение хочу пожелать успехов и терпения тем, кто только начинает участвовать в соревнованиях по машинному обучению!
      </p><p class="clear"/>
    </div>

    
  </div></body></html>