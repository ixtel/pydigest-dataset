<html><body><div><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-sketch-rnn" class="anchor" href="#sketch-rnn" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>sketch-rnn</h1>

<p>Implementation multi-layer recurrent neural network (RNN, LSTM GRU) used to model and generate sketches stored in .svg vector graphic files.  The methodology used is to combine Mixture Density Networks with a RNN, along with modelling dynamic end-of-stroke and end-of-content probabilities learned from a large corpus of similar .svg files, to generate drawings that is simlar to the vector training data.</p>

<p>See my blog post at <a href="http://blog.otoro.net/2015/12/28/recurrent-net-dreams-up-fake-chinese-characters-in-vector-format-with-tensorflow/">blog.otoro.net</a> for a detailed description on applying <code>sketch-rnn</code>  to learn to generate fake Chinese characters in vector format.</p>

<p>Example Training Sketches (20 randomly chosen out of 11000 <a href="http://kanjivg.tagaini.net/">KanjiVG</a> dataset):</p>

<p><a href="https://camo.githubusercontent.com/3c5e3c5941d02a01338d22d763790c953dfa5fba/68747470733a2f2f63646e2e7261776769742e636f6d2f686172646d6172752f736b657463682d726e6e2f6d61737465722f6578616d706c652f747261696e696e672e737667" target="_blank"><img src="https://camo.githubusercontent.com/3c5e3c5941d02a01338d22d763790c953dfa5fba/68747470733a2f2f63646e2e7261776769742e636f6d2f686172646d6172752f736b657463682d726e6e2f6d61737465722f6578616d706c652f747261696e696e672e737667" alt="Example Training Sketches" data-canonical-src="https://cdn.rawgit.com/hardmaru/sketch-rnn/master/example/training.svg"/></a></p>

<p>Generated Sketches (Temperature = 0.1):</p>

<p><a href="https://camo.githubusercontent.com/1a03fcfdb74d1eb0ce229e1f3888a182c5714265/68747470733a2f2f63646e2e7261776769742e636f6d2f686172646d6172752f736b657463682d726e6e2f6d61737465722f6578616d706c652f6f75747075742e737667" target="_blank"><img src="https://camo.githubusercontent.com/1a03fcfdb74d1eb0ce229e1f3888a182c5714265/68747470733a2f2f63646e2e7261776769742e636f6d2f686172646d6172752f736b657463682d726e6e2f6d61737465722f6578616d706c652f6f75747075742e737667" alt="Generated Sketches" data-canonical-src="https://cdn.rawgit.com/hardmaru/sketch-rnn/master/example/output.svg"/></a></p>

<h1><a id="user-content-basic-usage" class="anchor" href="#basic-usage" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Basic Usage</h1>

<p>I tested the implementation on TensorFlow 0.50.  I also used the following libraries to help:</p>

<pre><code>svgwrite
IPython.display.SVG
IPython.display.display
xml.etree.ElementTree
argparse
cPickle
svg.path
</code></pre>

<h2><a id="user-content-loading-in-training-data" class="anchor" href="#loading-in-training-data" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Loading in Training Data</h2>

<p>The training data is located inside the <code>data</code> subdirectory.  In this repo, I've included <code>kanji.cpkl</code> which is a preprocessed array of KanjiVG characters.</p>

<p>To add a new set of training data, for example, from the <a href="http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/">TU Berlin Sketch Database</a>, you have to create a subdirectory, say <code>tuberlin</code> inside the <code>data</code> directory, and in additionl create a directory of the same name in the <code>save</code> directory.  So you end up with <code>data/tuberlin/</code> and <code>save/tuberlin</code>, where <code>tuberlin</code> is defined as a name field for flags in the training and sample programs later on.  <code>save/tuberlin</code> will contain the check-pointed trained models later on.</p>

<p>Now, put a large collection .svg files into <code>data/tuberlin/</code>.  You can even create subdirectories within <code>data/tuberlin/</code> and it will work, as the <code>SketchLoader</code> class will scan the entire subdirectory tree.</p>

<p>Currently, <code>sketch-rnn</code> only processes <code>path</code> elements inside svg files, and within the <code>path</code> elements, it only cares about lines and belzier curves at the moment.  I found this sufficient to handle TUBerlin and KanjiVG databases, although it wouldn't be difficult to extent to process the other curve elements, even shape elements in the future.</p>

<p>You can use <code>utils.py</code> to play out some random training data after the svg files have been copied in:</p>

<pre><code>%run -i utils.py
loader = SketchLoader(data_filename = 'tuberlin')
draw_stroke_color(random.choice(loader.raw_data))
</code></pre>

<p><a href="https://camo.githubusercontent.com/74be630e67274f8b13217aca0a2e63bd77b1a547/68747470733a2f2f63646e2e7261776769742e636f6d2f686172646d6172752f736b657463682d726e6e2f6d61737465722f6578616d706c652f656c657068616e742e737667" target="_blank"><img src="https://camo.githubusercontent.com/74be630e67274f8b13217aca0a2e63bd77b1a547/68747470733a2f2f63646e2e7261776769742e636f6d2f686172646d6172752f736b657463682d726e6e2f6d61737465722f6578616d706c652f656c657068616e742e737667" alt="Example Elephant from TU Berlin database" data-canonical-src="https://cdn.rawgit.com/hardmaru/sketch-rnn/master/example/elephant.svg"/></a></p>

<p>For this algorithm to work, I recommend the data be similar in size, and similar in style / content.  For examples if we have bananas, buildings, elephants, rockets, insects of varying shapes and sizes, it would most likely just produce gibberish.</p>

<h2><a id="user-content-training-the-model" class="anchor" href="#training-the-model" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Training the Model</h2>

<p>After the data is loaded, let's continue with the 'tuberlin' example, you can run <code>python train.py --dataset_name tuberlin</code></p>

<p>A number of flags can be set for training if you wish to experiment with the parameters.  You probably want to change these around, especially the scaling factors to better suit the sizes of your .svg data.</p>

<p>The default values are in <code>train.py</code></p>

<pre><code>--rnn_size RNN_SIZE             size of RNN hidden state
--num_layers NUM_LAYERS         number of layers in the RNN
--model MODEL                   rnn, gru, or lstm
--batch_size BATCH_SIZE         minibatch size
--seq_length SEQ_LENGTH         RNN sequence length
--num_epochs NUM_EPOCHS         number of epochs
--save_every SAVE_EVERY         save frequency
--grad_clip GRAD_CLIP           clip gradients at this value
--learning_rate LEARNING_RATE   learning rate
--decay_rate DECAY_RATE         decay rate after each epoch (adam is used)
--num_mixture NUM_MIXTURE       number of gaussian mixtures
--data_scale DATA_SCALE         factor to scale raw data down by
--keep_prob KEEP_PROB           dropout keep probability
--stroke_importance_factor F    gradient boosting of sketch-finish event
--dataset_name DATASET_NAME     name of directory containing training data
</code></pre>

<h2><a id="user-content-sampling-a-sketch" class="anchor" href="#sampling-a-sketch" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Sampling a Sketch</h2>

<p>I've included a pretrained model in <code>/save</code> so it should work out of the box.  Running <code>python sample.py --filename output --num_picture 10 --dataset_name kanji</code> will generate an .svg file containing 10 fake Kanji characters using the pretrained model.  Please run <code>python sample.py --help</code> to examine extra flags, to see how to change things like number of sketches per row, etc.</p>

<p>It should be straight forward to examine <code>sample.py</code> to be able to generate sketches interactively using an IPython prompt rather than in the command line.  Running <code>%run -i sample.py</code> in an IPython interactive session would generate sketches shown in the IPython interface as well as generating an .svg output.</p>

<h2><a id="user-content-more-useful-links-pointers-datasets" class="anchor" href="#more-useful-links-pointers-datasets" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>More useful links, pointers, datasets</h2>

<ul>
<li><p>Alex Graves' <a href="http://arxiv.org/abs/1308.0850">paper</a> on text sequence and handwriting generation.</p></li>
<li><p>Karpathy's <a href="https://github.com/karpathy/char-rnn">char-rnn</a> tool, motivation for creating sketch-rnn.</p></li>
<li><p><a href="http://kanjivg.tagaini.net/">KanjiVG</a>.  Fantastic Database of Kanji Stroke Order.</p></li>
<li><p>Very clean TensorFlow implementation of <a href="https://github.com/sherjilozair/char-rnn-tensorflow">char-rnn</a>, written by <a href="https://github.com/sherjilozair">Sherjil Ozair</a>, where I based the skeleton of this code off of.</p></li>
<li><p><a href="https://pypi.python.org/pypi/svg.path">svg.path</a>.  I used this well written tool to help convert path data into line data.</p></li>
<li><p>CASIA Online and Offline Chinese <a href="http://www.nlpr.ia.ac.cn/databases/handwriting/Download.html">Handwriting Databases</a>.  Download stroke data for written cursive Simplifed Chinese.</p></li>
<li><p>How Do Humans Sketch Objects?  <a href="http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/">TU Berlin Sketch Database</a>.  Would be interesting to extend this work and generate random vector art of real life stuff.</p></li>
<li><p>Doraemon in <a href="http://yylam.blogspot.hk/2012/04/doraemon-in-svg-format-doraemonsvg.html">SVG format</a>.</p></li>
<li><p><a href="https://en.wikipedia.org/wiki/Potrace">Potrace</a>.  Beautiful looking tool to convert raster bitmapped drawings into SVG for potentially scaling up resolution of drawings.  Could potentially apply this to generate large amounts of training data.</p></li>
<li><p><a href="http://rosettacode.org/wiki/Bitmap/B%C3%A9zier_curves/Cubic">Rendering Belzier Curve Codes</a>.  I used this very useful code to convert Belzier curves into line segments.</p></li>
</ul>

<h1><a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>License</h1>

<p>MIT</p>
</article>
  </div></body></html>