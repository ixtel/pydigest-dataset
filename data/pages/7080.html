<html><body><div><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-parmesan" class="anchor" href="#parmesan" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Parmesan</h1>
<p>Parmesan is a library adding variational and semi-supervised neural network models to the neural network library <a href="http://github.com/Lasagne/Lasagne">Lasagne</a>.</p>
<a name="user-content-installation"/>
<h2><a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Installation</h2>
<p>Parmesan depends heavily on the <a href="http://github.com/Lasagne/Lasagne">Lasagne</a> and
<a href="http://deeplearning.net/software/theano">Theano</a> libraries. Please make sure you have these installed before installing Parmesan.</p>
<p><strong>Install Parmesan</strong></p>
<div class="highlight highlight-source-shell"><pre>git clone https://github.com/casperkaae/parmesan.git
<span class="pl-c1">cd</span> parmesan
python setup.py develop</pre></div>
<a name="user-content-documentation"/>
<h2><a id="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Documentation</h2>
<p>Work in progress. At the moment Parmesan primarily includes</p>
<ul>
<li>Layers for Monte Carlo approximation of integrals used in (importance weighted) variational autoencoders in <em>parmesan/layers/sample.py</em></li>
<li>Layers for constructing Ladder Networks in <em>parmesan/layers/ladderlayers.py</em></li>
<li>Layers for implementing normalizing flows in <em>parmesan/layers/flow.py</em></li>
</ul>
<p>Please see the source code and code examples for further details.</p>
<a name="user-content-examples"/>
<h2><a id="user-content-examples" class="anchor" href="#examples" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Examples</h2>
<ul>
<li><strong>examples/vae_vanilla.py</strong>: Variational autoencoder as described in Kingma et al. 2013</li>
<li><strong>examples/iw_vae.py</strong>: Variational autoencoder using importance sampling as described in Burda et al. 2015</li>
<li><strong>examples/iw_vae_normflow.py</strong>: Variational autoencoder using normalizing flows and importance sampling as described in Burda et al. 2015 and Rezende et al. 2015</li>
<li><strong>examples/mnist_ladder.py</strong>: Semi-supervised Ladder Network as described in Rasmus et al. 2015</li>
</ul>
<p><strong>Usage example</strong>:
Below is an image of the log-likelihood terms training an importance weighted autoencoder on MNIST using binomial sampling of the inputs before each epoch. Further we found it beneficial to add batch normalization to the fully connected layers. The training is done using one Monte Carlo sample to approximate the expectations over q(z|x) and one importance weighted sample.
The test performance was evaluated using 5000 importance weighted samples and be should be directly comparable to the results in Burda et al.
The final test performance is LL=-84.78 which is better than the current best published results at LL=-86.76 reported in Burda et al. table 1 (compare to top 1st row and 4th row in column labeled IWAE since we are training using a single importance weighted sample)).</p>
<a href="https://raw.githubusercontent.com/casperkaae/parmesan/master/misc/eval_L5000.png" target="_blank"><img alt="https://raw.githubusercontent.com/casperkaae/parmesan/master/misc/eval_L5000.png" src="https://raw.githubusercontent.com/casperkaae/parmesan/master/misc/eval_L5000.png"/></a>
<p>Similar results should be obtained by running</p>
<div class="highlight highlight-source-shell"><pre>python examples/iw_vae.py -eq_samples 1 -iw_samples 1 -lr 0.001 -nhidden 500 -nlatent 100 -nonlin_dec very_leaky_rectify -nonlin_enc rectify -batch_size 250 -anneal_lr_epoch 2000</pre></div>
<a name="user-content-development"/>
<h2><a id="user-content-development" class="anchor" href="#development" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Development</h2>
<p>Parmesan is work in progress, inputs, contributions and bug reports are very welcome.</p>
<dl>
<dt>The library is developed by</dt>
<dd><ul>
<li>Casper Kaae Sønderby</li>
<li>Søren Kaae Sønderby</li>
<li>Lars Maaløe</li>
</ul>
</dd>
</dl>
<a name="user-content-references"/>
<h2><a id="user-content-references" class="anchor" href="#references" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>References</h2>
<ul>
<li>Kingma, D. P., &amp; Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.</li>
<li>Burda, Y., Grosse, R., &amp; Salakhutdinov, R. (2015). Importance Weighted Autoencoders. arXiv preprint arXiv:1509.00519.</li>
<li>Rezende, D. J., &amp; Mohamed, S. (2015). Variational Inference with Normalizing Flows. arXiv preprint arXiv:1505.05770.</li>
<li>Rasmus, A., Valpola, H., Honkala, M., Berglund, M., &amp; Raiko, T. (2015). Semi-Supervised Learning with Ladder Network. arXiv preprint arXiv:1507.02672.</li>
</ul>

</article>
  </div></body></html>