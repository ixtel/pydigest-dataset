<html><body><div><div class="span8">
    
<p><em>This work is supported by <a href="http://continuum.io">Continuum Analytics</a>
and the <a href="http://www.darpa.mil/program/XDATA">XDATA Program</a>
as part of the <a href="http://blaze.pydata.org">Blaze Project</a></em></p>

<p>In this post we use Pandas in parallel across an HDFS cluster to read CSV data.
We coordinate these computations with dask.dataframe.  A screencast version of
this blogpost is available <a href="https://www.youtube.com/watch?v=LioaeHsZDBQ">here</a>
and the previous post in this series is available
<a href="http://matthewrocklin.com/blog/work/2016/02/17/dask-distributed-part1">here</a>.</p>

<p>To start, we connect to our scheduler, import the <code class="highlighter-rouge">hdfs</code> module from the
<code class="highlighter-rouge">distributed</code> library, and read our CSV data from HDFS.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">distributed</span> <span class="kn">import</span> <span class="n">Executor</span><span class="p">,</span> <span class="n">hdfs</span><span class="p">,</span> <span class="n">progress</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">e</span> <span class="o">=</span> <span class="n">Executor</span><span class="p">(</span><span class="s">'127.0.0.1:8786'</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">e</span>
<span class="o">&lt;</span><span class="n">Executor</span><span class="p">:</span> <span class="n">scheduler</span><span class="o">=</span><span class="mf">127.0</span><span class="o">.</span><span class="mf">0.1</span><span class="p">:</span><span class="mi">8786</span> <span class="n">workers</span><span class="o">=</span><span class="mi">64</span> <span class="n">threads</span><span class="o">=</span><span class="mi">64</span><span class="o">&gt;</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">nyc2014</span> <span class="o">=</span> <span class="n">hdfs</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'/nyctaxi/2014/*.csv'</span><span class="p">,</span>
<span class="o">...</span>               <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="s">'pickup_datetime'</span><span class="p">,</span> <span class="s">'dropoff_datetime'</span><span class="p">],</span>
<span class="o">...</span>               <span class="n">skipinitialspace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">nyc2015</span> <span class="o">=</span> <span class="n">hdfs</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'/nyctaxi/2015/*.csv'</span><span class="p">,</span>
<span class="o">...</span>               <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="s">'tpep_pickup_datetime'</span><span class="p">,</span> <span class="s">'tpep_dropoff_datetime'</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">nyc2014</span><span class="p">,</span> <span class="n">nyc2015</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">persist</span><span class="p">([</span><span class="n">nyc2014</span><span class="p">,</span> <span class="n">nyc2015</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">progress</span><span class="p">(</span><span class="n">nyc2014</span><span class="p">,</span> <span class="n">nyc2015</span><span class="p">)</span>
</code></pre>
</div>

<p><img src="http://mrocklin.github.com/blog/images/distributed-hdfs-read-csv.gif"/></p>

<p>Our data comes from the New York City Taxi and Limousine Commission which
publishes <a href="http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml">all yellow cab taxi rides in
NYC</a> for various
years.  This is a nice model dataset for computational tabular data because
it’s large enough to be annoying while also deep enough to be broadly
appealing.  Each year is about 25GB on disk and about 60GB in memory as a
Pandas DataFrame.</p>

<p>HDFS breaks up our CSV files into 128MB chunks on various hard drives spread
throughout the cluster.  The dask.distributed workers each read the chunks of
bytes local to them and call the <code class="highlighter-rouge">pandas.read_csv</code> function on these bytes,
producing 391 separate Pandas DataFrame objects spread throughout the memory of
our eight worker nodes.  The returned objects, <code class="highlighter-rouge">nyc2014</code> and <code class="highlighter-rouge">nyc2015</code>, are
<a href="http://dask.pydata.org/en/latest/dataframe.html">dask.dataframe</a> objects which
present a subset of the Pandas API to the user, but farm out all of the work to
the many Pandas dataframes they control across the network.</p>

<h2 id="play-with-distributed-data">Play with Distributed Data</h2>

<p>If we wait for the data to load fully into memory then we can perform
pandas-style analysis at interactive speeds.</p>



<table border="1" class="dataframe">
  <thead>
    <tr>
      <th/>
      <th>VendorID</th>
      <th>tpep_pickup_datetime</th>
      <th>tpep_dropoff_datetime</th>
      <th>passenger_count</th>
      <th>trip_distance</th>
      <th>pickup_longitude</th>
      <th>pickup_latitude</th>
      <th>RateCodeID</th>
      <th>store_and_fwd_flag</th>
      <th>dropoff_longitude</th>
      <th>dropoff_latitude</th>
      <th>payment_type</th>
      <th>fare_amount</th>
      <th>extra</th>
      <th>mta_tax</th>
      <th>tip_amount</th>
      <th>tolls_amount</th>
      <th>improvement_surcharge</th>
      <th>total_amount</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2</td>
      <td>2015-01-15 19:05:39</td>
      <td>2015-01-15 19:23:42</td>
      <td>1</td>
      <td>1.59</td>
      <td>-73.993896</td>
      <td>40.750111</td>
      <td>1</td>
      <td>N</td>
      <td>-73.974785</td>
      <td>40.750618</td>
      <td>1</td>
      <td>12.0</td>
      <td>1.0</td>
      <td>0.5</td>
      <td>3.25</td>
      <td>0</td>
      <td>0.3</td>
      <td>17.05</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>2015-01-10 20:33:38</td>
      <td>2015-01-10 20:53:28</td>
      <td>1</td>
      <td>3.30</td>
      <td>-74.001648</td>
      <td>40.724243</td>
      <td>1</td>
      <td>N</td>
      <td>-73.994415</td>
      <td>40.759109</td>
      <td>1</td>
      <td>14.5</td>
      <td>0.5</td>
      <td>0.5</td>
      <td>2.00</td>
      <td>0</td>
      <td>0.3</td>
      <td>17.80</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>2015-01-10 20:33:38</td>
      <td>2015-01-10 20:43:41</td>
      <td>1</td>
      <td>1.80</td>
      <td>-73.963341</td>
      <td>40.802788</td>
      <td>1</td>
      <td>N</td>
      <td>-73.951820</td>
      <td>40.824413</td>
      <td>2</td>
      <td>9.5</td>
      <td>0.5</td>
      <td>0.5</td>
      <td>0.00</td>
      <td>0</td>
      <td>0.3</td>
      <td>10.80</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>2015-01-10 20:33:39</td>
      <td>2015-01-10 20:35:31</td>
      <td>1</td>
      <td>0.50</td>
      <td>-74.009087</td>
      <td>40.713818</td>
      <td>1</td>
      <td>N</td>
      <td>-74.004326</td>
      <td>40.719986</td>
      <td>2</td>
      <td>3.5</td>
      <td>0.5</td>
      <td>0.5</td>
      <td>0.00</td>
      <td>0</td>
      <td>0.3</td>
      <td>4.80</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>2015-01-10 20:33:39</td>
      <td>2015-01-10 20:52:58</td>
      <td>1</td>
      <td>3.00</td>
      <td>-73.971176</td>
      <td>40.762428</td>
      <td>1</td>
      <td>N</td>
      <td>-74.004181</td>
      <td>40.742653</td>
      <td>2</td>
      <td>15.0</td>
      <td>0.5</td>
      <td>0.5</td>
      <td>0.00</td>
      <td>0</td>
      <td>0.3</td>
      <td>16.30</td>
    </tr>
  </tbody>
</table>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">nyc2014</span><span class="p">)</span>
<span class="mi">165114373</span>

<span class="o">&gt;&gt;&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">nyc2015</span><span class="p">)</span>
<span class="mi">146112989</span>
</code></pre>
</div>

<p>Interestingly it appears that the NYC cab industry has contracted a bit in the
last year.  There are <em>fewer</em> cab rides in 2015 than in 2014.</p>

<p>When we ask for something like the length of the full dask.dataframe we
actually ask for the length of all of the hundreds of Pandas dataframes and
then sum them up.  This process of reaching out to all of the workers completes
in around 200-300 ms, which is generally fast enough to feel snappy in an
interactive session.</p>

<p>The dask.dataframe API looks just like the Pandas API, except that we call
<code class="highlighter-rouge">.compute()</code> when we want an actual result.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">nyc2014</span><span class="o">.</span><span class="n">passenger_count</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="mf">279997507.0</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">nyc2015</span><span class="o">.</span><span class="n">passenger_count</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="mi">245566747</span>
</code></pre>
</div>

<p>Dask.dataframes build a plan to get your result and the distributed scheduler
coordinates that plan on all of the little Pandas dataframes on the workers
that make up our dataset.</p>



<p>Let’s appreciate for a moment all the work we didn’t have to do around CSV
handling because Pandas magically handled it for us.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">nyc2015</span><span class="o">.</span><span class="n">dtypes</span>
<span class="n">VendorID</span>                          <span class="n">int64</span>
<span class="n">tpep_pickup_datetime</span>     <span class="n">datetime64</span><span class="p">[</span><span class="n">ns</span><span class="p">]</span>
<span class="n">tpep_dropoff_datetime</span>    <span class="n">datetime64</span><span class="p">[</span><span class="n">ns</span><span class="p">]</span>
<span class="n">passenger_count</span>                   <span class="n">int64</span>
<span class="n">trip_distance</span>                   <span class="n">float64</span>
<span class="n">pickup_longitude</span>                <span class="n">float64</span>
<span class="n">pickup_latitude</span>                 <span class="n">float64</span>
<span class="n">RateCodeID</span>                        <span class="n">int64</span>
<span class="n">store_and_fwd_flag</span>               <span class="nb">object</span>
<span class="n">dropoff_longitude</span>               <span class="n">float64</span>
<span class="n">dropoff_latitude</span>                <span class="n">float64</span>
<span class="n">payment_type</span>                      <span class="n">int64</span>
<span class="n">fare_amount</span>                     <span class="n">float64</span>
<span class="n">extra</span>                           <span class="n">float64</span>
<span class="n">mta_tax</span>                         <span class="n">float64</span>
<span class="n">tip_amount</span>                      <span class="n">float64</span>
<span class="n">tolls_amount</span>                    <span class="n">float64</span>
<span class="n">improvement_surcharge</span>           <span class="n">float64</span>
<span class="n">total_amount</span>\<span class="n">r</span>                  <span class="n">float64</span>
<span class="n">dtype</span><span class="p">:</span> <span class="nb">object</span>
</code></pre>
</div>

<p>We didn’t have to find columns or specify data-types.  We didn’t have to parse
each value with an <code class="highlighter-rouge">int</code> or <code class="highlighter-rouge">float</code> function as appropriate.  We didn’t have to
parse the datetimes, but instead just specified a <code class="highlighter-rouge">parse_datetimes=</code> keyword.
The CSV parsing happened about as quickly as can be expected for this format,
clocking in at a network total of a bit under 1 GB/s.</p>

<p>Pandas is well loved because it removes all of these little hurdles from the
life of the analyst.  If we tried to reinvent a new
“Big-Data-Frame” we would have to reimplement all of the work already well done
inside of Pandas.  Instead, dask.dataframe just coordinates and reuses the code
within the Pandas library.  It is successful largely due to work from core
Pandas developers, notably Masaaki Horikoshi
(<a href="https://github.com/sinhrks/">@sinhrks</a>), who have done tremendous work to
align the API precisely with the Pandas core library.</p>

<h2 id="analyze-tips-and-payment-types">Analyze Tips and Payment Types</h2>

<p>In an effort to demonstrate the abilities of dask.dataframe we ask a simple
question of our data, <em>“how do New Yorkers tip?”</em>.  The 2015 NYCTaxi data is
quite good about breaking down the total cost of each ride into the fare
amount, tip amount, and various taxes and fees.  In particular this lets us
measure the percentage that each rider decided to pay in tip.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">nyc2015</span><span class="p">[[</span><span class="s">'fare_amount'</span><span class="p">,</span> <span class="s">'tip_amount'</span><span class="p">,</span> <span class="s">'payment_type'</span><span class="p">]]</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre>
</div>

<table border="1" class="dataframe">
  <thead>
    <tr>
      <th/>
      <th>fare_amount</th>
      <th>tip_amount</th>
      <th>payment_type</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>12.0</td>
      <td>3.25</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>14.5</td>
      <td>2.00</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>9.5</td>
      <td>0.00</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3.5</td>
      <td>0.00</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>15.0</td>
      <td>0.00</td>
      <td>2</td>
    </tr>
  </tbody>
</table>

<p>In the first two lines we see evidence supporting the 15-20% tip standard
common in the US.  The following three lines interestingly show zero tip.
Judging only by these first five lines (a very small sample) we see a strong
correlation here with the payment type.  We analyze this a bit more by counting
occurrences in the <code class="highlighter-rouge">payment_type</code> column both for the full dataset, and
filtered by zero tip:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="o">%</span><span class="n">time</span> <span class="n">nyc2015</span><span class="o">.</span><span class="n">payment_type</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="n">CPU</span> <span class="n">times</span><span class="p">:</span> <span class="n">user</span> <span class="mi">132</span> <span class="n">ms</span><span class="p">,</span> <span class="n">sys</span><span class="p">:</span> <span class="mi">0</span> <span class="n">ns</span><span class="p">,</span> <span class="n">total</span><span class="p">:</span> <span class="mi">132</span> <span class="n">ms</span>
<span class="n">Wall</span> <span class="n">time</span><span class="p">:</span> <span class="mi">558</span> <span class="n">ms</span>

<span class="mi">1</span>    <span class="mi">91574644</span>
<span class="mi">2</span>    <span class="mi">53864648</span>
<span class="mi">3</span>      <span class="mi">503070</span>
<span class="mi">4</span>      <span class="mi">170599</span>
<span class="mi">5</span>          <span class="mi">28</span>
<span class="n">Name</span><span class="p">:</span> <span class="n">payment_type</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">int64</span>

<span class="o">&gt;&gt;&gt;</span> <span class="o">%</span><span class="n">time</span> <span class="n">nyc2015</span><span class="p">[</span><span class="n">nyc2015</span><span class="o">.</span><span class="n">tip_amount</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">payment_type</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="n">CPU</span> <span class="n">times</span><span class="p">:</span> <span class="n">user</span> <span class="mi">212</span> <span class="n">ms</span><span class="p">,</span> <span class="n">sys</span><span class="p">:</span> <span class="mi">4</span> <span class="n">ms</span><span class="p">,</span> <span class="n">total</span><span class="p">:</span> <span class="mi">216</span> <span class="n">ms</span>
<span class="n">Wall</span> <span class="n">time</span><span class="p">:</span> <span class="mf">1.69</span> <span class="n">s</span>

<span class="mi">2</span>    <span class="mi">53862557</span>
<span class="mi">1</span>     <span class="mi">3365668</span>
<span class="mi">3</span>      <span class="mi">502025</span>
<span class="mi">4</span>      <span class="mi">170234</span>
<span class="mi">5</span>          <span class="mi">26</span>
<span class="n">Name</span><span class="p">:</span> <span class="n">payment_type</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">int64</span>
</code></pre>
</div>

<p>We find that almost all zero-tip rides correspond to payment type 2, and that
almost all payment type 2 rides don’t tip.  My un-scientific hypothesis here is
payment type 2 corresponds to cash fares and that we’re observing a tendancy of
drivers not to record cash tips.  However we would need more domain knowledge
about our data to actually make this claim with any degree of authority.</p>

<h2 id="analyze-tips-fractions">Analyze Tips Fractions</h2>

<p>Lets make a new column, <code class="highlighter-rouge">tip_fraction</code>, and then look at the average of this
column grouped by day of week and grouped by hour of day.</p>

<p>First, we need to filter out bad rows, both rows with this odd payment type,
and rows with zero fare (there are a surprising number of free cab rides in
NYC.)  Second we create a new column equal to the ratio of <code class="highlighter-rouge">tip_amount /
fare_amount</code>.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">df</span> <span class="o">=</span> <span class="n">nyc2015</span><span class="p">[(</span><span class="n">nyc2015</span><span class="o">.</span><span class="n">fare_amount</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">nyc2015</span><span class="o">.</span><span class="n">payment_type</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">)]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">tip_fraction</span><span class="o">=</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">tip_amount</span> <span class="o">/</span> <span class="n">df</span><span class="o">.</span><span class="n">fare_amount</span><span class="p">))</span>
</code></pre>
</div>

<p>Next we choose to groupby the pickup datetime column in order to see how the
average tip fraction changes by day of week and by hour.  The groupby and
datetime handling of Pandas makes these operations trivial.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">dayofweek</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">tpep_pickup_datetime</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">dayofweek</span><span class="p">)</span><span class="o">.</span><span class="n">tip_fraction</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">hour</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">tpep_pickup_datetime</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">hour</span><span class="p">)</span><span class="o">.</span><span class="n">tip_fraction</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">dayofweek</span><span class="p">,</span> <span class="n">hour</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">persist</span><span class="p">([</span><span class="n">dayofweek</span><span class="p">,</span> <span class="n">hour</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">progress</span><span class="p">(</span><span class="n">dayofweek</span><span class="p">,</span> <span class="n">hour</span><span class="p">)</span>
</code></pre>
</div>

<p><img src="http://mrocklin.github.com/blog/images/distributed-hdfs-groupby-tip-fraction.gif"/></p>

<p>Grouping by day-of-week doesn’t show anything too striking to my eye.  However
I would like to note at how generous NYC cab riders seem to be.  A 23-25% tip
can be quite nice:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">dayofweek</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="n">tpep_pickup_datetime</span>
<span class="mi">0</span>    <span class="mf">0.237510</span>
<span class="mi">1</span>    <span class="mf">0.236494</span>
<span class="mi">2</span>    <span class="mf">0.236073</span>
<span class="mi">3</span>    <span class="mf">0.246007</span>
<span class="mi">4</span>    <span class="mf">0.242081</span>
<span class="mi">5</span>    <span class="mf">0.232415</span>
<span class="mi">6</span>    <span class="mf">0.259974</span>
<span class="n">Name</span><span class="p">:</span> <span class="n">tip_fraction</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">float64</span>
</code></pre>
</div>

<p>But grouping by hour shows that late night and early morning riders are more
likely to tip extravagantly:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">hour</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="n">tpep_pickup_datetime</span>
<span class="mi">0</span>     <span class="mf">0.263602</span>
<span class="mi">1</span>     <span class="mf">0.278828</span>
<span class="mi">2</span>     <span class="mf">0.293536</span>
<span class="mi">3</span>     <span class="mf">0.276784</span>
<span class="mi">4</span>     <span class="mf">0.348649</span>
<span class="mi">5</span>     <span class="mf">0.248618</span>
<span class="mi">6</span>     <span class="mf">0.233257</span>
<span class="mi">7</span>     <span class="mf">0.216003</span>
<span class="mi">8</span>     <span class="mf">0.221508</span>
<span class="mi">9</span>     <span class="mf">0.217018</span>
<span class="mi">10</span>    <span class="mf">0.225618</span>
<span class="mi">11</span>    <span class="mf">0.231396</span>
<span class="mi">12</span>    <span class="mf">0.225186</span>
<span class="mi">13</span>    <span class="mf">0.235662</span>
<span class="mi">14</span>    <span class="mf">0.237636</span>
<span class="mi">15</span>    <span class="mf">0.228832</span>
<span class="mi">16</span>    <span class="mf">0.234086</span>
<span class="mi">17</span>    <span class="mf">0.240635</span>
<span class="mi">18</span>    <span class="mf">0.237488</span>
<span class="mi">19</span>    <span class="mf">0.272792</span>
<span class="mi">20</span>    <span class="mf">0.235866</span>
<span class="mi">21</span>    <span class="mf">0.242157</span>
<span class="mi">22</span>    <span class="mf">0.243244</span>
<span class="mi">23</span>    <span class="mf">0.244586</span>
<span class="n">Name</span><span class="p">:</span> <span class="n">tip_fraction</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">float64</span>
<span class="n">In</span> <span class="p">[</span><span class="mi">24</span><span class="p">]:</span>
</code></pre>
</div>

<p>We plot this with matplotlib and see a nice trough during business hours with a
surge in the early morning with an astonishing peak of 34% at 4am:</p>

<p><img src="http://mrocklin.github.com/blog/images/nyctaxi-2015-hourly-tips.png"/></p>

<h2 id="performance">Performance</h2>

<p>Lets dive into a few operations that run at different time scales.  This gives
a good understanding of the strengths and limits of the scheduler.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="o">%</span><span class="n">time</span> <span class="n">nyc2015</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
<span class="n">CPU</span> <span class="n">times</span><span class="p">:</span> <span class="n">user</span> <span class="mi">4</span> <span class="n">ms</span><span class="p">,</span> <span class="n">sys</span><span class="p">:</span> <span class="mi">0</span> <span class="n">ns</span><span class="p">,</span> <span class="n">total</span><span class="p">:</span> <span class="mi">4</span> <span class="n">ms</span>
<span class="n">Wall</span> <span class="n">time</span><span class="p">:</span> <span class="mf">20.9</span> <span class="n">ms</span>
</code></pre>
</div>

<p>This head computation is about as fast as a film projector.  You could perform
this roundtrip computation between every consecutive frame of a movie; to a
human eye this appears fluid.  In the <a href="http://matthewrocklin.com/blog/work/2016/02/17/dask-distributed-part1">last post</a>
we asked about how low we could bring latency.  In that post we were running
computations from my laptop in California and so were bound by transcontinental
latencies of 200ms.  This time, because we’re operating from the cluster, we
can get down to 20ms.  We’re only able to be this fast because we touch only a
single data element, the first partition.  Things change when we need to touch
the entire dataset.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="o">%</span><span class="n">time</span> <span class="nb">len</span><span class="p">(</span><span class="n">nyc2015</span><span class="p">)</span>
<span class="n">CPU</span> <span class="n">times</span><span class="p">:</span> <span class="n">user</span> <span class="mi">48</span> <span class="n">ms</span><span class="p">,</span> <span class="n">sys</span><span class="p">:</span> <span class="mi">0</span> <span class="n">ns</span><span class="p">,</span> <span class="n">total</span><span class="p">:</span> <span class="mi">48</span> <span class="n">ms</span>
<span class="n">Wall</span> <span class="n">time</span><span class="p">:</span> <span class="mi">271</span> <span class="n">ms</span>
</code></pre>
</div>

<p>The length computation takes 200-300 ms.  This computation takes longer because we
touch every individual partition of the data, of which there are 178.  The
scheduler incurs about 1ms of overhead per task, add a bit of latency
and you get the ~200ms total.  This means that the scheduler will likely be the
bottleneck whenever computations are very fast, such as is the case for
computing <code class="highlighter-rouge">len</code>.  Really, this is good news; it means that by improving the
scheduler we can reduce these durations even further.</p>

<p>If you look at the groupby computations above you can add the numbers in the
progress bars to show that we computed around 3000 tasks in around 7s.  It
looks like this computation is about half scheduler overhead and about half
bound by actual computation.</p>

<h2 id="conclusion">Conclusion</h2>

<p>We used dask+distributed on a cluster to read CSV data from HDFS
into a dask dataframe.  We then used dask.dataframe, which looks identical to
the Pandas dataframe, to manipulate our distributed dataset intuitively and
efficiently.</p>

<p>We looked a bit at the performance characteristics of simple computations.</p>

<h2 id="what-doesnt-work">What doesn’t work</h2>

<p>As always I’ll have a section like this that honestly says what doesn’t work
well and what I would have done with more time.</p>

<ul>
  <li>
    <p>Dask dataframe implements a commonly used <em>subset</em> of Pandas functionality,
not all of it.  It’s surprisingly hard to communicate the exact bounds of
this subset to users.  Notably, in the distributed setting we don’t have a
shuffle algorithm, so <code class="highlighter-rouge">groupby(...).apply(...)</code> and some joins are not
yet possible.</p>
  </li>
  <li>
    <p>If you want to use threads, you’ll need Pandas 0.18.0 which, at the time of
this writing, was still in release candidate stage.  This Pandas release
fixes some important GIL related issues.</p>
  </li>
  <li>
    <p>The 1ms overhead per task limit is significant.  While we can still scale
out to clusters far larger than what we have here, we probably won’t be
able to strongly accelerate very quick operations until we reduce this
number.</p>
  </li>
  <li>
    <p>We use the <a href="http://hdfs3.readthedocs.org/en/latest/">hdfs3 library</a> to read
data from HDFS.  This library seems to work great but is new and could use
more active users to flush out bug reports.</p>
  </li>
</ul>

<h2 id="links">Links</h2>



<h2 id="setup-and-data">Setup and Data</h2>

<p>You can obtain public data from the New York City Taxi and Limousine Commission
<a href="http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml">here</a>.  I
downloaded this onto the head node and dumped it into HDFS with commands like
the following:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>$ wget https://storage.googleapis.com/tlc-trip-data/2015/yellow_tripdata_2015-{01..12}.csv
$ hdfs dfs -mkdir /nyctaxi
$ hdfs dfs -mkdir /nyctaxi/2015
$ hdfs dfs -put yellow*.csv /nyctaxi/2015/
</code></pre>
</div>

<p>The cluster was hosted on EC2 and was comprised of nine <code class="highlighter-rouge">m3.2xlarges</code> with 8
cores and 30GB of RAM each.  Eight of these nodes were used as workers; they
used processes for parallelism, not threads.</p>

    <hr/>
    
    <hr/>
    


  <p id="disqus_thread"/>

<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">blog comments powered by <span class="logo-disqus">Disqus</span></a>




  </div>
  
</div></body></html>