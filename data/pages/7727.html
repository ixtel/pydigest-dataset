<html><body><div><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-reinforcement-learning-using-tensor-flow" class="anchor" href="#reinforcement-learning-using-tensor-flow" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Reinforcement Learning using Tensor Flow</h1>

<p><b> For all of you smarty pants, who discovered the "continuous" branch of my repo: it is not yet functional, I never got it to coverge and it probably has a software bug. If you want to play around with it you are welcome, but be prepared to do some serious debugging. If you do get it to work though, definitely let me know! ;-) I'll probably get back to fixing it some time in February / March.</b></p>

<h2><a id="user-content-quick-start" class="anchor" href="#quick-start" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Quick start</h2>

<p>Check out Karpathy game in <code>notebooks</code> folder.</p>

<p><a href="/nivwusquorum/tensorflow-deepq/blob/master/data/example.gif" target="_blank"><img src="/nivwusquorum/tensorflow-deepq/raw/master/data/example.gif"/></a></p>

<p><em>The image above depicts a strategy learned by the DeepQ controller. Available actions are accelerating top, bottom, left or right. The reward signal is +1 for the green fellas, -1 for red and -5 for orange.</em></p>

<h2><a id="user-content-requirements" class="anchor" href="#requirements" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Requirements</h2>

<ul>
<li><code>future==0.15.2</code></li>
<li><code>euclid==0.1</code></li>
</ul>

<h2><a id="user-content-how-does-this-all-fit-together" class="anchor" href="#how-does-this-all-fit-together" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>How does this all fit together.</h2>

<p><code>tf_rl</code> has controllers and simulators which can be pieced together using simulate function.</p>

<h2><a id="user-content-using-human-controller" class="anchor" href="#using-human-controller" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Using human controller.</h2>

<p>Want to have some fun controlling the simulation by yourself? You got it!
Use <code>tf_rl.controller.HumanController</code> in your simulation.</p>

<p>To issue commands run in terminal</p>

<pre lang="python3"><code>python3 tf_rl/controller/human_controller.py
</code></pre>

<p>For it to work you also need to have a redis server running locally.</p>

<h2><a id="user-content-writing-your-own-controller" class="anchor" href="#writing-your-own-controller" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Writing your own controller</h2>

<p>To write your own controller define a controller class with 3 functions:</p>

<ul>
<li><code>action(self, observation)</code> given an observation (usually a tensor of numbers) representing an observation returns action to perform.</li>
<li><code>store(self, observation, action, reward, newobservation)</code> called each time a transition is observed from <code>observation</code> to <code>newobservation</code>. Transition is a consequence of <code>action</code> and has associated <code>reward</code></li>
<li><code>training_step(self)</code> if your controller requires training that is the place to do it, should not take to long, because it will be called roughly every action execution.</li>
</ul>

<h2><a id="user-content-writing-your-own-simulation" class="anchor" href="#writing-your-own-simulation" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Writing your own simulation</h2>

<p>To write your own simulation define a simulation class with 4 functions:</p>

<ul>
<li><code>observe(self)</code> returns a current observation</li>
<li><code>collect_reward(self)</code> returns the reward accumulated since the last time function was called.</li>
<li><code>perform_action(self, action)</code> updates internal state to reflect the fact that <code>aciton</code> was executed</li>
<li><code>step(self, dt)</code> update internal state as if <code>dt</code> of simulation time has passed.</li>
<li><code>to_html(self, info=[])</code> generate an html visualization of the game. <code>info</code> can be optionally passed an has a list of strings that should be displayed along with the visualization</li>
</ul>

<h2><a id="user-content-creating-gifs-based-on-simulation" class="anchor" href="#creating-gifs-based-on-simulation" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Creating GIFs based on simulation</h2>

<p>The <code>simulate</code> method accepts <code>save_path</code> argument which is a folder where all the consecutive images will be stored.
To make them into a GIF use <code>scripts/make_gif.sh PATH</code> where path is the same as the path you passed to <code>save_path</code> argument</p>
</article>
  </div></body></html>