<html><body><div><article class="markdown-body entry-content" itemprop="text"><h2><a id="user-content-autorccar" class="anchor" href="#autorccar" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>AutoRCCar</h2>

<p><a href="https://youtu.be/BBwEF6WBUQs">See self-driving in action (Youtube)</a></p>

<p>A scaled down version of self-driving system using a RC car, Raspberry Pi, Arduino and open source software. The system uses a Raspberry Pi with a camera and an ultrasonic sensor as inputs, a processing computer that handles steering, object recognition (stop sign and traffic light) and distance measurement, and an Arduino board for RC car control.</p>

<h3><a id="user-content-dependencies" class="anchor" href="#dependencies" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Dependencies</h3>

<ul>
<li>Raspberry Pi: 

</li>
<li>Computer:

<ul>
<li>Numpy</li>
<li>OpenCV</li>
<li>Pygame</li>
<li>PiSerial</li>
</ul></li>
</ul>

<h3><a id="user-content-about" class="anchor" href="#about" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>About</h3>

<ul>
<li>raspberrt_pi/ 

<ul>
<li><strong><em>stream_client.py</em></strong>: stream video frames in jpeg format to the host computer</li>
<li><strong><em>ultrasonic_client.py</em></strong>: send distance data measured by sensor to the host computer</li>
</ul></li>
<li>arduino/

<ul>
<li><strong><em>rc_keyboard_control.ino</em></strong>: acts as a interface between rc controller and computer and allows user to send command via USB serial interface</li>
</ul></li>
<li>computer/

<ul>
<li>cascade_xml/ 

<ul>
<li>trained cascade classifiers xml files</li>
</ul></li>
<li>chess_board/ 

<ul>
<li>images for calibration, captured by pi camera </li>
</ul></li>
<li>training_data/ 

<ul>
<li>training image data for neural network in npz format</li>
</ul></li>
<li>testing_data/ 

<ul>
<li>testing image data for neural network in npz format</li>
</ul></li>
<li>training_images/ 

<ul>
<li>saved video frames during image training data collection stage (optional)</li>
</ul></li>
<li>mlp_xml/ 

<ul>
<li>trained neural network parameters in a xml file</li>
</ul></li>
<li><strong><em>rc_control_test.py</em></strong>: drive RC car with keyboard (testing purpose)</li>
<li><strong><em>picam_calibration.py</em></strong>: pi camera calibration, returns camera matrix</li>
<li><strong><em>collect_training_data.py</em></strong>: receive streamed video frames and label frames for later training</li>
<li><strong><em>mlp_training.py</em></strong>: neural network training</li>
<li><strong><em>mlp_predict_test.py</em></strong>: test trained neural network with testing data</li>
<li><strong><em>rc_driver.py</em></strong>: a multithread server program receives video frames and sensor data, and allows RC car drives by itself with stop sign, traffic light detection and front collision avoidance capabilities</li>
</ul></li>
</ul>

<h3><a id="user-content-how-to-drive" class="anchor" href="#how-to-drive" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>How to drive</h3>

<ol>
<li><p><strong>Flash Arduino</strong>: Flash <em>“rc_keyboard_control.ino”</em> to Arduino and run <em>“rc_control_test.py”</em> to drive the rc car with keyboard (testing purpose)</p></li>
<li><p><strong>Pi Camera calibration:</strong> Take multiple chess board images using pi camera at various angles and put them into “chess_board” folder, run <em>“picam_calibration.py”</em> and it returns the camera matrix, those parameters will be used in <em>“rc_driver.py”</em></p></li>
<li><p><strong>Collect training data and testing data:</strong> First run <em>“collect_training_data.py”</em> and then run <em>“stream_client.py”</em> on raspberry pi. User presses keyboard to drive the RC car, frames are saved only when there is a key press action. When finished driving, press “q” to exit, data is saved as a npz file. </p></li>
<li><p><strong>Neural network training:</strong> Run <em>“mlp_training.py”</em>, depend on the parameters chosen, it will take some time to train. After training, parameters are saved in “mlp_xml” folder</p></li>
<li><p><strong>Neural network testing:</strong> Run <em>“mlp_predict_test.py”</em> to load testing data from “testing_data” folder and trained parameters from the xml file in “mlp_xml” folder</p></li>
<li><p><strong>Cascade classifiers training (optional):</strong> trained stop sign and traffic light classifiers are included in the "cascade_xml" folder, if you are interested in training your own classifiers, please refer to <a href="http://docs.opencv.org/doc/user_guide/ug_traincascade.html">OpenCV documentation</a> and <a href="http://coding-robin.de/2013/07/22/train-your-own-opencv-haar-classifier.html">this great tutorial by Thorsten Ball</a></p></li>
<li><p><strong>Self-driving in action</strong>: First run <em>“rc_driver.py”</em> to start the server on the computer and then run <em>“stream_client.py”</em> and <em>“ultrasonic_client.py”</em> on raspberry pi. </p></li>
</ol>
</article>
  </div></body></html>