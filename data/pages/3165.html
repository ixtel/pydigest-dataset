<html><body><div><div class="all_external_links">
          <p><a href="http://hadoop.apache.org/">Hadoop</a> is the standard tool for distributed computing across really large data sets and is the reason why you see "Big Data" on advertisements as you walk through the airport. It has become an operating system for Big Data, providing a rich ecosystem of tools and techniques that allow you to use a large cluster of relatively cheap commodity hardware to do computing at supercomputer scale. Two ideas from Google in 2003 and 2004 made Hadoop possible: a framework for distributed storage (<a href="http://static.googleusercontent.com/media/research.google.com/en/us/archive/gfs-sosp2003.pdf">The Google File System</a>), which is implemented as HDFS in Hadoop, and a framework for distributed computing (<a href="http://research.google.com/archive/mapreduce.html">MapReduce</a>).</p>

<p>These two ideas have been the prime drivers for the advent of scaling analytics, large scale machine learning, and other big data appliances for the last ten years! However, in technology terms, ten years is an incredibly long time, and there are some well-known limitations that exist, with MapReduce in particular. Notably, programming MapReduce is difficult. You have to chain Map and Reduce tasks together in multiple steps for most analytics. This has resulted in <em>specialized</em> systems for performing SQL-like computations or machine learning. Worse, MapReduce requires data to be serialized to disk between each step, which means that the I/O cost of a MapReduce job is high, making interactive analysis and iterative algorithms very expensive; and the thing is, almost all optimization and machine learning is iterative.</p>

<p>To address these problems, Hadoop has been moving to a more general resource management framework for computation, <a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html">YARN</a> (Yet Another Resource Negotiator). YARN implements the next generation of MapReduce, but also allows applications to leverage distributed resources without having to compute with MapReduce. By generalizing the management of the cluster, research has moved toward generalizations of distributed computation, expanding the ideas first imagined in MapReduce.</p>

<p><a href="https://spark.apache.org/">Spark</a> is the first fast, general purpose distributed computing paradigm resulting from this shift and is gaining popularity rapidly. Spark extends the MapReduce model to support more types of computations using a functional programming paradigm, and it can cover a wide range of workflows that previously were implemented as specialized systems built on top of Hadoop. Spark uses in-memory caching to improve performance and, therefore, is fast enough to allow for interactive analysis (as though you were sitting on the Python interpreter, interacting with the cluster). Caching also improves the performance of iterative algorithms, which makes it great for data theoretic tasks, especially machine learning.</p>

<p>In this post we will first discuss how to set up Spark to start easily performing analytics, either simply on your local machine or in a cluster on EC2. We then will explore Spark at an introductory level, moving towards an understanding of what Spark is and how it works (hopefully motivating further exploration). In the last two sections we will start to interact with Spark on the command line and then demo how to write a Spark application in Python and submit it to the cluster as a Spark job.</p>

<h2 id="toc_0">Setting up Spark</h2>

<p>Spark is pretty simple to set up and get running on your machine. All you really need to do is download one of the pre-built packages and so long as you have Java 6+ and Python 2.6+ you can simply run the Spark binary on Windows, Mac OS X, and Linux. Ensure that the <code>java</code> program is on your <code>PATH</code> or that the <code>JAVA_HOME</code> environment variable is set. Similarly, <code>python</code> must also be in your <code>PATH</code>.</p>

<p>Assuming you already have Java and Python:</p>

<ol>
<li>Visit the <a href="http://spark.apache.org/downloads.html">Spark downloads</a> page</li>
<li>Select the latest Spark release (1.2.0 at the time of this writing), a prebuilt package for Hadoop 2.4, and download directly.</li>
</ol>

<p>At this point, you'll have to figure out how to go about things depending on your operating system. Windows users, please feel free to comment about tips to set up in the comments section.</p>

<p>Generally, my suggestion is to do as follows (on a POSIX OS):</p>

<ol>
<li><p>Unzip Spark</p>
<div class="highlight"><pre>~<span class="nv">$ </span>tar -xzf spark-1.2.0-bin-hadoop2.4.tgz
</pre></div></li>
<li><p>Move the unzipped directory to a working application directory (<code>C:\Program Files</code> for example on Windows, or <code>/opt/</code> on Linux). Where you move it to doesn't really matter, so long as you have permissions and can run the binaries there. I typically install Hadoop and related tools in <code>/srv/</code> on my Ubuntu boxes, and will use that directory here for illustration.</p>
<div class="highlight"><pre>~<span class="nv">$ </span>mv spark-1.2.0-bin-hadoop2.4 /srv/spark-1.2.0
</pre></div></li>
<li><p>Symlink the version of Spark to a <code>spark</code> directory. This will allow you to simply download new/older versions of Spark and modify the link to manage Spark versions without having to change your path or environment variables.</p>
<div class="highlight"><pre>~<span class="nv">$ </span>ln -s /srv/spark-1.2.0 /srv/spark
</pre></div></li>
<li><p>Edit your BASH profile to add Spark to your <code>PATH</code> and to set the <code>SPARK_HOME</code> environment variable. These helpers will assist you on the command line. On Ubuntu, simply edit the <code>~/.bash_profile</code> or <code>~/.profile</code> files and add the following:</p>
<div class="highlight"><pre><span class="nb">export </span><span class="nv">SPARK_HOME</span><span class="o">=</span>/srv/spark
<span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$SPARK_HOME</span>/bin:<span class="nv">$PATH</span>
</pre></div></li>
<li><p>After you source your profile (or simply restart your terminal), you should now be able to run a <code>pyspark</code> interpreter locally. Execute the <code>pyspark</code> command, and you should see a result as follows:</p>
<div class="highlight"><pre>~<span class="nv">$ </span>pyspark
Python 2.7.8 <span class="o">(</span>default, Dec  2 2014, 12:45:58<span class="o">)</span>
<span class="o">[</span>GCC 4.2.1 Compatible Apple LLVM 6.0 <span class="o">(</span>clang-600.0.54<span class="o">)]</span> on darwin
Type <span class="s2">"help"</span>, <span class="s2">"copyright"</span>, <span class="s2">"credits"</span> or <span class="s2">"license"</span> <span class="k">for </span>more information.
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Using Sparks default log4j profile: org/apache/spark/log4j-defaults.properties
<span class="o">[</span>… snip …<span class="o">]</span>
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _<span class="se">\ \/</span> _ <span class="se">\/</span> _ <span class="sb">`</span>/ __/  <span class="sb">`</span>_/
   /__ / .__/<span class="se">\_</span>,_/_/ /_/<span class="se">\_\ </span>  version 1.2.0
      /_/

Using Python version 2.7.8 <span class="o">(</span>default, Dec  2 2014 12:45:58<span class="o">)</span>
SparkContext available as sc.
&gt;&gt;&gt;
</pre></div></li>
</ol>

<p>At this point Spark is installed and ready to use on your local machine in "standalone mode." You can develop applications here and submit Spark jobs that will run in a multi-process/multi-threaded mode, or you can configure this machine as a client to a cluster (though this is not recommended as the driver plays an important role in Spark jobs and should be in the same network as the rest of the cluster). Probably the most you will do with Spark on your local machine beyond development is to use the <code>spark-ec2</code> scripts to configure an EC2 Spark cluster on Amazon's cloud.</p>

<h3 id="toc_1">Minimizing the Verbosity of Spark</h3>

<p>The execution of Spark (and PySpark) can be extremely verbose, with many INFO log messages printed out to the screen. This is particularly annoying during development, as Python stack traces or the output of <code>print</code> statements can be lost. In order to reduce the verbosity of Spark, you can configure the log4j settings in <code>$SPARK_HOME/conf</code>. First, create a copy of the <code>$SPARK_HOME/conf/log4j.properties.template</code> file, removing the ".template" extension.</p>
<div class="highlight"><pre>~<span class="nv">$ </span>cp <span class="nv">$SPARK_HOME</span>/conf/log4j.properties.template <span class="se">\</span>
      <span class="nv">$SPARK_HOME</span>/conf/log4j.properties
</pre></div>
<p>Edit the newly copied file and replace <code>INFO</code> with <code>WARN</code> at every line in the code. You log4j.properties file should look similar to:</p>
<div class="highlight"><pre><span class="c1"># Set everything to be logged to the console</span>
<span class="na">log4j.rootCategory</span><span class="o">=</span><span class="s">WARN, console</span>
<span class="na">log4j.appender.console</span><span class="o">=</span><span class="s">org.apache.log4j.ConsoleAppender</span>
<span class="na">log4j.appender.console.target</span><span class="o">=</span><span class="s">System.err</span>
<span class="na">log4j.appender.console.layout</span><span class="o">=</span><span class="s">org.apache.log4j.PatternLayout</span>
<span class="na">log4j.appender.console.layout.ConversionPattern</span><span class="o">=</span><span class="s">%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n</span>

<span class="c1"># Settings to quiet third party logs that are too verbose</span>
<span class="na">log4j.logger.org.eclipse.jetty</span><span class="o">=</span><span class="s">WARN</span>
<span class="na">log4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle</span><span class="o">=</span><span class="s">ERROR</span>
<span class="na">log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper</span><span class="o">=</span><span class="s">WARN</span>
<span class="na">log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter</span><span class="o">=</span><span class="s">WARN</span>
</pre></div>
<p>Now when you run PySpark you should get much simpler output messages! Special thanks to <a href="https://twitter.com/genomegeek">@genomegeek</a> who pointed this out at a District Data Labs workshop!</p>

<h3 id="toc_2">Using IPython Notebook with Spark</h3>

<p>When Googling around for helpful Spark tips, I discovered a couple posts that mentioned how to configure PySpark with IPython notebook. <a href="http://ipython.org/notebook.html">IPython notebook</a> is an essential tool for data scientists to present their scientific and theoretical work in an interactive fashion, integrating both text and Python code. For many data scientists, IPython notebook is their first introduction to Python and is used widely so I thought it would be worth including it in this post.</p>

<p>Most of the instructions here are adapted from an IPython notebook: <a href="http://nbviewer.ipython.org/gist/fperez/6384491/00-Setup-IPython-PySpark.ipynb">Setting up IPython with PySpark</a>. However, we will focus on connecting your IPython shell to PySpark in standalone mode on your local computer rather than on an EC2 cluster. If you would like to work with PySpark/IPython on a cluster, feel free to check out those instructions and if you do, please comment on how it went!</p>

<ol>
<li><p>Create an iPython notebook profile for our Spark configuration.</p>
<div class="highlight"><pre>~<span class="nv">$ </span>ipython profile create spark
<span class="o">[</span>ProfileCreate<span class="o">]</span> Generating default config file: u<span class="s1">'$HOME/.ipython/profile_spark/ipython_config.py'</span>
<span class="o">[</span>ProfileCreate<span class="o">]</span> Generating default config file: u<span class="s1">'$HOME/.ipython/profile_spark/ipython_notebook_config.py'</span>
<span class="o">[</span>ProfileCreate<span class="o">]</span> Generating default config file: u<span class="s1">'$HOME/.ipython/profile_spark/ipython_nbconvert_config.py'</span>
</pre></div>
<p>Keep note of where the profile has been created, and replace the appropriate paths in the following steps:</p></li>
<li><p>Create a file in <code>$HOME/.ipython/profile_spark/startup/00-pyspark-setup.py</code> and add the following:</p>
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="c"># Configure the environment</span>
<span class="k">if</span> <span class="s">'SPARK_HOME'</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">:</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'SPARK_HOME'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'/srv/spark'</span>

<span class="c"># Create a variable for our root path</span>
<span class="n">SPARK_HOME</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'SPARK_HOME'</span><span class="p">]</span>

<span class="c"># Add the PySpark/py4j to the Python Path</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">SPARK_HOME</span><span class="p">,</span> <span class="s">"python"</span><span class="p">,</span> <span class="s">"build"</span><span class="p">))</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">SPARK_HOME</span><span class="p">,</span> <span class="s">"python"</span><span class="p">))</span>
</pre></div></li>
<li><p>Start up an IPython notebook with the profile we just created.</p>
<div class="highlight"><pre>~<span class="nv">$ </span>ipython notebook --profile spark
</pre></div></li>
<li><p>In your notebook, you should see the variables we just created.</p>
</li>
<li><p>At the top of your IPython notebook, make sure you add the Spark context.</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span>  <span class="n">SparkContext</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span> <span class="s">'local'</span><span class="p">,</span> <span class="s">'pyspark'</span><span class="p">)</span>
</pre></div></li>
<li><p>Test the Spark context by doing a simple computation using IPython.</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">isprime</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    check if integer n is a prime</span>
<span class="sd">    """</span>
    <span class="c"># make sure n is a positive integer</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
    <span class="c"># 0 and 1 are not primes</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">False</span>
    <span class="c"># 2 is the only even prime number</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">True</span>
    <span class="c"># all other even numbers are not primes</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">n</span> <span class="o">&amp;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">False</span>
    <span class="c"># range starts with 3 and only needs to go up the square root of n</span>
    <span class="c"># for all odd numbers</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">n</span> <span class="o">%</span> <span class="n">x</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">False</span>
    <span class="k">return</span> <span class="bp">True</span>

<span class="c"># Create an RDD of numbers from 0 to 1,000,000</span>
<span class="n">nums</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="nb">xrange</span><span class="p">(</span><span class="mi">1000000</span><span class="p">))</span>

<span class="c"># Compute the number of primes in the RDD</span>
<span class="k">print</span> <span class="n">nums</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">isprime</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
</pre></div>
<p>If you get a number without errors, then your context is working correctly!</p></li>
</ol>

<p><strong>Editor's Note</strong>: The above configures an IPython context for directly invoking IPython notebook with PySpark. However, you can also launch a notebook using PySpark directly as follows:</p>
<div class="highlight"><pre>    <span class="nv">$ IPYTHON_OPTS</span><span class="o">=</span><span class="s2">"notebook --pylab inline"</span> pyspark
</pre></div>
<p>Either methodology works similarly depending on your use case for PySpark and IPython. The former allows you to more easily connect to a cluster with IPython notebook, and thus, it is the method I prefer.</p>

<h3 id="toc_3">Using Spark on EC2</h3>

<p>In my time teaching distributed computing with Hadoop, I've discovered that a lot can be taught locally on a <a href="https://districtdatalabs.silvrback.com/creating-a-hadoop-pseudo-distributed-environment">pseudo-distributed node</a> or in single-node mode. However, in order to really get what's happening, a cluster is necessary. There is often a disconnect between learning these skills and the actual computing requirements when data just gets too large. If you have a little bit of money to spend learning how to use Spark in detail, I would recommend setting up a quick cluster for experimentation. Note that a cluster of 5 slaves (and 1 master) used at a rate of approximately 10 hours per week will cost you approximately $45.18 per month.</p>

<p>A full discussion can be found at the Spark documentation: <a href="https://spark.apache.org/docs/latest/ec2-scripts.html">Running Spark on EC2</a>. Be sure to read this documentation thoroughly as you'll end up sending money on an EC2 cluster if you start these steps! I've highlighted a few key points here:</p>

<ol>
<li>Obtain a set of AWS EC2 key pairs (access key and secret key) via the <a href="http://aws.amazon.com/console/">AWS Console</a>.</li>
<li><p>Export your key pairs to your environment. Either issue these commands in your shell, or add them to your profile.</p>
<div class="highlight"><pre><span class="nb">export </span><span class="nv">AWS_ACCESS_KEY_ID</span><span class="o">=</span>myaccesskeyid
<span class="nb">export </span><span class="nv">AWS_SECRET_ACCESS_KEY</span><span class="o">=</span>mysecretaccesskey
</pre></div>
<p>Note that different utilities use different environment names, so make sure to use these for the Spark scripts.</p></li>
<li><p>Launch a cluster as follows:</p>
<div class="highlight"><pre>~<span class="nv">$ </span><span class="nb">cd</span> <span class="nv">$SPARK_HOME</span>/ec2
ec2<span class="nv">$ </span>./spark-ec2 -k &lt;keypair&gt; -i &lt;key-file&gt; -s &lt;num-slaves&gt; launch &lt;cluster-name&gt;
</pre></div></li>
<li><p>SSH into a cluster to run Spark jobs.</p>
<div class="highlight"><pre>ec2<span class="nv">$ </span>./spark-ec2 -k &lt;keypair&gt; -i &lt;key-file&gt; login &lt;cluster-name&gt;
</pre></div></li>
<li><p>Destroy a cluster as follows.</p>
<div class="highlight"><pre>ec2<span class="nv">$ </span>./spark-ec2 destroy &lt;cluster-name&gt;.
</pre></div></li>
</ol>

<p>These scripts will automatically create a local HDFS cluster for you to add data to, and there is a <code>copy-dir</code> command that will allow you to sync code and data to the cluster. However, your best bet is to simply use S3 for data storage and create RDDs that load data using the <code>s3://</code> URI.</p>

<h2 id="toc_4">What is Spark?</h2>

<p>Now that we have Spark set up, let's have a bit of a discussion about what Spark is. Spark is a general purpose cluster computing framework that provides efficient in-memory computations for large data sets by distributing computation across multiple computers. If you're familiar with Hadoop, then you know that any distributed computing framework needs to solve two problems: how to distribute data and how to distribute computation. Hadoop uses HDFS to solve the distributed data problem and MapReduce as the programming paradigm that provides effective distributed computation. Similarly, Spark has a functional programming API in multiple languages that provides more operators than map and reduce, and does this via a distributed data framework called <em>resilient distributed datasets</em> or RDDs.</p>

<p>RDDs are essentially a programming abstraction that represents a read-only collection of objects that are partitioned across machines. RDDs can be rebuilt from a lineage (and are therefore fault tolerant), are accessed via parallel operations, can be read from and written to distributed storages like HDFS or S3, and most importantly, can be cached in the memory of worker nodes for immediate reuse. Because RDDs can be cached in memory, Spark is extremely effective at <em>iterative</em> applications, where the data is being reused throughout the course of an algorithm. Most machine learning and optimization algorithms are iterative, making Spark an extremely effective tool for data science. Additionally, because Spark is so fast, it can be accessed in an <em>interactive</em> fashion via a command line prompt similar to the Python REPL.</p>

<p>The Spark library itself contains a lot of the application elements that have found their way into most Big Data applications including support for SQL-like querying of big data, machine learning and graph algorithms, and even support for live streaming data.</p>

<p>The core components are:</p>

<ul>
<li><strong>Spark Core</strong>: Contains the basic functionality of Spark; in particular the APIs that define RDDs and the operations and actions that can be undertaken upon them. The rest of Spark's libraries are built on top of the RDD and Spark Core.</li>
<li><strong>Spark SQL</strong>: Provides APIs for interacting with Spark via the Apache Hive variant of SQL called Hive Query Language (HiveQL). Every database table is represented as an RDD and Spark SQL queries are transformed into Spark operations. For those that are familiar with Hive and HiveQL, Spark can act as a drop-in replacement.</li>
<li><strong>Spark Streaming</strong>: Enables the processing and manipulation of live streams of data in real time. Many streaming data libraries (such as Apache Storm) exist for handling real-time data. Spark Streaming enables programs to leverage this data similar to how you would interact with a normal RDD as data is flowing in.</li>
<li><strong>MLlib</strong>: A library of common machine learning algorithms implemented as Spark operations on RDDs. This library contains scalable learning algorithms like classifications, regressions, etc. that require iterative operations across large data sets. The Mahout library, formerly the Big Data machine learning library of choice, will move to Spark for its implementations in the future.</li>
<li><strong>GraphX</strong>: A collection of algorithms and tools for manipulating graphs and performing parallel graph operations and computations. GraphX extends the RDD API to include operations for manipulating graphs, creating subgraphs, or accessing all vertices in a path.</li>
</ul>

<p>Because these components meet many Big Data requirements as well as the algorithmic and computational requirements of many data science tasks, Spark has been growing rapidly in popularity. Not only that, but Spark provides APIs in <em>Scala</em>, <em>Java</em>, and <em>Python</em>; meeting the needs for many different groups and allowing more data scientists to easily adopt Spark as their Big Data solution.</p>

<h3 id="toc_5">Programming Spark</h3>

<p>Programming Spark applications is similar to other data flow languages that had previously been implemented on Hadoop. Code is written in a <em>driver program</em> which is lazily evaluated, and upon an action, the driver code is distributed across the cluster to be executed by workers on their partitions of the RDD. Results are then sent back to the driver for aggregation or compilation. Essentially the driver program creates one or more RDDs, applies operations to transform the RDD, then invokes some action on the transformed RDD.</p>

<p>These steps are outlined as follows:</p>

<ol>
<li>Define one or more RDDs either through accessing data stored on disk (HDFS, Cassandra, HBase, Local Disk), parallelizing some collection in memory, <em>transforming</em> an existing RDD, or by <em>caching</em> or <em>saving</em>.</li>
<li>Invoke operations on the RDD by passing <em>closures</em> (functions) to each element of the RDD. Spark offers over 80 high level operators beyond Map and Reduce.</li>
<li>Use the resulting RDDs with <em>actions</em> (e.g. count, collect, save, etc.). Actions kick off the computing on the cluster.</li>
</ol>

<p>When Spark runs a closure on a worker, any variables used in the closure are copied to that node, but are maintained within the local scope of that closure. Spark provides two types of <em>shared</em> variables that can be interacted with by all workers in a restricted fashion. <em>Broadcast variables</em> are distributed to all workers, but are read-only. Broadcast variables can be used as lookup tables or stopword lists. <em>Accumulators</em> are variables that workers can "add" to using associative operations and are typically used as counters.</p>

<p>Spark applications are essentially the manipulation of RDDs through <em>transformations</em> and <em>actions</em>. Future posts will go into this in greater detail, but this understanding should be enough to execute the example programs below.</p>

<h3 id="toc_6">Spark Execution</h3>

<p>A brief note on the execution of Spark. Essentially, Spark applications are run as independent sets of processes, coordinated by a <code>SparkContext</code> in a <em>driver</em> program. The context will connect to some cluster manager (e.g. YARN) which allocates system resources. Each worker in the cluster is managed by an <em>executor</em>, which is in turn managed by the <code>SparkContext</code>. The executor manages computation as well as storage and caching on each machine.</p>

<p>What is important to note is that application code is sent from the driver to the executors, and the executors specify the context and the various <em>tasks</em> to be run. The executors communicate back and forth with the driver for data sharing or for interaction. Drivers are key participants in Spark jobs, and therefore, they should be on the same network as the cluster. This is different from Hadoop code, where you might submit a job from anywhere to the JobTracker, which then handles the execution on the cluster.</p>

<h2 id="toc_7">Interacting with Spark</h2>

<p>The easiest way to start working with Spark is via the interactive command prompt. To open the PySpark terminal, simply  type in <code>pyspark</code> on the command line.</p>
<div class="highlight"><pre>~<span class="nv">$ </span>pyspark
<span class="o">[</span>… snip …<span class="o">]</span>
&gt;&gt;&gt;
</pre></div>
<p>PySpark will automatically create a <code>SparkContext</code> for you to work with, using the local Spark configuration. It is exposed to the terminal via the <code>sc</code> variable. Let's create our first RDD.</p>
<div class="highlight"><pre><span class="o">&gt;&gt;&gt;</span> <span class="n">text</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s">"shakespeare.txt"</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span> <span class="n">text</span>
<span class="n">shakespeare</span><span class="o">.</span><span class="n">txt</span> <span class="n">MappedRDD</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="n">at</span> <span class="n">textFile</span> <span class="n">at</span> <span class="n">NativeMethodAccessorImpl</span><span class="o">.</span><span class="n">java</span><span class="p">:</span><span class="o">-</span><span class="mi">2</span>
</pre></div>
<p>The <code>textFile</code> method loads the <a href="http://bit.ly/16c7kPV">complete works of Shakespeare</a> into an RDD named text. If you inspect the RDD you can see that it is a MappedRDD and that the path to the file is a relative path from the current working directory (pass in a correct path to the shakespeare.txt file on your system). Let's start to transform this RDD in order to compute the "hello world" of distributed computing: "word count."</p>
<div class="highlight"><pre><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">add</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
<span class="o">...</span>     <span class="k">return</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">words</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">flatMap</span><span class="p">(</span><span class="n">tokenize</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span> <span class="n">words</span>
<span class="n">PythonRDD</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="n">at</span> <span class="n">RDD</span> <span class="n">at</span> <span class="n">PythonRDD</span><span class="o">.</span><span class="n">scala</span><span class="p">:</span><span class="mi">43</span>
</pre></div>
<p>We first imported the operator <code>add</code>, which is a named function that can be used as a closure for addition. We'll use this function later. The first thing we have to do is split our text into words. We created a function called <code>tokenize</code> whose argument is some piece of text and who returns a list of the tokens (words) in that text by simply splitting on whitespace. We then created a new RDD called <code>words</code> by transforming the <code>text</code> RDD through the application of the <code>flatMap</code> operator, and passed it the closure <code>tokenize</code>. As you can see, <code>words</code> is a <code>PythonRDD</code>, but the execution should have happened instantaneously. Clearly, we haven't split the entire Shakespeare data set into a list of words yet.</p>

<p>If you've done the Hadoop "word count" using MapReduce, you'll know that the next steps are to map each word to a key value pair, where the key is the word and the value is a 1, and then use a reducer to sum the 1s for each key.</p>

<p>First, let's apply our map.</p>
<div class="highlight"><pre><span class="o">&gt;&gt;&gt;</span> <span class="n">wc</span> <span class="o">=</span> <span class="n">words</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span> <span class="n">wc</span><span class="o">.</span><span class="n">toDebugString</span><span class="p">()</span>
<span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="n">PythonRDD</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="n">at</span> <span class="n">RDD</span> <span class="n">at</span> <span class="n">PythonRDD</span><span class="o">.</span><span class="n">scala</span><span class="p">:</span><span class="mi">43</span>
<span class="o">|</span>  <span class="n">shakespeare</span><span class="o">.</span><span class="n">txt</span> <span class="n">MappedRDD</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="n">at</span> <span class="n">textFile</span> <span class="n">at</span> <span class="n">NativeMethodAccessorImpl</span><span class="o">.</span><span class="n">java</span><span class="p">:</span><span class="o">-</span><span class="mi">2</span>
<span class="o">|</span>  <span class="n">shakespeare</span><span class="o">.</span><span class="n">txt</span> <span class="n">HadoopRDD</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="n">at</span> <span class="n">textFile</span> <span class="n">at</span> <span class="n">NativeMethodAccessorImpl</span><span class="o">.</span><span class="n">java</span><span class="p">:</span><span class="o">-</span><span class="mi">2</span>
</pre></div>
<p>Instead of using a named function, we will use an anonymous function (with the <code>lambda</code> keyword in Python). This line of code will map the lambda to each element of words. Therefore, each <code>x</code> is a word, and the word will be transformed into a tuple (word, 1) by the anonymous closure. In order to inspect the lineage so far, we can use the <code>toDebugString</code> method to see how our <code>PipelinedRDD</code> is being transformed. We can then apply the <code>reduceByKey</code> action to get our word counts and then write those word counts to disk.</p>
<div class="highlight"><pre><span class="o">&gt;&gt;&gt;</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">wc</span><span class="o">.</span><span class="n">reduceByKey</span><span class="p">(</span><span class="n">add</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">counts</span><span class="o">.</span><span class="n">saveAsTextFile</span><span class="p">(</span><span class="s">"wc"</span><span class="p">)</span>
</pre></div>
<p>Once we finally invoke the action <code>saveAsTextFile</code>,  the distributed job kicks off and you should see a lot of <code>INFO</code> statements as the job runs "across the cluster" (or simply as multiple processes on your local machine). If you exit the interpreter, you should see a directory called "wc" in your current working directory.</p>
<div class="highlight"><pre><span class="nv">$ </span>ls wc/
_SUCCESS   part-00000 part-00001
</pre></div>
<p>Each part file represents a partition of the final RDD that was computed by various processes on your computer and saved to disk. If you use the <code>head</code> command on one of the part files, you should see tuples of word count pairs.</p>
<div class="highlight"><pre><span class="nv">$ </span>head wc/part-00000
<span class="o">(</span>u<span class="s1">'fawn'</span>, 14<span class="o">)</span>
<span class="o">(</span>u<span class="s1">'Fame.'</span>, 1<span class="o">)</span>
<span class="o">(</span>u<span class="s1">'Fame,'</span>, 2<span class="o">)</span>
<span class="o">(</span>u<span class="s1">'kinghenryviii@7731'</span>, 1<span class="o">)</span>
<span class="o">(</span>u<span class="s1">'othello@36737'</span>, 1<span class="o">)</span>
<span class="o">(</span>u<span class="s1">'loveslabourslost@51678'</span>, 1<span class="o">)</span>
<span class="o">(</span>u<span class="s1">'1kinghenryiv@54228'</span>, 1<span class="o">)</span>
<span class="o">(</span>u<span class="s1">'troilusandcressida@83747'</span>, 1<span class="o">)</span>
<span class="o">(</span>u<span class="s1">'fleeces'</span>, 1<span class="o">)</span>
<span class="o">(</span>u<span class="s1">'midsummersnightsdream@71681'</span>, 1<span class="o">)</span>
</pre></div>
<p>Note that none of the keys are sorted as they would be in Hadoop (due to a necessary shuffle and sort phase between the Map and Reduce tasks). However, you are guaranteed that each key appears only once across all part files as you used the <code>reduceByKey</code> operator on the counts RDD. If you want, you could use the <code>sort</code> operator to ensure that all the keys are sorted before writing them to disk.</p>

<h2 id="toc_8">Writing a Spark Application</h2>

<p>Writing Spark applications is similar to working with Spark in the interactive console. The API is the same. First, you need to get access to the <code>SparkContext</code>, which was automatically loaded for you by the <code>pyspark</code> application.</p>

<p>A basic template for writing a Spark application in Python is as follows:</p>
<div class="highlight"><pre><span class="c">## Spark Application - execute with spark-submit</span>

<span class="c">## Imports</span>
<span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkConf</span><span class="p">,</span> <span class="n">SparkContext</span>

<span class="c">## Module Constants</span>
<span class="n">APP_NAME</span> <span class="o">=</span> <span class="s">"My Spark Application"</span>

<span class="c">## Closure Functions</span>

<span class="c">## Main functionality</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">sc</span><span class="p">):</span>
    <span class="k">pass</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="c"># Configure Spark</span>
    <span class="n">conf</span> <span class="o">=</span> <span class="n">SparkConf</span><span class="p">()</span><span class="o">.</span><span class="n">setAppName</span><span class="p">(</span><span class="n">APP_NAME</span><span class="p">)</span>
    <span class="n">conf</span> <span class="o">=</span> <span class="n">conf</span><span class="o">.</span><span class="n">setMaster</span><span class="p">(</span><span class="s">"local[*]"</span><span class="p">)</span>
    <span class="n">sc</span>   <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="n">conf</span><span class="o">=</span><span class="n">conf</span><span class="p">)</span>

    <span class="c"># Execute Main functionality</span>
    <span class="n">main</span><span class="p">(</span><span class="n">sc</span><span class="p">)</span>
</pre></div>
<p>This template gives you a sense of what is needed in a Spark application: imports for various Python libraries, module constants, an identifying application name for debugging and for the Spark UI, closures or other custom operation functions, and finally, some main analytical methodology that is run as the driver. In our <code>ifmain</code>, we create the <code>SparkContext</code> and execute main with the context as configured. This will allow us to easily import driver code into the <code>pyspark</code> context without execution. Note that here a Spark configuration is hard coded into the <code>SparkConf</code> via the <code>setMaster</code> method, but typically you would just allow this value to be configured from the command line, so you will see this line commented out.</p>

<p>To close or exit the program use <code>sc.stop()</code> or <code>sys.exit(0)</code>.</p>

<p>In order to demonstrate a common use of Spark, let's take a look at a common use case where we read in a CSV file of data and compute some aggregate statistic. In this case, we're looking at the <a href="http://bit.ly/1Dz76xB">on-time flight data set</a> from the U.S. Department of Transportation, recording all U.S. domestic flight departure and arrival times along with their departure and arrival delays for the month of April, 2014. I typically use this data set because one month is manageable for exploration, but the entire data set needs to be computed upon with a cluster. The entire app is as follows:</p>
<div class="highlight"><pre><span class="c">## Spark Application - execute with spark-submit</span>

<span class="c">## Imports</span>
<span class="kn">import</span> <span class="nn">csv</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">StringIO</span> <span class="kn">import</span> <span class="n">StringIO</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>
<span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">add</span><span class="p">,</span> <span class="n">itemgetter</span>
<span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkConf</span><span class="p">,</span> <span class="n">SparkContext</span>

<span class="c">## Module Constants</span>
<span class="n">APP_NAME</span> <span class="o">=</span> <span class="s">"Flight Delay Analysis"</span>
<span class="n">DATE_FMT</span> <span class="o">=</span> <span class="s">"%Y-%m-</span><span class="si">%d</span><span class="s">"</span>
<span class="n">TIME_FMT</span> <span class="o">=</span> <span class="s">"%H%M"</span>

<span class="n">fields</span>   <span class="o">=</span> <span class="p">(</span><span class="s">'date'</span><span class="p">,</span> <span class="s">'airline'</span><span class="p">,</span> <span class="s">'flightnum'</span><span class="p">,</span> <span class="s">'origin'</span><span class="p">,</span> <span class="s">'dest'</span><span class="p">,</span> <span class="s">'dep'</span><span class="p">,</span>
            <span class="s">'dep_delay'</span><span class="p">,</span> <span class="s">'arv'</span><span class="p">,</span> <span class="s">'arv_delay'</span><span class="p">,</span> <span class="s">'airtime'</span><span class="p">,</span> <span class="s">'distance'</span><span class="p">)</span>
<span class="n">Flight</span>   <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s">'Flight'</span><span class="p">,</span> <span class="n">fields</span><span class="p">)</span>

<span class="c">## Closure Functions</span>
<span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="n">row</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Parses a row and returns a named tuple.</span>
<span class="sd">    """</span>

    <span class="n">row</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">strptime</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">DATE_FMT</span><span class="p">)</span><span class="o">.</span><span class="n">date</span><span class="p">()</span>
    <span class="n">row</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span>  <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">strptime</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="n">TIME_FMT</span><span class="p">)</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">row</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span>  <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="mi">6</span><span class="p">])</span>
    <span class="n">row</span><span class="p">[</span><span class="mi">7</span><span class="p">]</span>  <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">strptime</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="mi">7</span><span class="p">],</span> <span class="n">TIME_FMT</span><span class="p">)</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">row</span><span class="p">[</span><span class="mi">8</span><span class="p">]</span>  <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="mi">8</span><span class="p">])</span>
    <span class="n">row</span><span class="p">[</span><span class="mi">9</span><span class="p">]</span>  <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="mi">9</span><span class="p">])</span>
    <span class="n">row</span><span class="p">[</span><span class="mi">10</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="mi">10</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">Flight</span><span class="p">(</span><span class="o">*</span><span class="n">row</span><span class="p">[:</span><span class="mi">11</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="n">line</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Operator function for splitting a line with csv module</span>
<span class="sd">    """</span>
    <span class="n">reader</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">reader</span><span class="p">(</span><span class="n">StringIO</span><span class="p">(</span><span class="n">line</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">reader</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">delays</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Show a bar chart of the total delay per airline</span>
<span class="sd">    """</span>
    <span class="n">airlines</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">delays</span><span class="p">]</span>
    <span class="n">minutes</span>  <span class="o">=</span> <span class="p">[</span><span class="n">d</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">delays</span><span class="p">]</span>
    <span class="n">index</span>    <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">xrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">airlines</span><span class="p">)))</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">axe</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="n">bars</span> <span class="o">=</span> <span class="n">axe</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">minutes</span><span class="p">)</span>

    <span class="c"># Add the total minutes to the right</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">air</span><span class="p">,</span> <span class="nb">min</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">airlines</span><span class="p">,</span> <span class="n">minutes</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">min</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">bars</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s">'#d9230f'</span><span class="p">)</span>
            <span class="n">axe</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s">" </span><span class="si">%0.0f</span><span class="s"> min"</span> <span class="o">%</span> <span class="nb">min</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="nb">min</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">idx</span><span class="o">+</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">va</span><span class="o">=</span><span class="s">'center'</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">bars</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s">'#469408'</span><span class="p">)</span>
            <span class="n">axe</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s">" </span><span class="si">%0.0f</span><span class="s"> min"</span> <span class="o">%</span> <span class="nb">min</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">idx</span><span class="o">+</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">va</span><span class="o">=</span><span class="s">'center'</span><span class="p">)</span>

    <span class="c"># Set the ticks</span>
    <span class="n">ticks</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="n">idx</span><span class="o">+</span> <span class="mf">0.5</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">index</span><span class="p">],</span> <span class="n">airlines</span><span class="p">)</span>
    <span class="n">xt</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="p">[</span><span class="s">' '</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">xt</span><span class="p">))</span>

    <span class="c"># minimize chart junk</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="s">'x'</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span><span class="s">'white'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'-'</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Total Minutes Delayed per Airline'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c">## Main functionality</span>
<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">sc</span><span class="p">):</span>

    <span class="c"># Load the airlines lookup dictionary</span>
    <span class="n">airlines</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s">"ontime/airlines.csv"</span><span class="p">)</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">split</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>

    <span class="c"># Broadcast the lookup dictionary to the cluster</span>
    <span class="n">airline_lookup</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">airlines</span><span class="p">)</span>

    <span class="c"># Read the CSV Data into an RDD</span>
    <span class="n">flights</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s">"ontime/flights.csv"</span><span class="p">)</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">split</span><span class="p">)</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">parse</span><span class="p">)</span>

    <span class="c"># Map the total delay to the airline (joined using the broadcast value)</span>
    <span class="n">delays</span>  <span class="o">=</span> <span class="n">flights</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">f</span><span class="p">:</span> <span class="p">(</span><span class="n">airline_lookup</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="n">f</span><span class="o">.</span><span class="n">airline</span><span class="p">],</span>
                                     <span class="n">add</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">dep_delay</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">arv_delay</span><span class="p">)))</span>

    <span class="c"># Reduce the total delay for the month to the airline</span>
    <span class="n">delays</span>  <span class="o">=</span> <span class="n">delays</span><span class="o">.</span><span class="n">reduceByKey</span><span class="p">(</span><span class="n">add</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
    <span class="n">delays</span>  <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">delays</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">itemgetter</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

    <span class="c"># Provide output from the driver</span>
    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">delays</span><span class="p">:</span>
        <span class="k">print</span> <span class="s">"</span><span class="si">%0.0f</span><span class="s"> minutes delayed</span><span class="se">\t</span><span class="si">%s</span><span class="s">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c"># Show a bar chart of the delays</span>
    <span class="n">plot</span><span class="p">(</span><span class="n">delays</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="c"># Configure Spark</span>
    <span class="n">conf</span> <span class="o">=</span> <span class="n">SparkConf</span><span class="p">()</span><span class="o">.</span><span class="n">setMaster</span><span class="p">(</span><span class="s">"local[*]"</span><span class="p">)</span>
    <span class="n">conf</span> <span class="o">=</span> <span class="n">conf</span><span class="o">.</span><span class="n">setAppName</span><span class="p">(</span><span class="n">APP_NAME</span><span class="p">)</span>
    <span class="n">sc</span>   <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="n">conf</span><span class="o">=</span><span class="n">conf</span><span class="p">)</span>

    <span class="c"># Execute Main functionality</span>
    <span class="n">main</span><span class="p">(</span><span class="n">sc</span><span class="p">)</span>
</pre></div>
<p>To run this code (presuming that you have a directory called ontime with the two CSV files in the same directory), use the <code>spark-submit</code> command as follows:</p>

<p>This will create a Spark job using the localhost as the master, and look for the two CSV files in an ontime directory that is in the same directory as <code>app.py</code>. The final result shows that the total delays (in minutes) for the month of April go from arriving early if you're flying out of the continental U.S. to Hawaii or Alaska to an aggregate total delay for most big airlines. Note especially that we can visualize the result using <code>matplotlib</code> directly on the driver program, <code>app.py</code>:</p>

<p><img alt="Airline delays" src="https://silvrback.s3.amazonaws.com/uploads/ae516a51-06ae-400d-a93d-5c2ba3d3590f/delays_large.png"/></p>

<p>So what is this code doing? Let's look particularly at the <code>main</code> function which does the work most directly related to Spark. First, we load up a CSV file into an RDD, then map the <code>split</code> function to it. The <code>split</code> function parses each line of text using the <code>csv</code> module and returns a tuple that represents the row. Finally we pass the <code>collect</code> action to the RDD, which brings the data from the RDD back to the driver as a Python list. In this case, <code>airlines.csv</code> is a small jump table that will allow us to join airline codes with the airline full name. We will store this jump table as a Python dictionary and then broadcast it to every node in the cluster using <code>sc.broadcast</code>.</p>

<p>Next, the <code>main</code> function loads the much larger <code>flights.csv</code>. After splitting the CSV rows, we map the <code>parse</code> function to the CSV row, which converts dates and times to Python dates and times, and casts floating point numbers appropriately. It also stores the row as a <code>NamedTuple</code> called <code>Flight</code> for efficient ease of use.</p>

<p>With an RDD of <code>Flight</code> objects in hand, we map an anonymous function that transforms the RDD to a series of key-value pairs where the key is the name of the airline and the value is the sum of the arrival and departure delays. Each airline has its delay summed together using the <code>reduceByKey</code> action and the <code>add</code> operator, and this RDD is collected back to the driver (again the number airlines in the data is relatively small). Finally the delays are sorted in ascending order, then the output is printed to the console as well as visualized using <code>matplotlib</code>.</p>

<p>This example is kind of long, but hopefully it illustrates the interplay of the cluster and the driver program (sending out for analytics, then bringing results back to the driver) as well as the role of Python code in a Spark application.</p>

<h2 id="toc_9">Conclusion</h2>

<p>Although far from a complete introduction to Spark, we hope that you have a better feel for what Spark is, and how to conduct fast, in-memory distributed computing with Python. At the very least, you should be able to get Spark up and running and start exploring data either on your local machine in stand alone mode or via Amazon EC2. You should even be able to get iPython notebook set up and configured to run Spark!</p>

<p>Spark doesn't solve the distributed storage problem (usually Spark gets its data from HDFS), but it does provide a rich functional programming API for distributed computation. This framework is built upon the idea of <em>resilient distributed datasets</em> or "RDDs" for short. RDDs are a programming abstraction that represents a partitioned collection of objects, allowing for distributed operations to be performed upon them. RDDs are fault-tolerant (the resilient part) and, most importantly, can be stored in memory on worker nodes for immediate reuse. In memory storage provides for faster and more easily expressed iterative algorithms as well as enabling real-time interactive analyses.</p>

<p>Because the Spark library has an API available in Python, Scala, and Java, as well as built-in modules for machine learning, streaming data, graph algorithms, and SQL-like queries; it has rapidly become one of the most important distributed computation frameworks that exists today. When coupled with YARN, Spark serves to <em>augment</em> not replace existing Hadoop clusters and will be an important part of Big Data in the future, opening up new avenues of data science exploration.</p>

<h2 id="toc_10">Helpful Links</h2>

<p>Hopefully you've enjoyed this post! Writing never happens in a vacuum, so here are a few helpful links that helped me write the post; ones that you might want to review to explore Spark further. Note that some of the book links are affiliate links, meaning that if you click on them and purchase, you're helping to support District Data Labs!</p>

<p>This was more of an introductory post than is typical for District Data Labs articles , but there are some data and code associated with the introduction that you can find here:</p>



<p>District Data Labs also regularly holds a <a href="https://districtdatalabs.squarespace.com/fast-data-apps-with-spark-and-python">Fast Data Applications with Spark &amp; Python</a> workshop. If you're interested in learning more about Spark, you should definitely consider attending. For more information about that and to see when it is being offered next, check out the <a href="https://districtdatalabs.squarespace.com/fast-data-apps-with-spark-and-python">course page</a>. </p>

<h3 id="toc_11">Spark Papers</h3>

<p>Like Hadoop, Spark has some fundamental papers that I believe should be required reading for serious data scientists that need to do distributed computing on large data sets. The first is a workshop paper from HotOS (hot topics in operating systems) that describes Spark in an easily understandable fashion. The second is a more theoretical paper that describes RDDs in detail.</p>

<ol>
<li><p>M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, and I. Stoica, “<a href="http://www.cs.berkeley.edu/%7Ematei/papers/2010/hotcloud_spark.pdf">Spark: cluster computing with working sets</a>,” in Proceedings of the 2nd USENIX conference on Hot topics in cloud computing, 2010, pp. 10–10.</p></li>
<li><p>M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauley, M. J. Franklin, S. Shenker, and I. Stoica, “<a href="https://www.cs.berkeley.edu/%7Ematei/papers/2012/nsdi_spark.pdf">Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing</a>,” in Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation, 2012, pp. 2–2.</p></li>
</ol>

<h3 id="toc_12">Books on Spark</h3>

<ol>
<li><a href="http://amzn.to/15NRz1X">Learning Spark</a></li>
<li><a href="http://amzn.to/1zMiUKG">Advanced Analytics with Spark</a></li>
</ol>

<h3 id="toc_13">Helpful Blog Posts</h3>

<ol>
<li><a href="http://nbviewer.ipython.org/gist/fperez/6384491/00-Setup-IPython-PySpark.ipynb">Setting up IPython with PySpark</a></li>
<li><a href="http://databricks.gitbooks.io/databricks-spark-reference-applications/">Databricks Spark Reference Applications</a></li>
<li><a href="https://spark.apache.org/docs/latest/ec2-scripts.html">Running Spark on EC2</a></li>
<li><a href="https://aws.amazon.com/articles/Elastic-MapReduce/4926593393724923">Run Spark and SparkSQL on Amazon Elastic MapReduce</a></li>
</ol>

<p>Once again, thanks to <a href="https://twitter.com/genomegeek">@genomegeek</a> for his contributions to this post!</p>

        </div>
        </div></body></html>