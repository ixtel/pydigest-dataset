<html><body><div><article class="markdown-body entry-content" itemprop="text"><p>Tools for working with word frequencies from various corpora.</p>

<p>Author: Rob Speer</p>

<h2><a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Installation</h2>

<p>wordfreq requires Python 3 and depends on a few other Python modules
(msgpack-python, langcodes, and ftfy). You can install it and its dependencies
in the usual way, either by getting it from pip:</p>

<pre><code>pip3 install wordfreq
</code></pre>

<p>or by getting the repository and running its setup.py:</p>

<pre><code>python3 setup.py install
</code></pre>

<p>Japanese and Chinese have additional external dependencies so that they can be
tokenized correctly.</p>

<p>To be able to look up word frequencies in Japanese, you need to additionally
install mecab-python3, which itself depends on libmecab-dev and its dictionary.
These commands will install them on Ubuntu:</p>

<pre><code>sudo apt-get install mecab-ipadic-utf8 libmecab-dev
pip3 install mecab-python3
</code></pre>

<p>To be able to look up word frequencies in Chinese, you need Jieba, a
pure-Python Chinese tokenizer:</p>

<pre><code>pip3 install jieba
</code></pre>

<p>These dependencies can also be requested as options when installing wordfreq.
For example:</p>

<pre><code>pip3 install wordfreq[mecab,jieba]
</code></pre>

<h2><a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Usage</h2>

<p>wordfreq provides access to estimates of the frequency with which a word is
used, in 18 languages (see <em>Supported languages</em> below). It loads
efficiently-packed data structures that contain all words that appear at least
once per million words.</p>

<p>The most useful function is:</p>

<pre><code>word_frequency(word, lang, wordlist='combined', minimum=0.0)
</code></pre>

<p>This function looks up a word's frequency in the given language, returning its
frequency as a decimal between 0 and 1. In these examples, we'll multiply the
frequencies by a million (1e6) to get more readable numbers:</p>

<pre><code>&gt;&gt;&gt; from wordfreq import word_frequency
&gt;&gt;&gt; word_frequency('cafe', 'en') * 1e6
14.45439770745928

&gt;&gt;&gt; word_frequency('café', 'en') * 1e6
4.7863009232263805

&gt;&gt;&gt; word_frequency('cafe', 'fr') * 1e6
2.0417379446695274

&gt;&gt;&gt; word_frequency('café', 'fr') * 1e6
77.62471166286912
</code></pre>

<p>The parameters are:</p>

<ul>
<li><p><code>word</code>: a Unicode string containing the word to look up. Ideally the word
is a single token according to our tokenizer, but if not, there is still
hope -- see <em>Tokenization</em> below.</p></li>
<li><p><code>lang</code>: the BCP 47 or ISO 639 code of the language to use, such as 'en'.</p></li>
<li><p><code>wordlist</code>: which set of word frequencies to use. Current options are
'combined', which combines up to five different sources, and
'twitter', which returns frequencies observed on Twitter alone.</p></li>
<li><p><code>minimum</code>: If the word is not in the list or has a frequency lower than
<code>minimum</code>, return <code>minimum</code> instead. In some applications, you'll want
to set <code>minimum=1e-6</code> to avoid a discontinuity where the list ends, because
a frequency of 1e-6 (1 per million) is the threshold for being included in
the list at all.</p></li>
</ul>

<p>Other functions:</p>

<p><code>tokenize(text, lang)</code> splits text in the given language into words, in the same
way that the words in wordfreq's data were counted in the first place. See
<em>Tokenization</em>. Tokenizing Japanese requires the optional dependency <code>mecab-python3</code>
to be installed.</p>

<p><code>top_n_list(lang, n, wordlist='combined')</code> returns the most common <em>n</em> words in
the list, in descending frequency order.</p>

<pre><code>&gt;&gt;&gt; from wordfreq import top_n_list
&gt;&gt;&gt; top_n_list('en', 10)
['the', 'of', 'to', 'in', 'and', 'a', 'i', 'you', 'is', 'it']

&gt;&gt;&gt; top_n_list('es', 10)
['de', 'la', 'que', 'el', 'en', 'y', 'a', 'no', 'los', 'es']
</code></pre>

<p><code>iter_wordlist(lang, wordlist='combined')</code> iterates through all the words in a
wordlist, in descending frequency order.</p>

<p><code>get_frequency_dict(lang, wordlist='combined')</code> returns all the frequencies in
a wordlist as a dictionary, for cases where you'll want to look up a lot of
words and don't need the wrapper that <code>word_frequency</code> provides.</p>

<p><code>supported_languages(wordlist='combined')</code> returns a dictionary whose keys are
language codes, and whose values are the data file that will be loaded to
provide the requested wordlist in each language.</p>

<p><code>random_words(lang='en', wordlist='combined', nwords=5, bits_per_word=12)</code>
returns a selection of random words, separated by spaces. <code>bits_per_word=n</code>
will select each random word from 2^n words.</p>

<p>If you happen to want an easy way to get <a href="https://xkcd.com/936/">a memorable, xkcd-style
password</a> with 60 bits of entropy, this function will almost do the
job. In this case, you should actually run the similar function <code>random_ascii_words</code>,
limiting the selection to words that can be typed in ASCII.</p>

<h2><a id="user-content-sources-and-supported-languages" class="anchor" href="#sources-and-supported-languages" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Sources and supported languages</h2>

<p>We compiled word frequencies from seven different sources, providing us
examples of word usage on different topics at different levels of formality.
The sources (and the abbreviations we'll use for them) are:</p>

<ul>
<li><strong>LeedsIC</strong>: The Leeds Internet Corpus</li>
<li><strong>SUBTLEX</strong>: The SUBTLEX word frequency lists</li>
<li><strong>OpenSub</strong>: Data derived from OpenSubtitles but not from SUBTLEX</li>
<li><strong>Twitter</strong>: Messages sampled from Twitter's public stream</li>
<li><strong>Wpedia</strong>: The full text of Wikipedia in 2015</li>
<li><strong>Other</strong>: We get additional English frequencies from Google Books Syntactic
Ngrams 2013, and Chinese frequencies from the frequency dictionary that
comes with the Jieba tokenizer.</li>
</ul>

<p>The following 17 languages are well-supported, with reasonable tokenization and
at least 3 different sources of word frequencies:</p>

<pre><code>Language    Code    SUBTLEX OpenSub LeedsIC Twitter Wpedia  Other
──────────────────┼─────────────────────────────────────────────────────
Arabic      ar    │ -       Yes     Yes     Yes     Yes     -
German      de    │ Yes     -       Yes     Yes[1]  Yes     -
Greek       el    │ -       Yes     Yes     Yes     Yes     -
English     en    │ Yes     Yes     Yes     Yes     Yes     Google Books
Spanish     es    │ -       Yes     Yes     Yes     Yes     -
French      fr    │ -       Yes     Yes     Yes     Yes     -
Indonesian  id    │ -       Yes     -       Yes     Yes     -
Italian     it    │ -       Yes     Yes     Yes     Yes     -
Japanese    ja    │ -       -       Yes     Yes     Yes     -
Malay       ms    │ -       Yes     -       Yes     Yes     -
Dutch       nl    │ Yes     Yes     -       Yes     Yes     -
Polish      pl    │ -       Yes     -       Yes     Yes     -
Portuguese  pt    │ -       Yes     Yes     Yes     Yes     -
Russian     ru    │ -       Yes     Yes     Yes     Yes     -
Swedish     sv    │ -       Yes     -       Yes     Yes     -
Turkish     tr    │ -       Yes     -       Yes     Yes     -
Chinese     zh    │ Yes     -       Yes     -       -       Jieba
</code></pre>

<p>Additionally, Korean is marginally supported. You can look up frequencies in
it, but we have too few data sources for it so far:</p>

<pre><code>Language    Code    SUBTLEX OpenSub LeedsIC Twitter Wpedia
──────────────────┼───────────────────────────────────────
Korean      ko    │ -       -       -       Yes     Yes
</code></pre>

<p>[1] We've counted the frequencies from tweets in German, such as they are, but
you should be aware that German is not a frequently-used language on Twitter.
Germans just don't tweet that much.</p>

<h2><a id="user-content-tokenization" class="anchor" href="#tokenization" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Tokenization</h2>

<p>wordfreq uses the Python package <code>regex</code>, which is a more advanced
implementation of regular expressions than the standard library, to
separate text into tokens that can be counted consistently. <code>regex</code>
produces tokens that follow the recommendations in <a href="http://unicode.org/reports/tr29/">Unicode
Annex #29, Text Segmentation</a>.</p>

<p>There are language-specific exceptions:</p>

<ul>
<li>In Arabic, it additionally normalizes ligatures and removes combining marks.</li>
<li>In Japanese, instead of using the regex library, it uses the external library
<code>mecab-python3</code>. This is an optional dependency of wordfreq, and compiling
it requires the <code>libmecab-dev</code> system package to be installed.</li>
<li>In Chinese, it uses the external Python library <code>jieba</code>, another optional
dependency.</li>
</ul>

<p>When wordfreq's frequency lists are built in the first place, the words are
tokenized according to this function.</p>

<p>Because tokenization in the real world is far from consistent, wordfreq will
also try to deal gracefully when you query it with texts that actually break
into multiple tokens:</p>

<pre><code>&gt;&gt;&gt; word_frequency('New York', 'en')
0.0002315934248950231
&gt;&gt;&gt; word_frequency('北京地铁', 'zh')  # "Beijing Subway"
3.2187603965715087e-06
</code></pre>

<p>The word frequencies are combined with the half-harmonic-mean function in order
to provide an estimate of what their combined frequency would be. In Chinese,
where the word breaks must be inferred from the frequency of the resulting
words, there is also a penalty to the word frequency for each word break that
must be inferred.</p>

<p>This method of combining word frequencies implicitly assumes that you're asking
about words that frequently appear together. It's not multiplying the
frequencies, because that would assume they are statistically unrelated. So if
you give it an uncommon combination of tokens, it will hugely over-estimate
their frequency:</p>

<pre><code>&gt;&gt;&gt; word_frequency('owl-flavored', 'en')
1.3557098723512335e-06
</code></pre>

<h2><a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>License</h2>

<p><code>wordfreq</code> is freely redistributable under the MIT license (see
<code>MIT-LICENSE.txt</code>), and it includes data files that may be
redistributed under a Creative Commons Attribution-ShareAlike 4.0
license (<a href="https://creativecommons.org/licenses/by-sa/4.0/">https://creativecommons.org/licenses/by-sa/4.0/</a>).</p>

<p><code>wordfreq</code> contains data extracted from Google Books Ngrams
(<a href="http://books.google.com/ngrams">http://books.google.com/ngrams</a>) and Google Books Syntactic Ngrams
(<a href="http://commondatastorage.googleapis.com/books/syntactic-ngrams/index.html">http://commondatastorage.googleapis.com/books/syntactic-ngrams/index.html</a>).
The terms of use of this data are:</p>

<pre><code>Ngram Viewer graphs and data may be freely used for any purpose, although
acknowledgement of Google Books Ngram Viewer as the source, and inclusion
of a link to http://books.google.com/ngrams, would be appreciated.
</code></pre>

<p>It also contains data derived from the following Creative Commons-licensed
sources:</p>



<p>It contains data from various SUBTLEX word lists: SUBTLEX-US, SUBTLEX-UK,
SUBTLEX-CH, SUBTLEX-DE, and SUBTLEX-NL, created by Marc Brysbaert et al.
(see citations below) and available at
<a href="http://crr.ugent.be/programs-data/subtitle-frequencies">http://crr.ugent.be/programs-data/subtitle-frequencies</a>.</p>

<p>I (Rob Speer) have obtained permission by e-mail from Marc Brysbaert to
distribute these wordlists in wordfreq, to be used for any purpose, not just
for academic use, under these conditions:</p>

<ul>
<li>Wordfreq and code derived from it must credit the SUBTLEX authors.</li>
<li>It must remain clear that SUBTLEX is freely available data.</li>
</ul>

<p>These terms are similar to the Creative Commons Attribution-ShareAlike license.</p>

<p>Some additional data was collected by a custom application that watches the
streaming Twitter API, in accordance with Twitter's Developer Agreement &amp;
Policy. This software gives statistics about words that are commonly used on
Twitter; it does not display or republish any Twitter content.</p>

<h2><a id="user-content-citations-to-work-that-wordfreq-is-built-on" class="anchor" href="#citations-to-work-that-wordfreq-is-built-on" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Citations to work that wordfreq is built on</h2>

<ul>
<li><p>Brysbaert, M. &amp; New, B. (2009). Moving beyond Kucera and Francis: A Critical
Evaluation of Current Word Frequency Norms and the Introduction of a New and
Improved Word Frequency Measure for American English. Behavior Research
Methods, 41 (4), 977-990.
<a href="http://sites.google.com/site/borisnew/pub/BrysbaertNew2009.pdf">http://sites.google.com/site/borisnew/pub/BrysbaertNew2009.pdf</a></p></li>
<li><p>Brysbaert, M., Buchmeier, M., Conrad, M., Jacobs, A. M., Bölte, J., &amp; Böhl, A.
(2015). The word frequency effect. Experimental Psychology.
<a href="http://econtent.hogrefe.com/doi/abs/10.1027/1618-3169/a000123?journalCode=zea">http://econtent.hogrefe.com/doi/abs/10.1027/1618-3169/a000123?journalCode=zea</a></p></li>
<li><p>Brysbaert, M., Buchmeier, M., Conrad, M., Jacobs, A.M., Bölte, J., &amp; Böhl, A.
(2011). The word frequency effect: A review of recent developments and
implications for the choice of frequency estimates in German. Experimental
Psychology, 58, 412-424.</p></li>
<li><p>Cai, Q., &amp; Brysbaert, M. (2010). SUBTLEX-CH: Chinese word and character
frequencies based on film subtitles. PLoS One, 5(6), e10729.
<a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0010729">http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0010729</a></p></li>
<li><p>Dave, H. (2011). Frequency word lists.
<a href="https://invokeit.wordpress.com/frequency-word-lists/">https://invokeit.wordpress.com/frequency-word-lists/</a></p></li>
<li><p>Davis, M. (2012). Unicode text segmentation. Unicode Standard Annex, 29.
<a href="http://unicode.org/reports/tr29/">http://unicode.org/reports/tr29/</a></p></li>
<li><p>Keuleers, E., Brysbaert, M. &amp; New, B. (2010). SUBTLEX-NL: A new frequency
measure for Dutch words based on film subtitles. Behavior Research Methods,
42(3), 643-650.
<a href="http://crr.ugent.be/papers/SUBTLEX-NL_BRM.pdf">http://crr.ugent.be/papers/SUBTLEX-NL_BRM.pdf</a></p></li>
<li><p>Kudo, T. (2005). Mecab: Yet another part-of-speech and morphological
analyzer.
<a href="http://mecab.sourceforge.net/">http://mecab.sourceforge.net/</a></p></li>
<li><p>van Heuven, W. J., Mandera, P., Keuleers, E., &amp; Brysbaert, M. (2014).
SUBTLEX-UK: A new and improved word frequency database for British English.
The Quarterly Journal of Experimental Psychology, 67(6), 1176-1190.
<a href="http://www.tandfonline.com/doi/pdf/10.1080/17470218.2013.850521">http://www.tandfonline.com/doi/pdf/10.1080/17470218.2013.850521</a></p></li>
</ul>
</article>
  </div></body></html>