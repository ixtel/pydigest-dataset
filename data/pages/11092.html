<html><body><div><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-elephas-distributed-deep-learning-with-keras--spark-" class="anchor" href="#elephas-distributed-deep-learning-with-keras--spark-" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Elephas: Distributed Deep Learning with Keras &amp; Spark <a href="https://travis-ci.org/maxpumperla/elephas"><img src="https://camo.githubusercontent.com/8bea5946f16ee0596d55f7837015b5d81bb32247/68747470733a2f2f7472617669732d63692e6f72672f6d617870756d7065726c612f656c65706861732e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/maxpumperla/elephas.svg?branch=master"/></a></h1>

<p>Elephas is an extension of <a href="http://keras.io">Keras</a>, which allows you to run distributed deep learning models at scale with <a href="http://spark.apache.org">Spark</a>. Schematically, elephas works as follows.</p>

<p><a href="/maxpumperla/elephas/blob/master/elephas.gif" target="_blank"><img src="/maxpumperla/elephas/raw/master/elephas.gif" alt="Elephas"/></a></p>

<h2><a id="user-content-introduction" class="anchor" href="#introduction" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Introduction</h2>

<p>Elephas brings deep learning with <a href="http://keras.io">Keras</a> to <a href="http://spark.apache.org">Spark</a>. Elephas intends to keep the simplicity and high usability of Keras, thereby allowing for fast prototyping of distributed models, which can be run on massive data sets. For an introductory example, see the following <a href="https://github.com/maxpumperla/elephas/blob/master/examples/Spark_ML_Pipeline.ipynb">iPython notebook</a>.</p>

<p>ἐλέφας is Greek for <em>ivory</em> and an accompanying project to κέρας, meaning <em>horn</em>. If this seems weird mentioning, like a bad dream, you should confirm it actually is at the <a href="https://github.com/fchollet/keras/blob/master/README.md">Keras documentation</a>. Elephas also means <em>elephant</em>, as in stuffed yellow elephant.</p>

<p>Elephas implements a class of data-parallel algorithms on top of Keras, using Spark's RDDs and data frames. Keras Models are initialized on the driver, then serialized and shipped to workers, alongside with data and broadcasted model parameters. Spark workers deserialize the model, train their chunk of data and send their gradients back to the driver. The "master" model on the driver is updated by an optimizer, which takes gradients either synchronously or asynchronously.</p>

<h2><a id="user-content-getting-started" class="anchor" href="#getting-started" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Getting started</h2>

<h3><a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Installation</h3>

<p>Depending on what OS you are using, you may need to install some prerequisite modules (LAPACK, BLAS, fortran compiler) first.</p>

<p>For example, on Ubuntu Linux:</p>

<pre><code>sudo apt-get install liblapack-dev libblas-dev gfortran
</code></pre>

<p>Install elephas from PyPI with </p>

<pre><code>pip install elephas
</code></pre>

<p>A quick way to install Spark locally is to use homebrew on Mac like this</p>

<pre><code>brew install spark
</code></pre>

<p>or linuxbrew on linux.</p>

<pre><code>brew install apache-spark
</code></pre>

<p>The brew version of Spark is outdated, so we recommend installing Spark as follows:</p>

<pre><code>wget http://apache.mirrors.tds.net/spark/spark-1.5.2/spark-1.5.2-bin-hadoop2.6.tgz -P ~
sudo tar zxvf ~/spark-* -C /usr/local
sudo mv /usr/local/spark-* /usr/local/spark 
</code></pre>

<p>After that, make sure to put these path variables to your shell profile (e.g. <code>~/.zshrc</code>):</p>

<pre><code>export SPARK_HOME=/usr/local/spark
export PATH=$PATH:$SPARK_HOME/bin
</code></pre>

<p>If this is not an option, you should simply follow the instructions at the <a href="http://spark.apache.org/downloads.html">Spark download section</a>.  </p>

<h3><a id="user-content-basic-example" class="anchor" href="#basic-example" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Basic example</h3>

<p>After installing both Elephas and Spark, training a model is done schematically as follows:</p>

<ul>
<li>Create a local pyspark context</li>
</ul>

<div class="highlight highlight-source-python"><pre><span class="pl-k">from</span> pyspark <span class="pl-k">import</span> SparkContext, SparkConf
conf <span class="pl-k">=</span> SparkConf().setAppName(<span class="pl-s"><span class="pl-pds">'</span>Elephas_App<span class="pl-pds">'</span></span>).setMaster(<span class="pl-s"><span class="pl-pds">'</span>local[8]<span class="pl-pds">'</span></span>)
sc <span class="pl-k">=</span> SparkContext(<span class="pl-v">conf</span><span class="pl-k">=</span>conf)</pre></div>

<ul>
<li>Define and compile a Keras model</li>
</ul>

<div class="highlight highlight-source-python"><pre>model <span class="pl-k">=</span> Sequential()
model.add(Dense(<span class="pl-c1">128</span>, <span class="pl-v">input_dim</span><span class="pl-k">=</span><span class="pl-c1">784</span>))
model.add(Activation(<span class="pl-s"><span class="pl-pds">'</span>relu<span class="pl-pds">'</span></span>))
model.add(Dropout(<span class="pl-c1">0.2</span>))
model.add(Dense(<span class="pl-c1">128</span>))
model.add(Activation(<span class="pl-s"><span class="pl-pds">'</span>relu<span class="pl-pds">'</span></span>))
model.add(Dropout(<span class="pl-c1">0.2</span>))
model.add(Dense(<span class="pl-c1">10</span>))
model.add(Activation(<span class="pl-s"><span class="pl-pds">'</span>softmax<span class="pl-pds">'</span></span>))
model.compile(<span class="pl-v">loss</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>categorical_crossentropy<span class="pl-pds">'</span></span>, <span class="pl-v">optimizer</span><span class="pl-k">=</span>SGD())</pre></div>

<ul>
<li>Create an RDD from numpy arrays </li>
</ul>

<div class="highlight highlight-source-python"><pre><span class="pl-k">from</span> elephas.utils.rdd_utils <span class="pl-k">import</span> to_simple_rdd
rdd <span class="pl-k">=</span> to_simple_rdd(sc, <span class="pl-c1">X_train</span>, <span class="pl-c1">Y_train</span>)</pre></div>

<ul>
<li>A SparkModel is defined by passing Spark context and Keras model. Additionally, one has choose an optimizer used for updating the elephas model, an update frequency, a parallelization mode and the degree of parallelism, i.e. the number of workers.</li>
</ul>

<div class="highlight highlight-source-python"><pre><span class="pl-k">from</span> elephas.spark_model <span class="pl-k">import</span> SparkModel
<span class="pl-k">from</span> elephas <span class="pl-k">import</span> optimizers <span class="pl-k">as</span> elephas_optimizers

adagrad <span class="pl-k">=</span> elephas_optimizers.Adagrad()
spark_model <span class="pl-k">=</span> SparkModel(sc,model, <span class="pl-v">optimizer</span><span class="pl-k">=</span>adagrad, <span class="pl-v">frequency</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>epoch<span class="pl-pds">'</span></span>, <span class="pl-v">mode</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>asynchronous<span class="pl-pds">'</span></span>, <span class="pl-v">num_workers</span><span class="pl-k">=</span><span class="pl-c1">2</span>)
spark_model.train(rdd, <span class="pl-v">nb_epoch</span><span class="pl-k">=</span><span class="pl-c1">20</span>, <span class="pl-v">batch_size</span><span class="pl-k">=</span><span class="pl-c1">32</span>, <span class="pl-v">verbose</span><span class="pl-k">=</span><span class="pl-c1">0</span>, <span class="pl-v">validation_split</span><span class="pl-k">=</span><span class="pl-c1">0.1</span>, <span class="pl-v">num_workers</span><span class="pl-k">=</span><span class="pl-c1">8</span>)</pre></div>

<ul>
<li>Run your script using spark-submit</li>
</ul>

<pre><code>spark-submit --driver-memory 1G ./your_script.py
</code></pre>

<p>Increasing the driver memory even further may be necessary, as the set of parameters in a network may be very large and collecting them on the driver eats up a lot of resources. See the examples folder for a few working examples.</p>

<h3><a id="user-content-spark-mllib-example" class="anchor" href="#spark-mllib-example" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Spark MLlib example</h3>

<p>Following up on the last example, to create an RDD of LabeledPoints for supervised training from pairs of numpy arrays, use </p>

<div class="highlight highlight-source-python"><pre><span class="pl-k">from</span> elephas.utils.rdd_utils <span class="pl-k">import</span> to_labeled_point
lp_rdd <span class="pl-k">=</span> to_labeled_point(sc, <span class="pl-c1">X_train</span>, <span class="pl-c1">Y_train</span>, <span class="pl-v">categorical</span><span class="pl-k">=</span><span class="pl-c1">True</span>)</pre></div>

<p>Training a given LabeledPoint-RDD is very similar to what we've seen already</p>

<div class="highlight highlight-source-python"><pre><span class="pl-k">from</span> elephas.spark_model <span class="pl-k">import</span> SparkMLlibModel
adadelta <span class="pl-k">=</span> elephas_optimizers.Adadelta()
spark_model <span class="pl-k">=</span> SparkMLlibModel(sc,model, <span class="pl-v">optimizer</span><span class="pl-k">=</span>adadelta, <span class="pl-v">frequency</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>batch<span class="pl-pds">'</span></span>, <span class="pl-v">mode</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>hogwild<span class="pl-pds">'</span></span>, <span class="pl-v">num_workers</span><span class="pl-k">=</span><span class="pl-c1">2</span>)
spark_model.train(lp_rdd, <span class="pl-v">nb_epoch</span><span class="pl-k">=</span><span class="pl-c1">20</span>, <span class="pl-v">batch_size</span><span class="pl-k">=</span><span class="pl-c1">32</span>, <span class="pl-v">verbose</span><span class="pl-k">=</span><span class="pl-c1">0</span>, <span class="pl-v">validation_split</span><span class="pl-k">=</span><span class="pl-c1">0.1</span>, <span class="pl-v">categorical</span><span class="pl-k">=</span><span class="pl-c1">True</span>, <span class="pl-v">nb_classes</span><span class="pl-k">=</span>nb_classes)</pre></div>

<h3><a id="user-content-spark-ml-example" class="anchor" href="#spark-ml-example" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Spark ML example</h3>

<p>To train a model with a SparkML estimator on a data frame, use the following syntax.</p>

<div class="highlight highlight-source-python"><pre>df <span class="pl-k">=</span> to_data_frame(sc, <span class="pl-c1">X_train</span>, <span class="pl-c1">Y_train</span>, <span class="pl-v">categorical</span><span class="pl-k">=</span><span class="pl-c1">True</span>)
test_df <span class="pl-k">=</span> to_data_frame(sc, <span class="pl-c1">X_test</span>, <span class="pl-c1">Y_test</span>, <span class="pl-v">categorical</span><span class="pl-k">=</span><span class="pl-c1">True</span>)

adadelta <span class="pl-k">=</span> elephas_optimizers.Adadelta()
estimator <span class="pl-k">=</span> ElephasEstimator(sc,model, 
        <span class="pl-v">nb_epoch</span><span class="pl-k">=</span>nb_epoch, <span class="pl-v">batch_size</span><span class="pl-k">=</span>batch_size, <span class="pl-v">optimizer</span><span class="pl-k">=</span>adadelta, <span class="pl-v">frequency</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>batch<span class="pl-pds">'</span></span>, <span class="pl-v">mode</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>asynchronous<span class="pl-pds">'</span></span>, <span class="pl-v">num_workers</span><span class="pl-k">=</span><span class="pl-c1">2</span>,
        <span class="pl-v">verbose</span><span class="pl-k">=</span><span class="pl-c1">0</span>, <span class="pl-v">validation_split</span><span class="pl-k">=</span><span class="pl-c1">0.1</span>, <span class="pl-v">categorical</span><span class="pl-k">=</span><span class="pl-c1">True</span>, <span class="pl-v">nb_classes</span><span class="pl-k">=</span>nb_classes)

fitted_model <span class="pl-k">=</span> estimator.fit(df)</pre></div>

<p>Fitting an estimator results in a SparkML transformer, which we can use for predictions and other evaluations by calling the transform method on it.</p>

<div class="highlight highlight-source-python"><pre>prediction <span class="pl-k">=</span> fitted_model.transform(test_df)
pnl <span class="pl-k">=</span> prediction.select(<span class="pl-s"><span class="pl-pds">"</span>label<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>prediction<span class="pl-pds">"</span></span>)
pnl.show(<span class="pl-c1">100</span>)

prediction_and_label<span class="pl-k">=</span> pnl.map(<span class="pl-k">lambda</span> <span class="pl-smi">row</span>: (row.label, row.prediction))
metrics <span class="pl-k">=</span> MulticlassMetrics(prediction_and_label)
<span class="pl-c1">print</span>(metrics.precision())
<span class="pl-c1">print</span>(metrics.recall())</pre></div>

<h2><a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Usage</h2>

<p>In the first example above we have seen that an elephas model is instantiated like this</p>

<div class="highlight highlight-source-python"><pre>spark_model <span class="pl-k">=</span> SparkModel(sc,model, <span class="pl-v">optimizer</span><span class="pl-k">=</span>adagrad, <span class="pl-v">frequency</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>epoch<span class="pl-pds">'</span></span>, <span class="pl-v">mode</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>asynchronous<span class="pl-pds">'</span></span>, <span class="pl-v">num_workers</span><span class="pl-k">=</span><span class="pl-c1">2</span>)</pre></div>

<p>So, apart from the canonical Spark context and Keras model, Elephas models have four parameters to tune and we will describe each of them next.</p>

<h3><a id="user-content-model-updates-optimizers" class="anchor" href="#model-updates-optimizers" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Model updates (optimizers)</h3>

<p><code>optimizer</code>: The optimizers module in elephas is an adaption of the same module in keras, i.e. it provides the user with the following list of optimizers:</p>

<ul>
<li><code>SGD</code></li>
<li><code>RMSprop</code></li>
<li><code>Adagrad</code></li>
<li><code>Adadelta</code></li>
<li><code>Adam</code></li>
</ul>

<p>Once constructed, each of these can be passed to the <em>optimizer</em> parameter of the model. Updates in keras are computed with the help of theano, so most of the data structures in keras optimizers stem from theano. In elephas, gradients have already been computed by the respective workers, so it makes sense to entirely work with numpy arrays internally. </p>

<p>Note that in order to set up an elephas model, you have to specify two optimizers, one for elephas and one for the underlying keras model. Individual workers produce updates according to keras optimizers and the "master" model on the driver uses elephas optimizers to aggregate them. For starters, we recommend keras models with SGD and elephas models with Adagrad or Adadelta.</p>

<h3><a id="user-content-update-frequency" class="anchor" href="#update-frequency" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Update frequency</h3>

<p><code>frequency</code>: The user can decide how often updates are passed to the master model by controlling the <em>frequency</em> parameter. To update every batch, choose 'batch' and to update only after every epoch, choose 'epoch'.</p>

<h3><a id="user-content-update-mode" class="anchor" href="#update-mode" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Update mode</h3>

<p><code>mode</code>: Currently, there's three different modes available in elephas, each corresponding to a different heuristic or parallelization scheme adopted, which is controlled by the <em>mode</em> parameter. The default property is 'asynchronous'.</p>

<h4><a id="user-content-asynchronous-updates-with-read-and-write-locks-modeasynchronous" class="anchor" href="#asynchronous-updates-with-read-and-write-locks-modeasynchronous" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Asynchronous updates with read and write locks (<code>mode='asynchronous'</code>)</h4>

<p>This mode implements the algorithm described as <em>downpour</em> in [1], i.e. each worker can send updates whenever they are ready. The master model makes sure that no update gets lost, i.e. multiple updates get applied at the "same" time,  by locking the master parameters while reading and writing parameters. This idea has been used in Google's DistBelief framework. </p>

<h4><a id="user-content-asynchronous-updates-without-locks-modehogwild" class="anchor" href="#asynchronous-updates-without-locks-modehogwild" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Asynchronous updates without locks (<code>mode='hogwild'</code>)</h4>

<p>Essentially the same procedure as above, but without requiring the locks. This heuristic assumes that we still fare well enough, even if we loose an update here or there. Updating parameters lock-free in a non-distributed setting for SGD goes by the name 'Hogwild!' [2], it's distributed extension is called 'Dogwild!' [3].  </p>

<h4><a id="user-content-synchronous-updates-modesynchronous" class="anchor" href="#synchronous-updates-modesynchronous" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Synchronous updates (<code>mode='synchronous'</code>)</h4>

<p>In this mode each worker sends a new batch of parameter updates at the same time, which are then processed on the master. Accordingly, this algorithm is sometimes called <em>batch synchronous parallel</em> or just BSP.</p>

<h3><a id="user-content-degree-of-parallelization-number-of-workers" class="anchor" href="#degree-of-parallelization-number-of-workers" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Degree of parallelization (number of workers)</h3>

<p><code>num_workers</code>: Lastly, the degree to which we parallelize our training data is controlled by the parameter <em>num_workers</em>.</p>

<h2><a id="user-content-discussion" class="anchor" href="#discussion" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Discussion</h2>

<p>Premature parallelization may not be the root of all evil, but it may not always be the best idea to do so. Keep in mind that more workers mean less data per worker and parallelizing a model is not an excuse for actual learning. So, if you can perfectly well fit your data into memory <em>and</em> you're happy with training speed of the model consider just using keras. </p>

<p>One exception to this rule may be that you're already working within the Spark ecosystem and want to leverage what's there. The above SparkML example shows how to use evaluation modules from Spark and maybe you wish to further process the outcome of an elephas model down the road. In this case, we recommend to use elephas as a simple wrapper by setting num_workers=1.</p>

<p>Note that right now elephas restricts itself to data-parallel algorithms for two reasons. First, Spark simply makes it very easy to distribute data. Second, neither Spark nor Theano make it particularly easy to split up the actual model in parts, thus making model-parallelism practically impossible to realize.</p>

<p>Having said all that, we hope you learn to appreciate elephas as a pretty easy to setup and use playground for data-parallel deep-learning algorithms.</p>

<h2><a id="user-content-future-work--contributions" class="anchor" href="#future-work--contributions" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Future work &amp; contributions</h2>

<p>Constructive feedback and pull requests for elephas are very welcome. Here's a few things we're having in mind for future development </p>

<ul>
<li>Tighter Spark ML integration. Pipelines do not work yet.</li>
<li>Benchmarks for training speed and accuracy.</li>
<li>Some real-world tests on EC2 instances with large data sets like imagenet.</li>
</ul>

<h2><a id="user-content-literature" class="anchor" href="#literature" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Literature</h2>

<p>[1] J. Dean, G.S. Corrado, R. Monga, K. Chen, M. Devin, QV. Le, MZ. Mao, M’A. Ranzato, A. Senior, P. Tucker, K. Yang, and AY. Ng. <a href="http://research.google.com/archive/large_deep_networks_nips2012.html">Large Scale Distributed Deep Networks</a>.</p>

<p>[2] F. Niu, B. Recht, C. Re, S.J. Wright <a href="http://arxiv.org/abs/1106.5730">HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent</a></p>

<p>[3] C. Noel, S. Osindero. <a href="http://stanford.edu/%7Erezab/nips2014workshop/submits/dogwild.pdf">Dogwild! — Distributed Hogwild for CPU &amp; GPU</a></p>
</article>
  </div></body></html>