<html><body><div><div class="content html_format">
      <h4>Введение</h4><p>
Сегодня я продолжу рассказ о применении методов анализа данных и машинного обучения на практических примерах. В прошлой </p><a href="http://habrahabr.ru/post/204500/">статье</a><p> мы с вами разбирались с задачей кредитного скоринга. Ниже я попытаюсь продемонстрировать решение другой задачи с того же </p><a href="https://www.tcsbank.ru/tournament/">турнира</a><p>, а именно «Задачи о паспортах» (Задание №2).</p><p>
При решении будут показаны основы анализа текстовой информации, а также ее кодирование для построения модели с помощью Python и модулей для анализа данных (</p><a href="http://pandas.pydata.org/pandas-docs/stable/">pandas</a><p>, </p><a href="http://scikit-learn.org/">scikit-learn</a><p>, </p><a href="https://pymorphy2.readthedocs.org/en/latest/index.html">pymorphy</a><p>).
</p><a name="habracut"/>
<h4>Постановка задачи</h4><p>
При работе с большим объёмом данных важно поддерживать их чистоту. А при заполнении заявки на банковский продукт необходимо указывать полные паспортные данные, в том числе и поле «кем выдан паспорт», число различных вариантов написаний одного и того же отделения потенциальными клиентами может достигать нескольких сотен. Важно понимать, не ошибся ли клиент, заполняя другие поля: «код подразделения», «серию/номер паспорта». Для этого необходимо сверять «код подразделения» и «кем выдан паспорт».</p><p>
Задача заключается в том, чтобы проставить коды подразделений для записей из </p><a href="https://static.tcsbank.ru/documents/olymp/passport_test_set.csv">тестовой выборки</a><p>, основываясь на </p><a href="https://static.tcsbank.ru/documents/olymp/passport_training_set.csv">обучающей выборке</a><p>.

</p><h4>Предварительная обработка данных</h4><p>
Загрузим данные и посмотрим, что мы имеем:

</p><pre><code class="python">from pandas import read_csv
import pymorphy2
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.cross_validation import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, roc_auc_score
from sklearn.decomposition import PCA

train = read_csv('https://static.tcsbank.ru/documents/olymp/passport_training_set.csv',';', index_col='id' ,encoding='cp1251')
train.head(5)
</code></pre>
<table>
<tr>
<th/>
<th>passport_div_code</th>
<th>passport_issuer_name</th>
<th>passport_issue_month/year</th>
</tr>
<tr>
<th>id</th>
<th/>
<th/>
<th/>
</tr>
<tr>
<th>1</th>
<td>422008</td>
<td>БЕЛОВСКИМ УВД КЕМЕРОВСКОЙ ОБЛАСТИ</td>
<td>11M2001</td>
</tr>
<tr>
<th>2</th>
<td>500112</td>
<td>ТП №2 В ГОР. ОРЕХОВО-ЗУЕВО ОУФМС РОССИИ ПО МО ...</td>
<td>03M2009</td>
</tr>
<tr>
<th>3</th>
<td>642001</td>
<td>ВОЛЖСКИМ РОВД ГОР.САРАТОВА</td>
<td>04M2002</td>
</tr>
<tr>
<th>4</th>
<td>162004</td>
<td>УВД МОСКОВСКОГО РАЙОНА Г.КАЗАНЬ</td>
<td>12M2002</td>
</tr>
<tr>
<th>5</th>
<td>80001</td>
<td>ОТДЕЛОМ ОФМС РОССИИ ПО РЕСП КАЛМЫКИЯ В Г ЭЛИСТА</td>
<td>08M2009</td>
</tr>
</table><p>
Теперь можно посмотреть как пользователи записывают поле «кем выдан паспорт» на примере какого-либо подразделения:

</p><pre><code class="python">example_code = train.passport_div_code[train.passport_div_code.duplicated()].values[0]
for i in train.passport_issuer_name[train.passport_div_code == example_code].drop_duplicates():
    print i
</code></pre><p>
ОТДЕЛЕНИЕМ УФМС РОССИИ ПО РЕСПУБЛИКЕ КАРЕЛИЯ В МЕДВЕЖ. Р-Е</p><p>
ОТДЕЛЕНИЕМ УФМС РОССИИ ПО Р. КАРЕЛИЯ В МЕДВЕЖЬЕГОРСКОМ РАЙОНЕ</p><p>
ОТДЕЛЕНИЕМ УФМС РОССИИ ПО РЕСП КАРЕЛИЯ В МЕДВЕЖЬЕГОРСКОМ Р-НЕ</p><p>
ОТДЕЛЕНИЕМ УФМС РОССИИ ПО РЕСПУБЛИКЕ КАРЕЛИЯ В МЕДВЕЖЬЕГОРСКОМ РАЙОНЕ</p><p>
ОУФМС РОССИИ ПО РЕСПУБЛИКЕ КАРЕЛИЯ В МЕДВЕЖЬЕГОРСКОМ РАЙОНЕ</p><p>
УФМС РОССИИ ПО РК В МЕДВЕЖЬЕГОРСКОМ РАЙОНЕ</p><p>
ОТДЕЛЕНИЕМ УФМС РОССИИ ПО РЕСПУБЛИКЕ КАРЕЛИЯ МЕДВЕЖЬЕГОРСКОМ Р-ОНЕ</p><p>
ОТДЕЛЕНИЕМ УФМС РОССИИ ПО РК В МЕДВЕЖЬЕГОРСКОМ РАЙОНЕ</p><p>
ОТДЕЛЕНИЕМ УФМС РОССИИ ПО РЕСПУБЛИКЕ КОРЕЛИЯ В МЕДВЕЖИГОРСКОМ РАЙОНЕ</p><p>
УФМС РОССИИ ПО Р. КАРЕЛИЯ МЕДВЕЖЬЕГОРСКОГО Р-НА</p><p>
ОТДЕЛОМ УФМС РОССИИ ПО РЕСПУБЛИКЕ КАРЕЛИЯ В МЕДВЕЖЬЕГОРСКОМ</p><p>
УФМС РЕСПУБЛИКИ КАРЕЛИИ МЕДВЕЖЬЕГОРСКОГО Р-ОН</p><p>
МЕДВЕЖЬЕГОРСКИМ ОВД
</p><p>
Как можно заметить нужно на поле действительно заполняется криво. Но для нормально кодирования мы должны привести это поле к более-менее нормальному (однозначному) виду.</p><p>
Для начала я бы предложил привести все записи к одному регистру, например, чтобы все буквы стали строчными. Это легко сделать с помощью атрибута str, столбца DataFrame'a. Этот атрибут позволяет работать со столбцом как с строкой, а также выполнять различного рода поиск и замену по регулярным выражениям:

</p><pre><code class="python">train.passport_issuer_name = train.passport_issuer_name.str.lower()
train[train.passport_div_code == example_code].head(5)
</code></pre>
<table>
<tr>
<th/>
<th>passport_div_code</th>
<th>passport_issuer_name</th>
<th>passport_issue_month/year</th>
</tr>
<tr>
<th>id</th>
<th/>
<th/>
<th/>
</tr>
<tr>
<th>19</th>
<td>100010</td>
<td>отделением уфмс россии по республике карелия в...</td>
<td>04M2008</td>
</tr>
<tr>
<th>22</th>
<td>100010</td>
<td>отделением уфмс россии по р. карелия в медвежь...</td>
<td>10M2009</td>
</tr>
<tr>
<th>5642</th>
<td>100010</td>
<td>отделением уфмс россии по респ карелия в медве...</td>
<td>08M2008</td>
</tr>
<tr>
<th>6668</th>
<td>100010</td>
<td>отделением уфмс россии по республике карелия в...</td>
<td>08M2011</td>
</tr>
<tr>
<th>8732</th>
<td>100010</td>
<td>отделением уфмс россии по республике карелия в...</td>
<td>08M2012</td>
</tr>
</table><p>
C регистром определились. Далее надо по возможности избавиться от популярных сокращений, например район, город и т.д. Сделаем это с помощью регулярных выражений. </p><b>Pandas</b><p> предоставляет удобное использование регулярных выражений применительно к каждому столбцу. Это выглядит так:

</p><pre><code class="python">train.passport_issuer_name = train.passport_issuer_name.str.replace(u'р-(а|й|о|н|е)*',u'район')
train.passport_issuer_name = train.passport_issuer_name.str.replace(u' г( |\.|(ор(\.| )))', u' город ')
train.passport_issuer_name = train.passport_issuer_name.str.replace(u' р(\.|есп )', u' республика ')
train.passport_issuer_name = train.passport_issuer_name.str.replace(u' адм([а-я]*)(\.)?', u' административный ')
train.passport_issuer_name = train.passport_issuer_name.str.replace(u' окр(\.| |уга( )?)', u' округ ')
train.passport_issuer_name = train.passport_issuer_name.str.replace(u' ао ', u' административный округ ')
</code></pre><p>
Теперь избавимся от всех лишних символов, кроме русских букв, дефисов и пробелов. Это связано с тем, что паспорт о одинаковым подразделением может выдаваться отделами с разными номерами, и это ухудшит дальнейшую кодировку:

</p><pre><code class="python">train.passport_issuer_name = train.passport_issuer_name.str.replace(u' - ?', u'-')
train.passport_issuer_name = train.passport_issuer_name.str.replace(u'[^а-я -]','')
train.passport_issuer_name = train.passport_issuer_name.str.replace(u'- ',' ')
train.passport_issuer_name = train.passport_issuer_name.str.replace(u'  *',' ')
</code></pre><p>
На следующем шаге, надо расшифровать аббревиатуры, типа УВД, УФНС, ЦАО, ВАО и т.д., т.к. этих их в принципе не много, но на качестве дальнейшего кодирования это скажется положительно. Например если у нас будет две записи «УВД» и «управление внутренних дел», то закодированы они будут по разному, т. к. для компьютера это разные значения.</p><p>
Итак перейдем к расшифровке. И, для начала, заведем словарь сокращений, с помощью которого мы и сделаем расшифровку:

</p><pre><code class="python">sokr = {u'нао': u'ненецкий автономный округ',
u'хмао': u'ханты-мансийский автономный округ',
u'чао': u'чукотский автономный округ',
u'янао': u'ямало-ненецкий автономный округ',
u'вао': u'восточный административный округ',
u'цао': u'центральный административный округ',
u'зао': u'западный административный округ',
u'cао': u'северный административный округ',
u'юао': u'южный административный округ',
u'юзао': u'юго-западный округ',
u'ювао': u'юго-восточный округ',
u'свао': u'северо-восточный округ',
u'сзао': u'северо-западный округ',
u'оуфмс': u'отдел управление федеральной миграционной службы',
u'офмс': u'отдел федеральной миграционной службы',
u'уфмс': u'управление федеральной миграционной службы',
u'увд': u'управление внутренних дел',
u'ровд': u'районный отдел внутренних дел',
u'говд': u'городской отдел внутренних дел',
u'рувд': u'районное управление внутренних дел',
u'овд': u'отдел внутренних дел',
u'оувд': u'отдел управления внутренних дел',
u'мро': u'межрайонный отдел',
u'пс': u'паспортный стол',
u'тп': u'территориальный пункт'}
</code></pre>
<p>
Теперь, собственно произведем расшифровку абривеатур и отформатируем полученные записи:

</p><pre><code class="python">for i in sokr.iterkeys():
    train.passport_issuer_name = train.passport_issuer_name.str.replace(u'( %s )|(^%s)|(%s$)' % (i,i,i), u' %s ' % (sokr[i]))
    
#удалим лишние пробелы в конце и начале строки
train.passport_issuer_name = train.passport_issuer_name.str.lstrip()
train.passport_issuer_name = train.passport_issuer_name.str.rstrip()
</code></pre><p>
Предварительный этап обработки поля «кем выдан паспорт» на этом закончим. И перейдем к полю, в котором находится дата выдачи.</p><p>
Как можно заметить данные в нем хранятся в виде: </p><i>месяц<b>M</b>год</i><p>.</p><p>
Соответственно можно просто убрать букву «M» и привести поле к числовому типу. Но если хорошо подумать, то это поле можно удалить, т.к. на один месяц в году может приходиться несколько подразделений выдававших паспорт, и соответственно это может испортить нашу модель. Исходя из этого удалим его из выборки:

</p><pre><code class="python">train = train.drop(['passport_issue_month/year'], axis=1)
</code></pre><p>
Теперь мы можем перейти к анализу данных.

</p><h4>Анализ данных</h4><p>
Итак, данные для построения модели у нас есть, но они находятся в текстовом виде. Для построения модели хорошо бы было их закодировать в числовом виде.</p><p>
Авторы пакета </p><a href="http://scikit-learn.org/">scikit-learn</a><p> заботливо о нас позаботились и добавили </p><a href="http://scikit-learn.org/stable/modules/feature_extraction.html#feature-extraction">несколько способов</a><p> для извлечения и кодирования текстовых данных. Из них мне больше всего нравятся два:
</p><ol>
<li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher">FeatureHasher</a></li>
<li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer">CountVectorizer</a></li>
<li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer">HashingVectorizer</a></li>
</ol>
<b>FeatureHasher</b><p> преобразовывает строку в числовой массив заданной длинной с помощью хэш-функции (32-разрядная версия </p><a href="http://ru.wikipedia.org/wiki/Murmur2">Murmurhash3</a><p>)
</p><b>CountVectorizer</b><p> преобразовывает входной текст в матрицу, значениями которой, являются количества вхождения данного ключа(слова) в текст. В отличие от FeatureHasher имеет больше настраиваемых параметров(например можно задать </p><a href="http://lingvocourse.ru/wiki/index.php/%D0%A2%D0%BE%D0%BA%D0%B5%D0%BD%D0%B8%D0%B7%D0%B0%D1%82%D0%BE%D1%80">токенизатор</a><p>), но работает медленнее.</p><p>
Для более точного понимания работы CountVectorizer приведем простой пример. Допустим есть таблица с текстовыми значениями:
</p><table>
<tr>
<th>Значение</th>
</tr>
<tr>
<td>раз два три</td>
</tr>
<tr>
<td>три четыре два два</td>
</tr>
<tr>
<td>раз раз раз четыре</td>
</tr>
</table><p>
Для начала CountVectorizer собирает уникальные ключи из всех записей, в нашем примере это будет:
</p><p>
[раз, два, три, четыре]
</p><p>
Длина списка из уникальных ключей и будет длиной нашего закодированного текста (в нашем случае это 4). А номера элементов будут соответствовать, количеству раз встречи данного ключа с данным номером в строке:
</p><p>
раз два три --&gt; [1,1,1,0]</p><p>
три четыре два два --&gt; [0,2,1,1]
</p><p>
Соответственно после кодировки, применения данного метода мы получим:
</p><table>
<tr>
<th>Значение</th>
</tr>
<tr>
<td>1,1,1,0</td>
</tr>
<tr>
<td>0,2,1,1</td>
</tr>
<tr>
<td>3,0,0,1</td>
</tr>
</table>
<b>HashingVectorizer</b><p> является смесью двух выше описанных методов. В нем можно и регулировать размер закодированной строки (как в </p><i>FeatureHasher</i><p>) и настраивать токенизатор (как в </p><i>CountVectorizer</i><p>). К тому же его производительность ближе к FeatureHasher.</p><p>
Итак, вернемся к анализу. Если мы посмотрим по внимательнее на наш набор данных то можно заметить, что есть похожие строки но записанные по разному например: "</p><i>… республик<b>а</b> карелия...</i><p>" и "</p><i>… по республик<b>е</b> карелия...</i><p>".</p><p>
Соответственно, если мы попробуем применить один из методов кодирования сейчас мы получим очень похожие значения. Такие случаем можно минимизировать если все слова в записи мы приведем к </p><a href="http://pymorphy2.readthedocs.org/en/0.1/glossary.html#term-1">нормальной форме</a><p>.</p><p>
Для этой задачи хорошо подходит </p><a href="http://habrahabr.ru/post/176575/">pymorphy</a><p> или </p><a href="http://nltk.org/">nltk</a><p>. Я буду использовать первый, т.к. он изначально создавался для работы с русским языком. Итак, функция которая будет отвечать за нормализацию и очиску строки выглядит так:

</p><pre><code class="python">def f_tokenizer(s):
    morph = pymorphy2.MorphAnalyzer()
    if type(s) == unicode:
        t = s.split(' ')
    else:
        t = s
    f = []
    for j in t:
        m = morph.parse(j.replace('.',''))
        if len(m) &lt;&gt; 0:
            wrd = m[0]
            if wrd.tag.POS not in ('NUMR','PREP','CONJ','PRCL','INTJ'):
                f.append(wrd.normal_form)
    return f
</code></pre><p>
Функция делает следующее:
</p><ul>
<li>Сначала она преобразовывает строку в список</li>
<li>Затем для всех слов производит разбор</li>
<li>Если слово является числительным, предикативном, предлогом, союзом, частицей или междометием не включаем его в конечный набор</li>
<li>Если слово не попало в предыдущий список, берем его нормальную форму и добавляем в финальный набор</li>
</ul><p>
Теперь, когда есть функция для нормализации можно приступить к кодированию с помощью метода </p><i>CountVectorizer</i><p>. Он выбран потому, что ему можно передать нашу функцию, как токенизатор и он составит список ключей по значениям полученным в результате работы нашей функции:

</p><pre><code class="python">coder = HashingVectorizer(tokenizer=f_tokenizer, n_features=256)
</code></pre><p>
Как можно заметить при создании метода кроме токенизатора мы задаем еще один параметр </p><i>n_features</i><p>. Через данный параметр задается длина закодированной строки (в нашем случае строка кодируется при помощи 256 столбцов). Кроме того, у </p><i>HashingVectorizer</i><p> есть еще одно преимущество перед </p><i>CountVectorizer</i><p>, но сразу может выполнять нормализацию значений, что хорошо для таких алгоритмов, как SVM.</p><p>
Теперь применим наш кодировщик к обучающему набору:

</p><pre><code class="python">TrainNotDuble = train.drop_duplicates()
trn = coder.fit_transform(TrainNotDuble.passport_issuer_name.tolist()).toarray()
</code></pre>
<h4>Построение модели</h4><p>
Для начала нам надо задать значения для столбца, в котором будут содержаться метки классов:

</p><pre><code class="python">target = TrainNotDuble.passport_div_code.values
</code></pre><p>
Задача, которую мы решаем сегодня, принадлежит к классу задач классификации со множеством классов. Для решения данной задачи лучше всего подошел алгоритм </p><a href="http://ru.wikipedia.org/wiki/Random_forest">RandomForest</a><p>. Остальные алгоритмы показали очень плохие результаты (менее 50%) поэтому я решил не занимать место в статье. При желании любой интересующийся может проверить данные результаты.</p><p>
Для оценки качества классификации будем использовать количество документов по которым принято правильное решение, т. е.
</p><img src="https://habrastorage.org/getpro/habr/post_images/090/795/d9a/090795d9a1358f2ab41e31d0b4d38539.png" title="LaTeX:Accuracy = \frac{P}{N}"/><p>
, где </p><i>P</i><p> — количество документов по которым классификатор принял правильное решение, а </p><i>N</i><p> – размер обучающей выборки.</p><p>
В пакете scikit-learn для этого есть функция: </p><b><a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score">accuracy_score</a></b><p>
Перед началом построения собственно модели, давайте сократим размерность с помощью «метода главных компонент», т.к. 256 столбцов для обучения довольно много:

</p><pre><code class="python">pca = PCA(n_components = 15)
trn = pca.fit_transform(trn)
</code></pre><p>
Модель будет выглядеть так:

</p><pre><code class="python">model = RandomForestClassifier(n_estimators = 100, criterion='entropy')

TRNtrain, TRNtest, TARtrain, TARtest = train_test_split(trn, target, test_size=0.4)
model.fit(TRNtrain, TARtrain)
print 'accuracy_score: ', accuracy_score(TARtest, model.predict(TRNtest))
</code></pre><p>
accuracy_score: 0.6523456

</p><h4>Заключение</h4><p>
В качестве вывода нужно отметить, что полученная точность в 65% близка к угадыванию. Чтобы улучшить нужно при первичной обработке обработать грамматические ошибки и различного рода описки. Данное действие также скажется положительно и на словаре при кодировании поля, т. е. его размер уменьшиться и соответственно уменьшиться длина строки после ее кодировки.</p><p>
Кроме того этап обучения тестовой выборки опущен специально, т. к. в нем нет ничего особенного, кроме его приведения к нужному виду (это можно легко сделать взяв за основу преобразования обучающей выборки)</p><p>
В статье я попытался показать минимальный список этапов по обработке текстовой информации для подачи ее алгоритмам машинного обучения. Возможно делающим первые шаги в анализе данных данная информация будет полезной.

</p><b>UPD</b><p>: Консоль </p><i>IPython Notebook</i> <a href="https://raw.github.com/kuznetsovin/DataScience/master/Опубликованные%20статьи/TKCPasportQuest.ipynb">TKCTask2Answer.ipynb</a>

      
      <p class="clear"/>
    </div>

    
  </div></body></html>