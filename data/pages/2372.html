<html><body><div><article class="col-md-10 col-md-offset-1">
            


<p><a href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes classification</a> is a simple, yet
effective algorithm. It's commonly used in things like text analytics and works well on both
 small datasets and massively scaled out, distributed systems.</p>
<h3>How does it work?</h3>
<p>Naive Bayes is based on, you guessed it, <a href="http://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes' theorem</a>.
Think back to your first statistics class. Bayes' theorem was that seemingly 
counterintuitive lecture on conditional probability.</p>
<p><img alt="" src="../static/img/bayes-theorem-neon-sign.jpg"/>
</p><center><i>I've tried to buy this on multiple occasions, but to no avail :(. Please <a href="mailto:greg@yhathq.com">email me</a> if you know where you can buy it</i></center>
<h3>Bayes' Theorem</h3>
<p><img alt="" src="../static/img/bayes-theorem.png"/>
The neon formula above might look intimidating, but it's actually not that 
complicated. To explain it, instead of using "events <code>A</code> and <code>B</code>", I'm going to 
use something a little more familiar. Let's say the two events in question are:</p>
<pre><code>A) I watched The Lego Movie today
B) I sat on the couch today
</code></pre>
<p><img alt="" src="../static/img/lego-movie-poster.jpg"/></p>
<p>So for my 2 events, let's break it down into it's Bayesian components:</p>
<p>I've seen <a href="http://www.rottentomatoes.com/m/the_lego_movie/">The Lego Movie</a> 10 times in the past 2 months--this is a lot, I know. 
I've been lucky enough that it's been playing on almost every plane I've been
on (as it should be! it's a great movie for both adults and kids). Since I've
watched The Lego Movie 10 out of the last 60 days, we'll say that:</p>
<p><code>P(A) = P(I watched The Lego Movie today) = 10 / 60, or ~0.17</code></p>
<p>I sit on the couch most days I'm at my apartment. I've traveled 14 days in the 
past 2 months, so to keep it simple, we'll assume I sat on my couch at least
once on every other day (hey, it's pretty comfy).</p>
<p><code>P(B) = P(I sat on the couch today) = (60 - 14) / 60, or ~0.76</code></p>
<p>I've seen The Lego Movie 10 times and 4 of those times have been on a plane. I 
think it's pretty safe to assume the rest of those times I was seated 
comfortably in my living room. So given that I've had 46 days of couchtime in 
the past 2 months, we can say that I watched The Lego Movie from my couch 6 / 10
times.</p>
<p><code>P(B|A) = P(I sat on the couch given that I watched The Lego Movie) = 6 / 10 = 0.60</code></p>
<p>Ok, ready for the magic! Using Bayes' theorem, I now have everything I need to 
calculate the Probability that I watched The Lego Movie today given that I 
sat on the couch.</p>
<p><code>P(A|B)=P(B|A)*P(A)/P(B) = (0.60 * 0.17) / 0.76</code></p>
<p><code>P(I watched The Lego Movie given that I sat on the couch) = 0.13</code></p>
<p>And voil√†! Given that I sat on the couch today, there is a 13% chance that I 
also watched The Lego Movie (wow, that's a lot of Lego time).</p>
<p>Now I wonder what the probability of me watching The Lego Movie from a double 
decker couch would be?</p>
<p><img alt="" src="../static/img/lego-movie-double-decker-counch.jpg"/></p>
<h3>Why should I use it?</h3>
<p>Where you see Naive Bayes classifiers pop up a lot is in <a href="http://en.wikipedia.org/wiki/Document_classification">document classification</a>. 
Naive Bayes is a great choice for this because it's pretty fast, it can handle 
a large number of features (i.e. words), and it's actually really effective. Take 
a look at what happens when you do some basic <a href="https://gist.github.com/glamp/1e2600ed62d5e76b21ee">benchmarking between Naive Bayes
and other methods</a> like SVM 
and RandomForest against the <a href="http://qwone.com/~jason/20Newsgroups/">20 Newsgroups dataset</a>.</p>
<p><img src="../static/img/20newsgroups-benchmark.png"/></p>
<p>Naive Bayes wins! Granted this is a relatively simple approach without much in
terms of feature engineering, but in my opinion that's part of the beauty of 
Naive Bayes!</p>
<p>Code for benchmarking is available <a href="https://gist.github.com/glamp/1e2600ed62d5e76b21ee">here</a>.</p>
<h3>Document Classification</h3>
<p>For our example we're going to be attempting to classify whether a wikipedia 
page is referring to a dinosaur or a <a href="http://en.wikipedia.org/wiki/Cryptozoology">cryptid (an animal from cryptozoology. 
Think Lochness Monster or Bigfoot)</a>.</p>
<p><img alt="" src="../static/img/yeti.jpg"/>
</p><center><i>My favorite Yeti, <strong>The Bumble</strong>, from the stop-motion holiday classic <a href="http://en.wikipedia.org/wiki/Rudolph_the_Red-Nosed_Reindeer_(TV_special)">Rudolph the Red-nosed Reindeer</a></i></center>
<p>We'll be using the <a href="https://github.com/yhat/python-naive-bayes/tree/master/sample-data">text from each wikipedia article</a> as features. What we'd 
expect is that certain words like "sighting" or "hoax" would be more commonly 
found in articles about cryptozoology, while words like "fossil" would be more
commonly found in articles about dinosaurs.</p>
<p>We'll do some basic word-tokenization to count the occurrences of each word and 
then calculate conditional probabilities for each word as it pertains to our
2 categories.</p>
<p><a href="https://github.com/yhat/python-naive-bayes">You can find the sample documents I used and the corresponding code here</a>.</p>
<h3>Tokenizing and counting</h3>
<p>First things first. We need to turn our files full of text into something a 
little more mathy. The simplest way to do this is to take the <a href="http://en.wikipedia.org/wiki/Bag-of-words_model">bag of words</a>
approach. That just means we'll be counting how many times each word appears in
each document. We'll also perform a little text normalization by removing 
punctuation and lowercasing the text (this means "Hello," and "hello" will now 
be considered the same word).</p>
<p>Once we've cleaned the text, we need a way to delineate words. A simple approach
is to just use a <a href="http://www.regexr.com/3a19r">good 'ole regex</a> that splits on 
whitespace and punctuation: <code>\W+</code>.</p>


<h3>Calculating our probabilities</h3>
<p>So now that we can count words, let's get cooking. The code below is going
to do the following:</p>
<ul>
<li>open each document</li>
<li>label it as either "crypto" or "dino" and keep track of how many of each label
there are (<code>priors</code>)</li>
<li>count the words for the document</li>
<li>add those counts to the <code>vocab</code>, or a corpus level word count</li>
<li>add those counts to the <code>word_counts</code>, for a category level word count</li>
</ul>


<h3>Classifying a new page</h3>
<p>And finally it's time for the math. We're going to use the word counts we 
calculated in the previous step to calculate the following:</p>
<p><em>Prior Probability for each category</em>, or for the layman, the percentage of
documents that belong to each category. We have 9 crypto docs and 8 dino docs,
so that gives us the following:</p>
<p><code>Prior Prob(crypto) = 9 / (8 + 9) = 0.53</code></p>
<p><code>Prior Prob(dino) = 8 / (8 + 9) = 0.47</code></p>
<p>Ok priors, check. The next thing we need are conditional probabilities for the 
words in the document we're trying to classify. How do we do that? Well we start
by doing a word count on a new document. We'll use the <a href="http://en.wikipedia.org/wiki/Yeti">Yeti</a>
page as our new document.</p>


<p>Alright, we've got our <code>counts</code>. Now we'll calculate <code>P(word|category)</code> for each word and multiply each of these conditional probabilities
together to calculate the <code>P(category|set of words)</code>. To prevent computational errors, we're going to perform the operations in logspace. All
this means is we're going to use the log(probability) so we require fewer decimal places. More on the mystical properties of logs <a href="https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html">here</a> and <a href="http://en.wikipedia.org/wiki/Arithmetic_underflow">here</a>.</p>


<p>Since we're slightly bending the rules of Bayes' Theorem, the results are not actual probabilities, but rather are "scores". 
All you really need to know is which one is <span>bigger</span>.
So our suspicions are confirmed, the "Yeti.txt" file is being classified overwhelmingly in favor of <code>crypto</code> (as we would hope).</p>
<p><img alt="" src="../static/img/lego-yeti.jpg"/>
</p><center>Bringing it home, the LEGO Yeti!</center>
<h3>Final Thoughts</h3>
<p>You can find all the code and documents used in this post <a href="https://github.com/yhat/python-naive-bayes/">on GitHub</a>.</p>
<p>Naive Bayes is great because it's fairly easy to see what's going on under the 
hood. It's a great way to start any text analysis and it can easily scale out of
core to work in a distributed environment. There are some excellent 
implementations in the Python community you can use as well, so if you don't 
want to roll your own, have no fear! The <a href="http://scikit-learn.org/stable/modules/naive_bayes.html">scikit-learn</a> and
<a href="http://www.nltk.org/book/ch06.html">nltk</a> versions are great places to start.</p>

        </article>
    </div></body></html>