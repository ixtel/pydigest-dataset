<html><body><div><div class="entry-content">
		<p><span>This post teaches you how to implement your own spam filter in under 100 lines of Python code. </span></p>
<p><img class=" size-full wp-image-475 aligncenter" src="https://cambridgecoding.files.wordpress.com/2016/01/spamwordcloud1.jpg?w=610" alt="SpamWordCloud.jpg"/></p>
<p>While doing this hands-on exercise, you’ll work with <i>natural language data, </i>learn how to detect <a href="https://sm.asisonline.org/migration/Pages/learn-words-spammers-use-lure-you-006874.aspx">the words spammers use</a> automatically, and learn how to use a <i>Naive Bayes classifier </i>for binary classification.<span id="more-391"/></p>
<p><b><i>Classification</i></b></p>
<p><span>Classification is ubiquitous – many things around us can be divided into two or more classes based on their characteristics. In keeping with the theme of bikes from <a href="http://blog.cambridgecoding.com/2016/01/03/getting-started-with-regression-and-decision-trees/"><span>the previous post on regression</span></a><span>, we call a vehicle with one wheel a “unicycle”, a vehicle with two wheels and no motor a “bicycle”, and a vehicle with two wheels and a motor – a “motorcycle”. Here, we are using the </span><i><span>number of wheels</span></i><span> and the </span><i><span>presence of a motor</span></i><span> to classify the vehicle. Machines can do the same: being exposed to a sufficient amount of data describing the instances of different classes, they can learn about their characteristic properties and detect the classes of the new instances. Binary classification assumes that the choice is between just two classes.</span></span></p>
<p><b><i>Spam filtering</i></b></p>
<p><span>Spam filtering is a classic example of a binary classification task familiar to anybody who has ever used email services. Now you’ll learn how to implement your own one.</span></p>
<p><span>The task is to distinguish between two types of emails, </span><i><span>“spam”</span></i><span> and “non-spam” often called </span><i><span>“</span></i><a href="https://en.wikipedia.org/wiki/Spam_(Monty_Python)"><i><span>ham</span></i></a><i><span>”</span></i><span>. The machine learning classifier will detect that an email is spam if it is characterised by certain features. The textual content of the email – words like “Viagra” or “lottery” or phrases like “You’ve won a $100,000,000! Click here!”, “Join now!”– is crucial in spam detection and offers some of the strongest cues:</span></p>
<p><img class="alignnone wp-image-504" src="https://cambridgecoding.files.wordpress.com/2016/01/untitled8.jpg?w=698&amp;h=232" alt="Untitled.jpg"/></p>
<p><span>Since you’ll be working with text data, you’ll be using the Python-based library Natural Language Toolkit (</span><a href="http://www.nltk.org"><span>NLTK</span></a><span>), which has rich functionality in natural language processing tasks. It is well supported and there is a helpful online </span><a href="http://www.nltk.org/book/"><span>book</span></a><span>.</span></p>
<p><span>Start by importing the toolkit:</span></p>
<pre class="brush: python; first-line: 2; title: ; notranslate" title="">
import nltk
</pre>
<p><span>To train the classifier, we need a representative dataset with both spam and ham emails. We’ll be using the </span><a href="http://labs-repos.iit.demokritos.gr/skel/i-config/downloads/enron-spam/preprocessed/"><span>Enron email dataset</span></a><span> which contains emails of both types stored in plain text format.</span></p>
<p><span>You can download and unarchive any of the folders in the directory with the code – for example,  download and unarchive “<strong>enron1/</strong>” folder, which contains 3672 legitimate (ham) emails and 1500 spam emails.</span></p>
<p>The spam detection algorithm will involve <strong><span>5 steps</span></strong>:<img class="alignnone size-full wp-image-568" src="https://cambridgecoding.files.wordpress.com/2016/01/proc11.jpg?w=610" alt="proc1.jpg"/></p>
<p>Let’s get started!</p>
<p> </p>
<p><span><b><i>Step 1: Loading the data</i></b></span><img class="alignnone size-full wp-image-572" src="https://cambridgecoding.files.wordpress.com/2016/01/proc2.jpg?w=610" alt="proc2.jpg"/></p>
<p><span>You need to read in the files from the spam and ham subfolders and keep them in two separate lists. To be able to iteratively read the files in a folder, add the following import statement:</span></p>
<pre class="brush: python; first-line: 3; title: ; notranslate" title="">
import os
</pre>
<p><span>Then define the function </span><strong>initialise_lists</strong><span> as follows:</span></p>
<pre class="brush: python; first-line: 12; title: ; notranslate" title="">
def init_lists(folder):
    a_list = []
    file_list = os.listdir(folder)
    for a_file in file_list:
        f = open(folder + a_file, 'r')
        a_list.append(f.read())
    f.close()
return a_list
</pre>
<p>Now you can use this function to create spam and ham lists. For that, add the following two lines of code to the main part of your program:</p>
<pre class="brush: python; first-line: 50; title: ; notranslate" title="">
spam = init_lists('enron1/spam/')
ham = init_lists('enron1/ham/')
</pre>
<p><span>Next, let’s combine the two lists keeping the labels. You can do this by creating a list of two-place </span><i><span>tuples </span></i><span>– Python objects, where the first member of the pair stores the text of the email and the second one – its label. To make your code compact use </span><a href="https://treyhunner.com/2015/12/python-list-comprehensions-now-in-color/"><span>list comprehensions</span></a><span>:</span></p>
<pre class="brush: python; first-line: 52; title: ; notranslate" title="">
all_emails = [(email, 'spam') for email in spam]
all_emails += [(email, 'ham') for email in ham]
</pre>
<p><span>Now, the first tuple in </span><strong>all_emails </strong><span> list contains the first spam email read in from the spam subfolder, the following 1499 ones are spam emails as well, while the 1501</span><span>th</span><span> tuple contains the first ham email read in from the ham subfolder. You can always check if your data is loaded correctly by checking the size of the data structure: </span></p>
<pre class="brush: python; first-line: 55; title: ; notranslate" title="">
print (len(all_emails))
</pre>
<p><span>This should print out 5172. Before starting to build the classifier, let’s randomly shuffle the spam and ham examples. This way it will be easier to organise the training data because any portion of </span><b>all_emails</b><span> will contain examples of both categories. Add the following import statement to the import statements block:</span></p>
<pre class="brush: python; first-line: 4; title: ; notranslate" title="">
import random
</pre>
<p><span>You can then shuffle the examples using the following code in the main part of the program:</span></p>
<pre class="brush: python; first-line: 54; title: ; notranslate" title="">
random.shuffle(all_emails)
</pre>
<p> </p>
<p><span><b><i>Step 2: Preprocessing the data</i></b></span><img class="alignnone size-full wp-image-577" src="https://cambridgecoding.files.wordpress.com/2016/01/proc3.png?w=610" alt="proc3.png"/></p>
<p><span>Currently, the data is stored as lines of text, for example: </span></p>
<p><span><span><strong><em>The purpose of the email is to recap the kickoff meeting held on yesterday.</em></strong></span> (ham email #013 in <strong>enron1</strong>/)</span></p>
<p><span><span><strong><em>People are getting rich using this system! Now it’s your turn! We’ve cracked the code and will show you…</em></strong></span> (spam email #046 in <strong>enron1</strong>/)</span></p>
<p><span><strong><em>Subject: popular meds at lowest prices </em></strong></span>(spam email #838 in <strong>enron1</strong>/)</p>
<p>To be able to use the words in these texts as features for your classifier, you need to preprocess the data and normalise it (so that different forms of the same word are treated as the same word) by:</p>
<ul>
<li><span>Splitting the text by white spaces and punctuation marks – the tools that are used for this purpose are called </span><a href="http://www.nltk.org/book/ch03.html"><i><span>tokenizers</span></i></a><i><span>, </span></i><span>and you can use a tokenizer provided with the NLTK. If you run an NLTK tokenizer on the ham sentence from the example above, you’ll get:</span></li>
</ul>
<p><img class="size-full wp-image-534 aligncenter" src="https://cambridgecoding.files.wordpress.com/2016/01/untitled11.jpg?w=610" alt="Untitled"/></p>
<ul>
<li><span>linking the different forms of the same word (for example, <em>price</em></span><i><span> </span></i><span>and </span><i><span>prices, </span></i><em><span>is </span></em><span>and <em>are</em></span><span>) to each other – the tools that can do that are called </span><a href="http://www.nltk.org/book/ch03.html"><i><span>lemmatizers</span></i></a><i><span>, </span></i><span>and you can again use one of those that come with the NLTK. For example, if you run a lemmatizer on the spam sentence from above, you’ll get:</span></li>
</ul>
<p><img class="size-full wp-image-547 aligncenter" src="https://cambridgecoding.files.wordpress.com/2016/01/untitled12.jpg?w=610" alt="Untitled.jpg"/></p>
<ul>
<li><span>converting all words to lowercase so that the classifier does not treat <em>People</em></span><i>, people </i><span>and </span><i><span>PEOPLE</span></i><span> as three separate features.</span></li>
</ul>
<p>Add the following import statement to the import statements block to include the tokenizer and lemmatizer:</p>
<pre class="brush: python; first-line: 6; title: ; notranslate" title="">
from nltk import word_tokenize, WordNetLemmatizer
</pre>
<p><span>Then define the </span><strong>preprocess</strong><span> function that will take a sentence as an input and will return the result of all the preprocessing operations:</span></p>
<pre class="brush: python; first-line: 21; title: ; notranslate" title="">
def preprocess(sentence):
    return [lemmatizer.lemmatize(word.lower()) for word in word_tokenize(sentence)]
</pre>
<p> </p>
<p><span><b><i>Step 3: Extracting the features</i></b></span><img class="alignnone size-full wp-image-580" src="https://cambridgecoding.files.wordpress.com/2016/01/proc4.jpg?w=610" alt="proc4.jpg"/></p>
<p>Once text is pre-processed, you can extract the features characterising spam and ham emails. The first thing to notice is that some words like “<em>the</em>”, “<i>is</i>” or <em>“of”</em> appear in all emails, don’t have much content to them and are therefore not going to help you distinguish spam from ham. Such words are called <i><span>stopwords</span></i><span> and they can be disregarded during classification. NLTK has a corpus of stopwords for several languages including English, which you can import as:</span></p>
<pre class="brush: python; first-line: 7; title: ; notranslate" title="">
from nltk.corpus import stopwords
</pre>
<p><span>Now you can access the stopword list for English as follows:</span></p>
<pre class="brush: python; first-line: 10; title: ; notranslate" title="">
stoplist = stopwords.words(‘english’)
</pre>
<p>For example, if you remove the stopwords from the ham sentence above, you’ll get:</p>
<p><img class="alignnone size-full wp-image-556" src="https://cambridgecoding.files.wordpress.com/2016/01/untitled13.jpg?w=610" alt="Untitled.jpg"/></p>
<p>This set of words summarises the content of the original sentence perfectly.</p>
<p>To extract the features – words that can tell the program whether the email is spam or ham – you’ll need to do the following:</p>
<ul>
<li><span>read in the text of the email;</span></li>
<li><span>preprocess it using the function </span><b>preprocess(sentence)</b><span> defined above;</span></li>
<li><span>for each word that is not in the stopword list, either calculate how frequently it occurs in the text, or simply register the fact that the word occurs in the email. The former approach is called the </span><i><span>bag-of-words (bow, </span></i><span>for short</span><i><span>)</span></i><span>, and it allows the classifier to notice that certain keywords may occur in both types of emails but with different frequencies, for example the word “Viagra” is much more frequent in spam than ham emails. Python </span><a href="https://docs.python.org/2/library/collections.html"><span>Counter subclass</span></a><span> allows to apply the bow model.</span></li>
</ul>
<p>To use the Counter subclass, add the import statement:</p>
<pre class="brush: python; first-line: 5; title: ; notranslate" title="">
from collections import Counter
</pre>
<p><span>You can control which model you want to use with the parameter </span><b>setting</b><span> in the following code (the simple word occurrence model is the default):</span></p>
<pre class="brush: python; title: ; notranslate" title="">
def get_features(text, setting):
    if setting=='bow':
        return {word: count for word, count in Counter(preprocess(text)).items() if not word in stoplist}
    else:
        return {word: True for word in preprocess(text) if not word in stoplist}
</pre>
<p><span>The code above uses </span><a href="https://docs.python.org/2/tutorial/datastructures.html"><span>dictionary comprehensions</span></a><span>.</span></p>
<p><span>Now you can extract the features from the emails and pair them with the email class label (“spam” or “ham”). Add the following line of code to the main part of the program if you want to use the </span><i><span>bow</span></i><span> model:</span></p>
<pre class="brush: python; first-line: 58; title: ; notranslate" title="">
    all_features = [(get_features(email, 'bow'), label) for (email, label) in all_emails]
</pre>
<p><span>and the following one for the </span><i><span>default</span></i><span> model:</span></p>
<pre class="brush: python; first-line: 58; title: ; notranslate" title="">
    all_features = [(get_features(email, ''), label) for (email, label) in all_emails]
</pre>
<p> </p>
<p><span><b><i>Step 4: Training a classifier</i></b></span><img class="alignnone size-full wp-image-583" src="https://cambridgecoding.files.wordpress.com/2016/01/proc5.jpg?w=610" alt="proc5.jpg"/></p>
<p><span>Now that the data is in the correct format, you can split it into a training set that will be used to train the classifier, and a test set that will be used to evaluate it. Typically, the data is split using 80% for training and the other 20% for testing.</span></p>
<p><span>Define a function </span><b>train</b><span> that will take the set of features and the proportion of the examples assigned to the training set as arguments:</span></p>
<pre class="brush: python; first-line: 31; title: ; notranslate" title="">
def train(features, samples_proportion):
    train_size = int(len(features) * samples_proportion)
    train_set, test_set = features[:train_size], features[train_size:]
    print ('Training set size = ' + str(len(train_set)) + ' emails')
    print ('Test set size = ' + str(len(test_set)) + ' emails')
</pre>
<p><span>The </span><b>print</b><span> statements will help you make sure that the data is split correctly: if you use 80% of the data in “enron1/” to train the classifier, the training set should contain 4137 emails, and the test set – 1035.</span></p>
<p><span>You can apply any classifier of your choice – a number of them come with </span><a href="http://www.nltk.org/book/ch06.html"><span>NLTK</span></a><span>. This post will show you how to use </span><b>Naive Bayes classifier</b><span>, which is a simple yet powerful classification algorithm that has been widely applied to spam filtering </span><a href="https://github.com/shanbady/NLTK-Boston-Python-Meetup"><span>before</span></a><span>. </span></p>
<p><span>The classifier tries to choose the most probable class, or label, among the two classes, spam and ham, i.e. </span><span> <img class="alignnone wp-image-408" src="https://cambridgecoding.files.wordpress.com/2016/01/untitled.jpg?w=102&amp;h=18" alt="Untitled"/>based on what it has learned about the features (presence or frequency of words in the emails of each type). More precisely, it’s trying to choose the most probable class given the words in the e-mail:</span></p>
<p><span><img class=" wp-image-410 aligncenter" src="https://cambridgecoding.files.wordpress.com/2016/01/untitled1.jpg?w=289&amp;h=27" alt="Untitled"/> </span></p>
<p><span>Don’t worry if the formula looks a bit complicated at first. It simply says that the classifier will assign the class (denoted as </span><b>c</b><span> with a hat) it will choose among the two classes by looking which of the two probabilities – “spam” given the words in the email </span><b>P(spam | words)</b><span>, or “ham” given the words in the email </span><b>P(ham | words)</b><span> – is higher (thus the </span><i><span>argmax</span></i><span>). These probabilities cannot be directly estimated, but Bayes rule allows you to swap the conditions and get:</span><br/>
<img class=" wp-image-412 aligncenter" src="https://cambridgecoding.files.wordpress.com/2016/01/untitled2.jpg?w=233&amp;h=43" alt="Untitled"/><span>Now the classifier will need to compare these fractions for the two classes. When comparing two fractions, you can disregard the denominator because it stays the same for both classes, and directly compare the product </span><b>P(words|spam)P(spam)</b><span> with </span><b>P(words|ham)P(ham)</b><span>.</span> <img class=" wp-image-414 aligncenter" src="https://cambridgecoding.files.wordpress.com/2016/01/untitled3.jpg?w=310&amp;h=31" alt="Untitled"/><span>Probabilities P(spam) and P(ham) are called the </span><i><span>prior probabilities, </span></i><span>and they show the distribution of “spam” and “ham” classes in the training set. The probabilities P(words|spam) and P(words|ham) are called </span><i><span>conditional probabilities </span></i><span>of having a particular set of features if the email is “spam” or if it is “ham”. Naive Bayes classifier assumes that each feature (word) occurs in a text independently of all other words, so we can multiply the conditional probabilities for each of the words directly. In short, the algorithm will say that an email is </span><b>spam</b><span> if:</span></p>
<p><span><img class=" wp-image-415 aligncenter" src="https://cambridgecoding.files.wordpress.com/2016/01/untitled4.jpg?w=516&amp;h=31" alt="Untitled"/></span></p>
<p><span>and </span><b>ham</b><span> otherwise.</span></p>
<p><span>The probabilities are calculated on the training data: for example, P(spam) in “enron1” dataset equals 0.29 (since there are 1500 spam emails out of the 5172), but you don’t have to estimate the probabilities yourself – the NLTK implementation of the algorithm will do it for you.</span></p>
<p>The probabilities are calculated on the training data: for example, P(spam) in “enron1” dataset equals 0.29 (since there are 1500 spam emails out of the 5172), but you don’t have to estimate the probabilities yourself – the NLTK implementation of the algorithm will do it for you.</p>
<p>To use this classifier, make sure that it has been imported:</p>
<pre class="brush: python; first-line: 8; title: ; notranslate" title="">
from nltk import NaiveBayesClassifier, classify
</pre>
<p><span>Then add the following code to the </span><b>train</b><span> function to train a model based on the training dataset:</span></p>
<pre class="brush: python; first-line: 38; title: ; notranslate" title="">
    classifier = NaiveBayesClassifier.train(train_set)
return train_set, test_set, classifier
</pre>
<p><span>Now you have a working spam filter, which you can use by calling to the </span><b>train</b><span> function from the main part of the program using the appropriate data split, for example:</span></p>
<pre class="brush: python; first-line: 62; title: ; notranslate" title="">
train_set, test_set, classifier = train(all_features, 0.8)
</pre>
<p><span>Now you have train and test set and also the classifier ready to detect your spam emails.</span></p>
<p> </p>
<p><span><b><i>Step 5: Evaluating your classifier performance</i></b></span><img class="alignnone size-full wp-image-585" src="https://cambridgecoding.files.wordpress.com/2016/01/proc6.jpg?w=610" alt="proc6.jpg"/></p>
<p><span>So far so good! But how do we know if your classifier is doing a good job at detecting spam messages?</span></p>
<p><span>One way to check this is to evaluate its performance on the test dataset – the part of the data you’ve set apart (20% if you’ve been following the settings from above). Let’s add a new function called </span><b>evaluate</b><span> to your program. It will take the </span><b>train_set, test_set</b><span> and </span><b>classifier</b><span> as arguments:</span></p>
<pre class="brush: python; first-line: 41; title: ; notranslate" title="">
def evaluate(train_set, test_set, classifier):
    print ('Accuracy on the training set = ' + str(classify.accuracy(classifier, train_set)))
    print ('Accuracy of the test set = ' + str(classify.accuracy(classifier, test_set)))
</pre>
<p><span>Accuracy on the </span><i><span>training set</span></i><span> will tell you how good the classifier is at learning the relevant information about the features and detecting the classes on the very same data it has been trained on. The accuracy on the </span><i><span>test set</span></i><span> shows how well it generalises this knowledge when applying it to the emails it hasn’t seen before. </span></p>
<p><span>To see how well the classifier performs on both sets, call this function from the main part of the program:</span></p>
<pre class="brush: python; first-line: 65; title: ; notranslate" title="">
evaluate(train_set, test_set, classifier)
</pre>
<p><span>Remember, that you have shuffled the data early on to avoid any bias, so every time you run your program you’ll be getting slightly different results – for example, the accuracy on the training set might vary between <strong>95%</strong> and <strong>98%</strong>, and the accuracy on the test set might vary between <strong>92%</strong> and <strong>95%</strong>.</span></p>
<p><span>From a natural language perspective, another thing to check is which words the classifier finds most discriminative when detecting a spam message. You can do this by adding just one line of code to the </span><b>evaluate</b><span> function:</span></p>
<pre class="brush: python; first-line: 46; title: ; notranslate" title="">
classifier.show_most_informative_features(20)
</pre>
<p><span>This will show you the top 20 most informative words (you can change this number). </span></p>
<p><span>For example:</span><img class="wp-image-422 aligncenter" src="https://cambridgecoding.files.wordpress.com/2016/01/untitled5.jpg?w=418&amp;h=106" alt="Untitled"/><span>tells you that in this dataset an email containing the word “prescription” is around 160 times more likely to be spam than ham, and the word “pain” is around 101 times more likely to be spam than ham. Again, the set of words will vary from one run to another since you shuffle the data before splitting. </span></p>
<p><b><i>A recap of what you’ve learned in this post:</i></b></p>
<ul>
<li><span>Classification is involved in many tasks in machine learning</span></li>
<li><span>Spam filtering is a binary classification task where you need to detect whether an email belongs to a “spam” or “ham” class</span></li>
<li><span>Word occurrence and frequency are some of the most informative features for spam detection</span></li>
<li><span>Before you can use words as features you need to preprocess the text data</span></li>
<li><a href="http://www.nltk.org"><span>NLTK</span></a><span> provides wide functionality for natural language processing</span></li>
<li><span>Naive Bayes is a simple yet quite powerful machine learning algorithm that can be used for binary classification.</span></li>
</ul>
<p> </p>
<p><b>Want to learn more?  Check out our two-day Data Science Bootcamp:</b></p>
<p><a href="https://cambridgecoding.com/datascience-bootcamp"><span>https://cambridgecoding.com/datascience-bootcamp</span></a></p>
<p> </p>
<hr/>
<p> </p>
<p>The <strong>full script</strong>:</p>
<pre class="brush: python; title: ; notranslate" title="">

from __future__ import print_function, division
import nltk
import os
import random
from collections import Counter
from nltk import word_tokenize, WordNetLemmatizer
from nltk.corpus import stopwords
from nltk import NaiveBayesClassifier, classify

stoplist = stopwords.words('english')

def init_lists(folder):
    a_list = []
    file_list = os.listdir(folder)
    for a_file in file_list:
        f = open(folder + a_file, 'r')
        a_list.append(f.read())
    f.close()
    return a_list

def preprocess(sentence):
    lemmatizer = WordNetLemmatizer()
    return [lemmatizer.lemmatize(word.lower()) for word in word_tokenize(unicode(sentence, errors='ignore'))]

def get_features(text, setting):
    if setting=='bow':
        return {word: count for word, count in Counter(preprocess(text)).items() if not word in stoplist}
    else:
        return {word: True for word in preprocess(text) if not word in stoplist}

def train(features, samples_proportion):
    train_size = int(len(features) * samples_proportion)
    # initialise the training and test sets
    train_set, test_set = features[:train_size], features[train_size:]
    print ('Training set size = ' + str(len(train_set)) + ' emails')
    print ('Test set size = ' + str(len(test_set)) + ' emails')
    # train the classifier
    classifier = NaiveBayesClassifier.train(train_set)
    return train_set, test_set, classifier

def evaluate(train_set, test_set, classifier):
    # check how the classifier performs on the training and test sets
    print ('Accuracy on the training set = ' + str(classify.accuracy(classifier, train_set)))
    print ('Accuracy of the test set = ' + str(classify.accuracy(classifier, test_set)))
    # check which words are most informative for the classifier
    classifier.show_most_informative_features(20)

if __name__ == &amp;amp;quot;__main__&amp;amp;quot;:
    # initialise the data
    spam = init_lists('enron1/spam/')
    ham = init_lists('enron1/ham/')
    all_emails = [(email, 'spam') for email in spam]
    all_emails += [(email, 'ham') for email in ham]
    random.shuffle(all_emails)
    print ('Corpus size = ' + str(len(all_emails)) + ' emails')

    # extract the features
    all_features = [(get_features(email, ''), label) for (email, label) in all_emails]
    print ('Collected ' + str(len(all_features)) + ' feature sets')

    # train the classifier
    train_set, test_set, classifier = train(all_features, 0.8)

    # evaluate its performance
    evaluate(train_set, test_set, classifier)
</pre>
<hr/>
<p><strong>ABOUT THE AUTHOR</strong></p>
<p><img class="alignleft wp-image-562 size-thumbnail" src="https://cambridgecoding.files.wordpress.com/2016/01/ekaterina.jpg?w=150&amp;h=150" alt=""/>EKATERINA KOCHMAR</p>
<p>Ekaterina is a research associate at the Computer Laboratory of the University of Cambridge. Her research focuses on Natural Language Processing (NLP) applications and Machine Learning methods. She holds a PhD in Natural Language and Information Processing and an MPhil in Advanced Computer Science from the University of Cambridge.</p>
<div id="jp-post-flair" class="sharedaddy sd-like-enabled sd-sharing-enabled"><div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" id="like-post-wrapper-104127362-391-56d5a0c35ec65" data-src="//widgets.wp.com/likes/#blog_id=104127362&amp;post_id=391&amp;origin=cambridgecoding.wordpress.com&amp;obj_id=104127362-391-56d5a0c35ec65" data-name="like-post-frame-104127362-391-56d5a0c35ec65"><h3 class="sd-title">Like this:</h3><p class="likes-widget-placeholder post-likes-widget-placeholder"><span class="button"><span>Like</span></span> <span class="loading">Loading...</span></p><span class="sd-text-color"/><a class="sd-link-color"/></div>
<p id="jp-relatedposts" class="jp-relatedposts">
	<h3 class="jp-relatedposts-headline"><em>Related</em></h3>
</p></div>			</div>

	</div></body></html>