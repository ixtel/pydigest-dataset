<html><body><div><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-naive-bayesian-classifier" class="anchor" href="#naive-bayesian-classifier" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Naive Bayesian Classifier</h1>

<p>yet another general purpose Naive Bayesian classifier.</p>

<h2><a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Installation</h2>

<p>You can install this package using the following <code>pip</code> command:</p>

<div class="highlight highlight-source-shell"><pre>$ sudo pip install naiveBayesClassifier</pre></div>

<h2><a id="user-content-example" class="anchor" href="#example" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Example</h2>

<div class="highlight highlight-source-python"><pre><span class="pl-s"><span class="pl-pds">"""</span></span>
<span class="pl-s">Suppose you have some texts of news and know their categories.</span>
<span class="pl-s">You want to train a system with this pre-categorized/pre-classified </span>
<span class="pl-s">texts. So, you have better call this data your training set.</span>
<span class="pl-s"><span class="pl-pds">"""</span></span>
<span class="pl-k">from</span> naiveBayesClassifier <span class="pl-k">import</span> tokenizer
<span class="pl-k">from</span> naiveBayesClassifier.trainer <span class="pl-k">import</span> Trainer
<span class="pl-k">from</span> naiveBayesClassifier.classifier <span class="pl-k">import</span> Classifier

newsTrainer <span class="pl-k">=</span> Trainer(tokenizer)

<span class="pl-c"># You need to train the system passing each text one by one to the trainer module.</span>
newsSet <span class="pl-k">=</span>[
    {<span class="pl-s"><span class="pl-pds">'</span>text<span class="pl-pds">'</span></span>: <span class="pl-s"><span class="pl-pds">'</span>not to eat too much is not enough to lose weight<span class="pl-pds">'</span></span>, <span class="pl-s"><span class="pl-pds">'</span>category<span class="pl-pds">'</span></span>: <span class="pl-s"><span class="pl-pds">'</span>health<span class="pl-pds">'</span></span>},
    {<span class="pl-s"><span class="pl-pds">'</span>text<span class="pl-pds">'</span></span>: <span class="pl-s"><span class="pl-pds">'</span>Russia is trying to invade Ukraine<span class="pl-pds">'</span></span>, <span class="pl-s"><span class="pl-pds">'</span>category<span class="pl-pds">'</span></span>: <span class="pl-s"><span class="pl-pds">'</span>politics<span class="pl-pds">'</span></span>},
    {<span class="pl-s"><span class="pl-pds">'</span>text<span class="pl-pds">'</span></span>: <span class="pl-s"><span class="pl-pds">'</span>do not neglect exercise<span class="pl-pds">'</span></span>, <span class="pl-s"><span class="pl-pds">'</span>category<span class="pl-pds">'</span></span>: <span class="pl-s"><span class="pl-pds">'</span>health<span class="pl-pds">'</span></span>},
    {<span class="pl-s"><span class="pl-pds">'</span>text<span class="pl-pds">'</span></span>: <span class="pl-s"><span class="pl-pds">'</span>Syria is the main issue, Obama says<span class="pl-pds">'</span></span>, <span class="pl-s"><span class="pl-pds">'</span>category<span class="pl-pds">'</span></span>: <span class="pl-s"><span class="pl-pds">'</span>politics<span class="pl-pds">'</span></span>},
    {<span class="pl-s"><span class="pl-pds">'</span>text<span class="pl-pds">'</span></span>: <span class="pl-s"><span class="pl-pds">'</span>eat to lose weight<span class="pl-pds">'</span></span>, <span class="pl-s"><span class="pl-pds">'</span>category<span class="pl-pds">'</span></span>: <span class="pl-s"><span class="pl-pds">'</span>health<span class="pl-pds">'</span></span>},
    {<span class="pl-s"><span class="pl-pds">'</span>text<span class="pl-pds">'</span></span>: <span class="pl-s"><span class="pl-pds">'</span>you should not eat much<span class="pl-pds">'</span></span>, <span class="pl-s"><span class="pl-pds">'</span>category<span class="pl-pds">'</span></span>: <span class="pl-s"><span class="pl-pds">'</span>health<span class="pl-pds">'</span></span>}
]

<span class="pl-k">for</span> news <span class="pl-k">in</span> newsSet:
    newsTrainer.train(news[<span class="pl-s"><span class="pl-pds">'</span>text<span class="pl-pds">'</span></span>], news[<span class="pl-s"><span class="pl-pds">'</span>category<span class="pl-pds">'</span></span>])

<span class="pl-c"># When you have sufficient trained data, you are almost done and can start to use</span>
<span class="pl-c"># a classifier.</span>
newsClassifier <span class="pl-k">=</span> Classifier(newsTrainer.data, tokenizer)

<span class="pl-c"># Now you have a classifier which can give a try to classifiy text of news whose</span>
<span class="pl-c"># category is unknown, yet.</span>
unknownInstance <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>Even if I eat too much, is not it possible to lose some weight<span class="pl-pds">"</span></span>
classification <span class="pl-k">=</span> newsClassifier.classify(unknownInstance)

<span class="pl-c"># the classification variable holds the possible categories sorted by </span>
<span class="pl-c"># their probablity value</span>
<span class="pl-c1">print</span> classification</pre></div>

<p><strong><em>Note</em></strong>: Definitely you will need much more training data than the amount in the above example. Really, a few lines of text like in the example is out of the question to be sufficient training set.</p>

<h2><a id="user-content-what-is-the-naive-bayes-theorem-and-classifier" class="anchor" href="#what-is-the-naive-bayes-theorem-and-classifier" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>What is the Naive Bayes Theorem and Classifier</h2>

<p>It is needles to explain everything once again here. Instead, one of the most eloquent explanations is quoted here.</p>

<p>The following explanation is quoted from <a href="https://github.com/jbrukh/bayesian/blob/master/bayesian.go">another Bayes classifier</a> which is written in Go. </p>

<blockquote>
<p>BAYESIAN CLASSIFICATION REFRESHER: suppose you have a set  of classes
(e.g. categories) C := {C_1, ..., C_n}, and a  document D consisting
of words D := {W_1, ..., W_k}.  We wish to ascertain the probability
that the document  belongs to some class C_j given some set of
training data  associating documents and classes.</p>

<p>By Bayes' Theorem, we have that</p>

<pre><code>P(C_j|D) = P(D|C_j)*P(C_j)/P(D).
</code></pre>

<p>The LHS is the probability that the document belongs to class  C_j
given the document itself (by which is meant, in practice,  the word
frequencies occurring in this document), and our program  will
calculate this probability for each j and spit out the  most likely
class for this document.</p>

<p>P(C_j) is referred to as the "prior" probability, or the  probability
that a document belongs to C_j in general, without  seeing the
document first. P(D|C_j) is the probability of seeing  such a
document, given that it belongs to C_j. Here, by assuming  that words
appear independently in documents (this being the   "naive"
assumption), we can estimate</p>

<pre><code>P(D|C_j) ~= P(W_1|C_j)*...*P(W_k|C_j)
</code></pre>

<p>where P(W_i|C_j) is the probability of seeing the given word  in a
document of the given class. Finally, P(D) can be seen as   merely a
scaling factor and is not strictly relevant to  classificiation,
unless you want to normalize the resulting  scores and actually see
probabilities. In this case, note that</p>

<pre><code>P(D) = SUM_j(P(D|C_j)*P(C_j))
</code></pre>

<p>One practical issue with performing these calculations is the 
possibility of float64 underflow when calculating P(D|C_j), as 
individual word probabilities can be arbitrarily small, and  a
document can have an arbitrarily large number of them. A  typical
method for dealing with this case is to transform the  probability to
the log domain and perform additions instead  of multiplications:</p>

<p>log P(C_j|D) ~ log(P(C_j)) + SUM_i(log P(W_i|C_j))</p>

<p>where i = 1, ..., k. Note that by doing this, we are discarding  the
scaling factor P(D) and our scores are no longer  probabilities;
however, the monotonic relationship of the  scores is preserved by the
log function.</p>
</blockquote>

<p>If you are very curious about Naive Bayes Theorem, you may find the following list helpful:</p>



<h1><a id="user-content-improvements" class="anchor" href="#improvements" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Improvements</h1>

<p>This classifier uses a very simple tokenizer which is jus a module to split sentences into words. If your training set is large, you can rely on the available tokenizer, otherwise you need to have a better tokenizer specialized to the language of your training texts.</p>

<h2><a id="user-content-todo" class="anchor" href="#todo" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>TODO</h2>



<h2><a id="user-content-authors" class="anchor" href="#authors" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>AUTHORS</h2>

<ul>
<li>Mustafa Atik @muatik</li>
<li>Nejdet Yucesoy @nejdetckenobi</li>
</ul>
</article>
  </div></body></html>