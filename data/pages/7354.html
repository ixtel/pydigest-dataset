<html><body><div><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-neural-storyteller" class="anchor" href="#neural-storyteller" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>neural-storyteller</h1>

<p>neural-storyteller is a recurrent neural network that generates little stories about images. This repository contains code for generating stories with your own images, as well as instructions for training new models.</p>

<p><a href="https://github.com/ryankiros/neural-storyteller/blob/master/images/ex1.jpg" target="_blank"><img src="https://github.com/ryankiros/neural-storyteller/raw/master/images/ex1.jpg" align="left"/></a>
<em>We were barely able to catch the breeze at the beach , and it felt as if someone stepped out of my mind . She was in love with him for the first time in months , so she had no intention of escaping . The sun had risen from the ocean , making her feel more alive than normal . She 's beautiful , but the truth is that I do n't know what to do . The sun was just starting to fade away , leaving people scattered around the Atlantic Ocean . I d seen the men in his life , who guided me at the beach once more .</em></p>

<p><a href="http://samim.io/">Samim</a> has made an awesome blog post with lots of results <a href="https://medium.com/@samim/generating-stories-about-images-d163ba41e4ed">here</a>.</p>

<p>Some more results from an older model trained on Adventure books can be found <a href="http://www.cs.toronto.edu/%7Erkiros/adv_L.html">here</a>.</p>

<p>The whole approach contains 4 components:</p>



<p>The 'style-shifting' operation is what allows our model to transfer standard image captions to the style of stories from novels. The only source of supervision in our models is from <a href="http://mscoco.org/">Microsoft COCO</a> captions. That is, we did not collect any new training data to directly predict stories given images.</p>

<p>Style shifting was inspired by <a href="http://arxiv.org/abs/1508.06576">A Neural Algorithm of Artistic Style</a> but the technical details are completely different.</p>

<h2><a id="user-content-how-does-it-work" class="anchor" href="#how-does-it-work" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>How does it work?</h2>

<p>We first train a recurrent neural network (RNN) decoder on romance novels. Each passage from a novel is mapped to a skip-thought vector. The RNN then conditions on the skip-thought vector and aims to generate the passage that it has encoded. We use romance novels collected from the BookCorpus <a href="http://www.cs.toronto.edu/%7Embweb/">dataset</a>.</p>

<p>Parallel to this, we train a visual-semantic embedding between COCO images and captions. In this model, captions and images are mapped into a common vector space. After training, we can embed new images and retrieve captions.</p>

<p>Given these models, we need a way to bridge the gap between retrieved image captions and passages in novels. That is, if we had a function F that maps a collection of image caption vectors <strong>x</strong> to a book passage vector F(<strong>x</strong>), then we could feed F(<strong>x</strong>) to the decoder to get our story. There is no such parallel data, so we need to construct F another way.</p>

<p>It turns out that skip-thought vectors have some intriguing properties that allow us to construct F in a really simple way. Suppose we have 3 vectors: an image caption <strong>x</strong>, a "caption style" vector <strong>c</strong> and a "book style" vector <strong>b</strong>. Then we define F as</p>

<p>F(<strong>x</strong>) = <strong>x</strong> - <strong>c</strong> + <strong>b</strong></p>

<p>which intuitively means: keep the "thought" of the caption, but replace the image caption style with that of a story. Then, we simply feed F(<strong>x</strong>) to the decoder.</p>

<p>How do we construct <strong>c</strong> and <strong>b</strong>? Here, <strong>c</strong> is the mean of the skip-thought vectors for Microsoft COCO training captions. We set <strong>b</strong> to be the mean of the skip-thought vectors for romance novel passages that are of length &gt; 100.</p>

<h4><a id="user-content-what-kind-of-biases-work" class="anchor" href="#what-kind-of-biases-work" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>What kind of biases work?</h4>

<p>Skip-thought vectors are sensitive to:</p>

<ul>
<li>length (if you bias by really long passages, it will decode really long stories)</li>
<li>punctuation</li>
<li>vocabulary</li>
<li>syntactic style (loosely speaking)</li>
</ul>

<p>For the last point, if you bias using text all written the same way the stories you get will also be written the same way.</p>

<h4><a id="user-content-what-can-the-decoder-be-trained-on" class="anchor" href="#what-can-the-decoder-be-trained-on" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>What can the decoder be trained on?</h4>

<p>We use romance novels, but that is because we have over 14 million passages to train on. Anything should work, provided you have a lot of text! If you want to train your own decoder, you can use the code available <a href="https://github.com/ryankiros/skip-thoughts/tree/master/decoding">here</a> Any models trained there can be substituted here.</p>

<h2><a id="user-content-dependencies" class="anchor" href="#dependencies" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Dependencies</h2>

<p>This code is written in python. To use it you will need:</p>

<ul>
<li>Python 2.7</li>
<li>A recent version of <a href="http://www.numpy.org/">NumPy</a> and <a href="http://www.scipy.org/">SciPy</a></li>
<li><a href="https://github.com/Lasagne/Lasagne">Lasagne</a></li>
<li>A version of Theano that Lasagne supports</li>
</ul>

<p>For running on CPU, you will need to install <a href="http://caffe.berkeleyvision.org">Caffe</a> and its python interface.</p>

<h2><a id="user-content-getting-started" class="anchor" href="#getting-started" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Getting started</h2>

<p>You will first need to download some pre-trained models and style vectors. Most of the materials are available in a single compressed file, which you can obtain by running</p>

<pre><code>wget http://www.cs.toronto.edu/~rkiros/neural_storyteller.zip
</code></pre>

<p>Included is a pre-trained decoder on romance novels, the decoder dictionary, caption and romance style vectors, MS COCO training captions and a pre-trained image-sentence embedding model.</p>

<p>Next, you need to obtain the pre-trained skip-thoughts encoder. Go <a href="https://github.com/ryankiros/skip-thoughts">here</a> and follow the instructions on the main page to obtain the pre-trained model.</p>

<p>Finally, we need the VGG-19 ConvNet parameters. You can obtain them by running</p>

<pre><code>wget https://s3.amazonaws.com/lasagne/recipes/pretrained/imagenet/vgg19.pkl
</code></pre>

<p>Note that this model is for non-commercial use only. Once you have all the materials, open <code>config.py</code> and specify the locations of all of the models and style vectors that you downloaded.</p>

<p>For running on CPU, you will need to download the VGG-19 prototxt and model by:</p>

<pre><code>wget http://www.robots.ox.ac.uk/~vgg/software/very_deep/caffe/VGG_ILSVRC_19_layers.caffemodel
wget https://gist.githubusercontent.com/ksimonyan/3785162f95cd2d5fee77/raw/bb2b4fe0a9bb0669211cf3d0bc949dfdda173e9e/VGG_ILSVRC_19_layers_deploy.prototxt
</code></pre>

<p>You also need to modify pycaffe and model path in <code>config.py</code>, and modify the flag in line 8 as:</p>

<pre><code>FLAG_CPU_MODE = True
</code></pre>

<h2><a id="user-content-generating-a-story" class="anchor" href="#generating-a-story" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Generating a story</h2>

<p>The images directory contains some sample images that you can try the model on. In order to generate a story, open Ipython and run the following:</p>

<pre><code>import generate
z = generate.load_all()
generate.story(z, './images/ex1.jpg')
</code></pre>

<p>If everything works, it will first print out the nearest COCO captions to the image (predicted by the visual-semantic embedding model). Then it will print out a story.</p>

<h4><a id="user-content-generation-options" class="anchor" href="#generation-options" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Generation options</h4>

<p>There are 2 knobs that can be tuned for generation: the number of retrieved captions to condition on as well as the beam search width. The defaults are</p>

<pre><code>generate.story(z, './images/ex1.jpg', k=100, bw=50)
</code></pre>

<p>where k is the number of captions to condition on and bw is the beam width. These are reasonable defaults but playing around with these can give you very different outputs! The higher the beam width, the longer it takes to generate a story.</p>

<p>If you bias by song lyrics, you can turn on the lyric flag which will print the output in multiple lines by comma delimiting. <code>neural_storyteller.zip</code> contains an additional bias vector called <code>swift_style.npy</code> which is the mean of skip-thought vectors across Taylor Swift lyrics. If you point <code>path_to_posbias</code> to this vector in <code>config.py</code>, you can generate captions in the style of Taylor Swift lyrics. For example:</p>

<pre><code>generate.story(z, './images/ex1.jpg', lyric=True)
</code></pre>

<p>should output</p>

<pre><code>You re the only person on the beach right now
you know
I do n't think I will ever fall in love with you
and when the sea breeze hits me
I thought
Hey
</code></pre>

<h2><a id="user-content-reference" class="anchor" href="#reference" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Reference</h2>

<p>This project does not have any associated paper with it. If you found this code useful, please consider citing:</p>

<p>Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. <strong>"Skip-Thought Vectors."</strong> <em>arXiv preprint arXiv:1506.06726 (2015).</em></p>

<pre><code>@article{kiros2015skip,
  title={Skip-Thought Vectors},
  author={Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan and Zemel, Richard S and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
  journal={arXiv preprint arXiv:1506.06726},
  year={2015}
}
</code></pre>

<p>If you also use the BookCorpus data for training new models, please also consider citing:</p>

<p>Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler.
<strong>"Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books."</strong> <em>arXiv preprint arXiv:1506.06724 (2015).</em></p>

<pre><code>@article{zhu2015aligning,
    title={Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books},
    author={Zhu, Yukun and Kiros, Ryan and Zemel, Richard and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
    journal={arXiv preprint arXiv:1506.06724},
    year={2015}
}
</code></pre>
</article>
  </div></body></html>