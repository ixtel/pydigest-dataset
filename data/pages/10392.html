<html><body><div><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-deep-visual-analogy-making" class="anchor" href="#deep-visual-analogy-making" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Deep Visual Analogy-Making</h1>

<p>Tensorflow implementation of <a href="http://www-personal.umich.edu/%7Ereedscot/nips2015.pdf">Deep Visual Analogy-Making</a>. The matlab code of the paper can be found <a href="http://www-personal.umich.edu/%7Ereedscot/files/nips2015-analogy.tar.gz">here</a>.</p>

<p><a href="https://github.com/carpedm20/visual-analogy-tensorflow/raw/83893d866557239a890053b55cb7105ebf54045e/assets/model.png" target="_blank"><img src="https://github.com/carpedm20/visual-analogy-tensorflow/raw/83893d866557239a890053b55cb7105ebf54045e/assets/model.png" alt="model"/></a></p>

<p>This implementation contains a deep network trained end-to-end to perform visual analogy making with</p>

<ol>
<li>Fully connected encoder &amp; decoder networks</li>
<li>Analogy transformations by vector addition and deep networks (vector multiplication is not implemented)</li>
<li>Regularizer for manifold traversal transformations</li>
</ol>

<p>This implementation conatins:</p>

<ol>
<li>Analogy transformations of <code>shape</code> dataset

<ul>
<li>with objective for vector-addition-based analogies (L_add)</li>
<li>with objective for multiple fully connected layers (L_deep)</li>
<li>with manifold traversal transformations</li>
</ul></li>
</ol>

<h2><a id="user-content-prerequisites" class="anchor" href="#prerequisites" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Prerequisites</h2>



<h2><a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Usage</h2>

<p>First, you need to download the dataset with:</p>

<pre><code>$ ./download.sh
</code></pre>

<p>To train a model with <code>shape</code> dataset:</p>

<pre><code>$ python main.py --dataset shape --is_train True
</code></pre>

<p>To test a model with <code>shape</code> dataset:</p>

<pre><code>$ python main.py --dataset shape 
</code></pre>

<h2><a id="user-content-results" class="anchor" href="#results" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Results</h2>

<p>Result of analogy transformations of <code>shape</code> dataset with fully connected layers (L_deep) after 1 day of training.</p>

<p><strong>From top to bottom</strong> for each : <em>Reference</em>, <em>output</em>, <em>query</em>, <em>target</em>, <em>prediction</em>, <em>manifold prediction</em> after 2 steps, and <em>manifold prediction</em> after 3 steps.</p>



<p><a href="/carpedm20/visual-analogy-tensorflow/blob/master/assets/rotate_160215.png" target="_blank"><img src="/carpedm20/visual-analogy-tensorflow/raw/master/assets/rotate_160215.png" alt="training in progress"/></a></p>



<p><a href="/carpedm20/visual-analogy-tensorflow/blob/master/assets/scale_160215.png" target="_blank"><img src="/carpedm20/visual-analogy-tensorflow/raw/master/assets/scale_160215.png" alt="training in progress"/></a></p>



<p><a href="/carpedm20/visual-analogy-tensorflow/blob/master/assets/xpos_160215.png" target="_blank"><img src="/carpedm20/visual-analogy-tensorflow/raw/master/assets/xpos_160215.png" alt="training in progress"/></a></p>



<p><a href="/carpedm20/visual-analogy-tensorflow/blob/master/assets/ypos_160215.png" target="_blank"><img src="/carpedm20/visual-analogy-tensorflow/raw/master/assets/ypos_160215.png" alt="training in progress"/></a></p>

<p>(in progress)</p>

<h2><a id="user-content-training-details" class="anchor" href="#training-details" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Training details</h2>

<p><a href="/carpedm20/visual-analogy-tensorflow/blob/master/assets/loss_160215.png" target="_blank"><img src="/carpedm20/visual-analogy-tensorflow/raw/master/assets/loss_160215.png" alt="training in progress"/></a></p>

<h2><a id="user-content-reference" class="anchor" href="#reference" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Reference</h2>



<h2><a id="user-content-author" class="anchor" href="#author" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Author</h2>

<p>Taehoon Kim / <a href="http://carpedm20.github.io/">@carpedm20</a></p>
</article>
  </div></body></html>