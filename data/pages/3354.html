<html><body><div><div class="post-content" itemprop="articleBody">
    <p>What is love?  Can you define love, or do you merely recognize its presence or absence without binding it in a definition?  A computer may be able to provide a definition of love, regurgitated from a database, but can a computer recognize love?  If defining love is so difficult for us, many of whom might cop out with "I know it when I see it", how much more difficult will a computer find this challenge?  On a lark, I decided to implement a machine learning system to predict a few things about a television show about love, <a title="Wikipedia: The Bachelor" href="http://en.wikipedia.org/wiki/The_Bachelor_%28U.S._TV_series%29" target="_blank">ABC's "The Bachelor"</a>.</p>

<p>My <del>girlfriend</del> fiancée regularly watches The Bachelor; valuing our relationship, I am sometimes drawn in.  The premise of the show is this: Chris Soules (The Bachelor), a single man, acts as the sole decision maker in a competition among 30 eligible women to identify his potential bride.  Each week, one or more of the 30 women is eliminated as the bachelor gets to know them.  This is the <a title="Wikipedia: The Bachelor, Season 19" href="http://en.wikipedia.org/wiki/The_Bachelor_(season_19)" target="_blank">19th Season </a>of this show (there have been 10 iterations of its partner show, <a title="Wikipedia: The Bachelorette" href="http://en.wikipedia.org/wiki/The_Bachelorette" target="_blank">"The Bachelorette"</a>)</p>
<p>If we can teach a computer to recognize some measure of love, it is certainly possible to predict who will depart the show next.  Regardless of how we frame this question, there are challenges.  First off, we have no clue how the Chris Soules "mental machine" works.  More importantly, the 30 data points we have (one per contestant) contain far too many features (most of which aren't quantifiable) of which we only know a dozen or so.  Given a dozen features, we cannot build a statistically reliable model for prediction.  For the sake of entertainment and self-enrichment, I decided to make an attempt in the face of these challenges.</p>
<p><strong>Machine Learning Techniques</strong></p>
<p>I decided to utilize a <a title="Wikipedia: Decision Tree Learning" href="http://en.wikipedia.org/wiki/Decision_tree_learning" target="_blank">Decision Tree</a> such that I could investigate the factors personally, but I decided to expand and also utilize a <a title="Wikipedia: Support Vector Machine" href="http://en.wikipedia.org/wiki/Support_vector_machine" target="_blank">Support Vector Machine</a> as well.  An understanding of the SVM coupled with a quick glance at the <a title="Scikit-Learn Homepage" href="http://scikit-learn.org/" target="_blank">Scikit-Learn</a> <a title="Scikit-Learn Algorithm Cheat Sheet" href="http://scikit-learn.org/stable/tutorial/machine_learning_map/" target="_blank">algorithm cheat-sheet</a> drove my decision to incorporate both.</p>
<p><strong>Data Collection</strong></p>
<p>I first wrote a scraper to pull some data off the open Internet about each candidate; a photograph, age, some ancillary data provided.  From the photograph, I manually identified features such as ethnicity as well as hair types (length, curliness, color).  From the ancillary data, I acquired other quantifiable numbers: number of tattoos and age as well as whether or not that contestant had been eliminated.</p>
<p>Each individual is encoded in JSON like so:</p>
<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="p">{</span>
     <span class="s">"hair_wavy"</span><span class="p">:</span> <span class="s">"straight"</span><span class="p">,</span>
     <span class="s">"hometown_name"</span><span class="p">:</span> <span class="s">"Hamilton"</span><span class="p">,</span>
     <span class="s">"height_inches"</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
     <span class="s">"age"</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span>
     <span class="s">"num_tattoos"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
     <span class="s">"hair_color"</span><span class="p">:</span> <span class="s">"dark"</span><span class="p">,</span>
     <span class="s">"name"</span><span class="p">:</span> <span class="s">"Alissa Giambrone"</span><span class="p">,</span>
     <span class="s">"hair_length"</span><span class="p">:</span> <span class="s">"chest"</span><span class="p">,</span>
     <span class="s">"date_fear"</span><span class="p">:</span> <span class="s">" Running into recent exes"</span><span class="p">,</span>
     <span class="s">"occupation"</span><span class="p">:</span> <span class="s">"Flight Attendant"</span><span class="p">,</span>
     <span class="s">"likes"</span><span class="p">:</span> <span class="p">[</span>
        <span class="s">"Family"</span><span class="p">,</span>
        <span class="s">"friends"</span><span class="p">,</span>
        <span class="s">"laughter"</span><span class="p">,</span>
        <span class="s">"hope"</span><span class="p">,</span>
        <span class="s">"faith"</span>
     <span class="p">],</span>
     <span class="s">"free_text"</span><span class="p">:</span> <span class="s">" If I never had to upset others, I would be very happy.  If I never got to play with puppies, I would be very sad.  If you could be any animal, what would you be? A wild mustang. Free to run and explore, they're unpredictable and beautiful, and are loyal to their herd.  If you won the lottery, what would you do with your winnings? Adopt dogs and charter a jet for my friends anf fmaily to fly around Europe, with unlimited champagne and a hot air balloon ride over Greece.  What's your most embarrassing moment? I was in-depth stalking a guy's Facebook page and sent my friend a long, detailed text about my findings...except I sent it to him. Oops.  What is your greatest achievement to date? Getting my yoga certification because I've been able to inspire others to do yoga and become instructors!  "</span><span class="p">,</span>
     <span class="s">"eliminated"</span><span class="p">:</span> <span class="n">true</span><span class="p">,</span>
     <span class="s">"photo_url"</span><span class="p">:</span> <span class="s">"http://static.east.abc.go.com/service/image/ratio/id/90fe9103-e788-418d-9eb1-4204b7ba1b96/dim/690.1x1.jpg?cb=51351345661"</span><span class="p">,</span>
     <span class="s">"hometown_state"</span><span class="p">:</span> <span class="s">" NJ"</span><span class="p">,</span>
     <span class="s">"ethnicity"</span><span class="p">:</span> <span class="s">"caucasian"</span><span class="p">,</span>
     <span class="s">"goneweek"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
     <span class="s">"featured"</span><span class="p">:</span> <span class="n">true</span><span class="p">,</span>
     <span class="s">"featured_num"</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
     <span class="s">"intro_order"</span><span class="p">:</span> <span class="mi">21</span>
<span class="p">}</span></code></pre></figure>
<p>Some data properties are categorical (such as ethnicity and hair color); some data properties are ordinal (hair length), and some data properties are <a title="Wikipedia: Ratio Scale" href="http://en.wikipedia.org/wiki/Level_of_measurement#Ratio_scale">ratios</a> (such as age or number of tattoos). The Python script encodes categorical data into ordinal, numeric values through a lookup dictionary.</p>
<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">hair_length</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">({</span><span class="s">"neck"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
                    <span class="s">"shoulder"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
                    <span class="s">"chest"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
                    <span class="s">"stomach"</span><span class="p">:</span> <span class="mi">4</span><span class="p">})</span></code></pre></figure>
<p><strong>Workflow</strong></p>
<p>Given this sparse data, it's hard to train the machine.  More importantly, we can't break the data into training and testing data; because there's so little data, we need to use all of it.  For this reason, I decide to randomly pick 25% of the data for training the tree or SVM, then run these very same points through the classifier.  To offset this horrible violation of machine learning principals, I'll run the model a thousand times and see who gets mis-classified as "eliminated" the most.</p>
<p>Here's some code I wrote to train the samples:</p>
<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">week_predict</span><span class="p">(</span><span class="n">tgt_data</span><span class="p">,</span> <span class="n">elims</span><span class="p">,</span> <span class="n">tgt_week</span><span class="p">,</span> <span class="n">sc_learn</span><span class="p">):</span>
    <span class="s">"""
    Given some data, make some predictions and return the average accuracy.
    :param tgt_data: json structure of data to be formatted
    :param elims: eliminated
    :param tgt_week:
    :param sc_learn:
    :return:
    """</span>

    <span class="n">learn_values</span> <span class="o">=</span> <span class="n">data_formatter</span><span class="p">(</span><span class="n">tgt_data</span><span class="p">,</span> <span class="n">elims</span><span class="p">,</span> <span class="n">tgt_week</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="n">y</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span> <span class="o">&lt;</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">learn_values</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.25</span><span class="p">):</span>
        <span class="n">samples</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">learn_values</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">learn_arr</span> <span class="o">=</span> <span class="n">learn_values</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
    <span class="c"># print "Sample selection: ", samples</span>
    <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">:</span>
        <span class="n">x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">learn_arr</span><span class="p">[</span><span class="n">index</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">learn_arr</span><span class="p">[</span><span class="n">index</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>

    <span class="c"># These next two lines courtesy of:</span>
    <span class="c"># http://scikit-learn.org/stable/modules/tree.html</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">sc_learn</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">clf</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">except</span> <span class="nb">ValueError</span><span class="p">:</span>
        <span class="c"># Sometimes we try to train with 0 classes.</span>
        <span class="k">return</span> <span class="nb">dict</span><span class="p">({</span><span class="s">"accuracy"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s">"departures"</span><span class="p">:</span> <span class="p">[]})</span>
    <span class="n">c</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">departures</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">learn_values</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">learn_values</span><span class="p">[</span><span class="n">item</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span> <span class="o">!=</span> <span class="n">learn_values</span><span class="p">[</span><span class="n">item</span><span class="p">][</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">c</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">learn_values</span><span class="p">[</span><span class="n">item</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c"># if they aren't eliminated</span>
                <span class="n">departures</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="nb">round</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">learn_values</span><span class="p">)</span> <span class="o">-</span> <span class="n">c</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">learn_values</span><span class="p">))</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">ret_val</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">({</span><span class="s">"accuracy"</span><span class="p">:</span> <span class="n">accuracy</span><span class="p">,</span>
                    <span class="s">"departures"</span><span class="p">:</span> <span class="n">departures</span><span class="p">})</span>
    <span class="k">return</span> <span class="n">ret_val</span></code></pre></figure>
<p><strong>Output</strong></p>
<p>These are my leading predictors for departure.  For the decision tree, here we have it:</p>
<blockquote><p>Departing: Britt Nilsson votes: 640<br/>
Departing: Carly Waddell votes: 581<br/>
Departing: Jade Roper votes: 538<br/>
Departing: Becca Tilley votes: 520<br/>
Departing: Megan Bell votes: 506<br/>
Departing: Kaitlyn Bristowe votes: 365</p></blockquote>
<p>For the SVM, the output:</p>
<blockquote><p>Departing: Britt Nilsson votes: 749<br/>
Departing: Megan Bell votes: 722<br/>
Departing: Becca Tilley votes: 714<br/>
Departing: Jade Roper votes: 707<br/>
Departing: Carly Waddell votes: 629<br/>
Departing: Kaitlyn Bristowe votes: 619</p></blockquote>
<p>I've arranged each of these in descending order; the more votes someone has for departure, the more frequently they are miscategorized as eliminated based on who has already been eliminated and who is still there.</p>
<p><strong>Analysis</strong></p>
<p>Does this make sense given the current state of the contest? After discussing these predicted outcomes with expert viewers, they were adamant that Britt, the contestant I've pegged as being most likely to leave, has almost no chance of departing.  She made a strong impression on Chris and seems to have a strong connection with him, so for her to leave would be a shocker; on the other hand, many of these same viewers thought Kaitlyn had a good chance of winning, which would validate her low departure score.</p>
<p><strong>UPDATE: </strong></p>
<p>This blog post didn't make it to the Internet in time for these predictions to be evaluated in public.  As such, (spoiler alert) we know that Britt and Carly departed in Week 7, followed by Jade in Week 8.  As such, I will make some predictions for next week:</p>
<p>Decision Tree results:</p>
<blockquote><p>Departing: Becca Tilley: 3590<br/>
Departing: Whitney Bischoff: 3357<br/>
Departing: Kaitlyn Bristowe: 3320</p></blockquote>
<p>Support Vector Machine results:</p>
<blockquote><p>Departing: Becca Tilley: 1799<br/>
Departing: Whitney Bischoff: 1760<br/>
Departing: Kaitlyn Bristowe: 1715</p></blockquote>
<p><strong>Analysis 2.0</strong></p>
<p>We've trained an SVM with just a few variables that don't capture the essence of the problem very well.  Furthermore, we did no fine tuning for the parameters in the SVM, nor did we prune the decision tree.</p>
<p>In order to rectify the data issue, I have put out a call to fans of The Bachelor for additional data about any facet of the show, including:</p>
<ul>
<li>How many kisses per episode per contestant</li>
<li>Who gets what date?</li>
<li>Number of minutes of on-air time featured per episode, per contestant</li>
<li>Distance from contestant hometown to Arlington, Iowa</li>
<li>Any other data</li>
</ul>
<p><strong>Tools Used</strong></p>
<p>I used mostly stock Python libraries to gather data and produce these predictions; for web scraping, I used <a title="selenium" href="https://selenium-python.readthedocs.org/" target="_blank">selenium</a> and <a title="BeautifulSoup" href="http://www.crummy.com/software/BeautifulSoup/" target="_blank">BeautifulSoup</a>.  For machine learning, <a title="scikit-learn" href="http://scikit-learn.org/stable/" target="_blank">scikit-learn</a>.  For keeping the code clean, <a title="pylint" href="http://www.pylint.org/" target="_blank">pylint</a>.</p>
<p>Code is available at: <a title="https://github.com/jamesfe/bachelor_tree" href="https://github.com/jamesfe/bachelor_tree" target="_blank">https://github.com/jamesfe/bachelor_tree</a></p>
<p>If you have questions, feel free to tweet me them over to <a href="https://twitter.com/jimmysthoughts" target="_blank">@jimmysthoughts</a>!</p>

  </div>

</div></body></html>