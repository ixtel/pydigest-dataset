<html><body><div><div class="content html_format">
      <i>Данная статья представляет перевод главы, обучающей работе с текстовыми данными, из официальной документации <a href="http://scikit-learn.org/0.15/tutorial/text_analytics/working_with_text_data.html">scikit-learn</a>.</i>
<p>
Цель этой главы — это исследование некоторых из самых важных инструментов в scikit-learn на одной частной задаче: анализ коллекции текстовых документов (новостные статьи) на 20 различных тематик.</p><p>
В этой главе мы рассмотрим как:
</p><ul>
<li>загрузить содержимое файла и категории</li>
<li>выделить вектора признаков, подходящих для машинного обучения</li>
<li>обучить одномерную модель выполнять категоризацию</li>
<li>использовать стратегию grid search, чтобы найти наилучшую конфигурацию для извлечения признаков и для классификатора</li>
</ul>
<a name="habracut"/>
<h3>Инструкция по установке</h3><p>
Чтобы начать практическое занятие, описанное в данной главе, у вас должен быть установлен scikit-learn и все компоненты, от которых он зависит (numpy, Scipy).</p><p>
Чтобы получить инструкцию по установке и рекомендации для разных ОС перейдите на </p><a href="http://scikit-learn.org/stable/install.html#installation-instructions">эту страницу</a><p>. </p><p>
Локальную копию данного занятия вы можете найти в своей папке:
</p><i>scikit-learn/doc/tutorial/text_analytics/</i>
<b><i>Теперь scikit-learn не устанавливается с папкой doc/ и прочим содержимым. Вы можете ее скачать с <a href="https://github.com/scikit-learn/scikit-learn">github.</a></i></b><p>
Папка с обучающими примерами должна содержать следующие файлы и папки:
</p><ul>
<li>*.rst files — источник учебных документов, обработанных с помощью sphinx </li>
<li>data — папка для хранения наборов данных в процессе обучения</li>
<li>skeletons — образцы неполных скриптов для упражнений</li>
<li>solutions — решения упражнений</li>
</ul><p>
Также вы можете скопировать skeletons в новую папку в любое место на вашем жестком диске, названную sklearn_tut_workspace, где вы будете редактировать ваши собственные файлы для упражнений. Так изначальные skeletons останутся неизменными:
</p><pre><code>% cp -r skeletons work_directory/sklearn_tut_workspace</code></pre><p>
Алгоритмы машинного обучения нуждаются в данных. Зайдите в каждую подпапку $TUTORIAL_HOME/data и запустите оттуда скрипт fetch_data.py (для начала прочтите их).</p><p>
Например:
</p><pre><code>% cd $TUTORIAL_HOME/data/languages
% less fetch_data.py
% python fetch_data.py</code></pre>

<h3>Загрузка 20 новостных наборов данных</h3><p>
Набор данных называется “Twenty Newsgroups”. Вот его официальное описание, взятое с </p><a href="http://people.csail.mit.edu/jrennie/20Newsgroups/">сайта</a><p>: 
</p><blockquote>Данные «The 20 Newsgroups» — это коллекция примерно из 20000 новостных документов, разделенная (приблизительно) равномерно между 20 различными категориями. Насколько нам известно, изначально она собиралась Кеном Ленгом (Ken Lang), возможно, для его работы «Newsweeder: Learning to filter netnews» («Новостной обозреватель: учимся фильтровать новости из сети»), хотя он явно не заявлял об этом. Коллекция «The 20 newsgroups» стала популярным набором данных для экспериментов с техниками машинного обучения для текстовых приложений, таких как классификация текста или его кластеризация.</blockquote><p>
Далее мы будем использовать встроенный загрузчик наборов данных для выборки «The 20 newsgroups» из scikit-learn. Иначе, выборку можно загрузить вручную с вэб сайта, использовать функцию </p><a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html#sklearn.datasets.load_files">sklearn.datasets.load_files</a><p> и указав папку «20news-bydate-train» для сохранения распакованного архива.</p><p>
Чтобы первый пример быстрее исполнялся, мы будем работать только с частью нашего набора данных, разбитой на 4 категории из 20 возможных:
</p><pre><code class="python">&gt;&gt;&gt; categories = ['alt.atheism', 'soc.religion.christian',
...               'comp.graphics', 'sci.med']</code></pre><p>
Мы можем загрузить список файлов, совпадающих с нужными категориями, как показано ниже:
</p><pre><code class="python">&gt;&gt;&gt; from sklearn.datasets import fetch_20newsgroups
&gt;&gt;&gt; twenty_train = fetch_20newsgroups(subset='train',
...     categories=categories, shuffle=True, random_state=42)</code></pre><p>
Возвращаемый набор данных — это scikit-learn совокупность: одномерный контейнер с полями, которые могут интерпретироваться как ключи в словаре python (dict keys), проще говоря — как признаки объекта (object attributes). Например, target_names содержит список названий запрошенных категорий:
</p><pre><code class="python">&gt;&gt;&gt; twenty_train.target_names
['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']</code></pre><p>
Сами файлы загружаются в память как атрибут data. Вы также можете ссылаться на названия файлов:
</p><pre><code class="python">&gt;&gt;&gt; len(twenty_train.data)
2257
&gt;&gt;&gt; len(twenty_train.filenames)
2257</code></pre><p>
Давайте выведем на печать первые строки из первого загруженного файла:
</p><pre><code class="python">&gt;&gt;&gt; print("\n".join(twenty_train.data[0].split("\n")[:3]))
From: sd345@city.ac.uk (Michael Collier)
Subject: Converting images to HP LaserJet III?
Nntp-Posting-Host: hampton

&gt;&gt;&gt; print(twenty_train.target_names[twenty_train.target[0]])
comp.graphics</code></pre><p>
Алгоритмы для обучения с учителем (контролируемого обучения) требуют, чтобы у каждого документа в обучающей выборке была помета определенной категории. В нашем случае, категория — это название новостной выборки, которая «случайно» оказывается названием папки, содержащей характерные документы.</p><p>
Для увеличения скорости и эффективного использования памяти, scikit-learn загружает целевой атрибут как массив целых чисел, который соответствует индексу названия категории из списка target_names. Индекс категории каждой выборки хранится в атрибуте target:
</p><pre><code class="python">&gt;&gt;&gt; twenty_train.target[:10]
array([1, 1, 3, 3, 3, 3, 3, 2, 2, 2])</code></pre><p>
Можно получить название категории:
</p><pre><code class="python">&gt;&gt;&gt; for t in twenty_train.target[:10]:
...     print(twenty_train.target_names[t])
...
comp.graphics
comp.graphics
soc.religion.christian
soc.religion.christian
soc.religion.christian
soc.religion.christian
soc.religion.christian
sci.med
sci.med
sci.med</code></pre><p>
Вы можетет заметить, что выборки были рандомно перетасованы (с помощью рандомного сгенирированного числа — fixed RNG seed). Такой метод подойдет, если вы хотите использовать только первые выборки для быстрого обучения модели и если вы хотите получить общее представление о результатах перед последующим переобучением на полном наборе данных.

</p><h3>Извлечение характерных признаков из текстовых файлов</h3><p>
Чтобы использовать машинное обучение на текстовых документах, первым делом, нужно перевести текстовое содержимое в числовой вектор признаков. 
</p><h5>«Мешок слов» (набор слов)</h5><p>
Наиболее интуитивно понятный способ сделать описанное выше преобразование — это представить текст в виде набора слов:
</p><ol>
<li>приписать уникальный целочисленный индекс каждому слову, появляющемуся в документах в обучающей выборке (например, построив словарь из слов с целочисленными индексами).</li>
<li>для каждого документа #i посчитать количество употреблений каждого слова w и сохранить его (количество) в X[i, j]. Это будет значение признака #j, где j — это индекс слова w в словаре.</li>
</ol><p>
Представление «мешок слов» подразумевает, что n_features — это некоторое количество уникальных слов в корпусе. Обычно, это количество превышает 100000. </p><p>
Если n_samples == 10000, то Х, сохраненный как массив numpy типа float32, потребовал бы </p><b>10000 x 100000 x 4 bytes = 4GB оперативной памяти (RAM)</b><p>, что едва осуществимо в современным компьютерах.</p><p>
К счастью, </p><b>большинство значений в X являются нулями</b><p>, поскольку в одном документе используется менее чем пара сотен уникальных слов. Поэтому «мешок слов» чаще всего является </p><b>высоко размерным разреженным набором данных</b><p>. Мы можем сэкономить много свободной оперативки, храня в памяти только лишь ненулевые части векторов признаков.</p><p>
Матрицы scipy.sparse — это структуры данных, которые именно это и делают — структурируют данные. В scikit-learn есть встроенная поддержка этих структур.

</p><h3>Токенизация текста с scikit-learn</h3><p>
Предобработка текста, токенизация и отфильтровывание стоп-слов включены в состав высоко уровневого компонента, который позволяет создать словарь характерных признаков и перевести документы в векторы признаков:
</p><pre><code class="python">&gt;&gt;&gt; from sklearn.feature_extraction.text import CountVectorizer
&gt;&gt;&gt; count_vect = CountVectorizer()
&gt;&gt;&gt; X_train_counts = count_vect.fit_transform(twenty_train.data)
&gt;&gt;&gt; X_train_counts.shape
(2257, 35788)</code></pre>
<a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer">CountVectorizer</a><p> поддерживает подсчет N-грам слов или последовательностей символов. Векторизатор строит словарь индексов признаков:
</p><pre><code class="python">&gt;&gt;&gt; count_vect.vocabulary_.get(u'algorithm')
4690</code></pre><p>
Значение индекса слова в словаре связано с его частотой употребления во всем обучающем корпусе.

</p><h3>От употреблений к частотности</h3><p>
Подсчет словоупотреблений — это хорошее начало, но есть проблема: в длинных документах среднее количество словоупотреблений будет выше, чем в коротких, даже если они посвящены одной теме. </p><p>
Чтобы избежать этих потенциальных несоответствий, достаточно разделить количество употреблений каждого слова в документе на общее количество слов в документе. Этот новый признак называется tf — Частота термина.</p><p>
Следующее уточнение меры tf — это снижение веса слова, которое появляется во многих документах в корпусе, и отсюда является менее информативным, чем те, которые используются только в небольшой части корпуса. Примером низко ифнормативных слов могут служить служебные слова, артикли, предлоги, союзы и т.п.</p><p>
Это снижение называется </p><a href="http://en.wikipedia.org/wiki/Tf–idf">tf–idf</a><p>, что значит “Term Frequency times Inverse Document Frequency” (обратная частота термина).</p><p>
Обе меры </p><b>tf</b><p> и </p><b>tf–idf</b><p> могут быть вычислены следующим образом:
</p><pre><code class="python">&gt;&gt;&gt; from sklearn.feature_extraction.text import TfidfTransformer
&gt;&gt;&gt; tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)
&gt;&gt;&gt; X_train_tf = tf_transformer.transform(X_train_counts)
&gt;&gt;&gt; X_train_tf.shape
(2257, 35788)</code></pre><p>
В примере кода, представленного выше, мы сначала используем метод fit(..), чтобы прогнать наш алгоритм оценки на данных, а потом — метод transform(..), чтобы преобразовать нашу числовую матрицу к представлению tf-idf. Эти два шага могут быть объединены и дадут тот же результат на выходе, но быстрее, что можно сделать с помощью пропуска излишней обработки. Для этого нужно использовать метод fit_transform(..), как показано ниже:
</p><pre><code class="python">&gt;&gt;&gt; tfidf_transformer = TfidfTransformer()
&gt;&gt;&gt; X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
&gt;&gt;&gt; X_train_tfidf.shape
(2257, 35788)</code></pre>
<p>
…</p><p>
Продолжение будет в </p><a href="http://habrahabr.ru/post/266025/">части 2</a><p>.

      
      </p><p class="clear"/>
    </div>

    
  </div></body></html>