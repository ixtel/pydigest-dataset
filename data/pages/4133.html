<html><body><div><div class="post-content section-inner thin">
		    
		    	<p class="mailmunch-forms-before-post"/><p>Notification type products have slowly started to rise in popularity as we get closer to the “Internet of Everything”. By the year 2025 we’ll get cars that drive you to QFC because they noticed you’re running low on milk and regularly bought it for the last 30 weeks. The correlation of you getting your milk on time leads to you performing better at work and boosting your career growth by 25% over 10 years, or so your refrigerator will report to you every week so.</p>
<p>But before that becomes a reality, I’ve got to adjust and write my programs by hand to give me alerts. Companies like <a title="Dataminr" href="http://dataminr.com/" target="_blank">Dataminr</a> and <a title="IFTTT" href="https://ifttt.com/dashboard" target="_blank">IFTTT</a> have capitalized on the notification market but in different industries. (Dataminr mainly for alerting wallstreet people, IFTTT for the common-folk). But customized scraping really has some awesome benefits to it, and this is where I think <a title="Kimono" href="https://www.kimonolabs.com/" target="_blank">Kimono</a> has some upside.</p>
<p>The NBA playoffs are coming up and my Blazers are playing the Grizzlies . Albeit if Damian Lillard doesn’t become an absolute killer and go off, my interest will probably stagnate after a first round exit when the Blazers have been 0 for 4 against them this year. That being said, I know there’s a lot of people that are interested in watching all NBA games regardless of who they’re rooting for and especially when the game is getting close. Therefore, I decided this would be a cool demonstration of a scraping mechanism by creating a notification system for when games should be close enough to watch.</p>
<p>NBA real time scores already have an API but I thought I would make one using Kimono’s really easy scraping chrome extension anyway. (Note: I don’t work for them, they just have a cool product). First we have to grab the values we think are important on the <a title="ESPN NBA" href="http://espn.go.com/nba/" target="_blank">ESPN NBA</a> home page. That would be the home team name, away team name, scores, and the score time. <a href="https://github.com/jayfeng1/notifications" target="_blank">Full github code is here</a>.</p>
<div id="attachment_562" class="wp-caption aligncenter"><a href="http://i1.wp.com/www.racketracer.com/wp-content/uploads/2015/04/screenshot-2015-04-15-22-37-49.png"><img class=" wp-image-562" src="http://i1.wp.com/www.racketracer.com/wp-content/uploads/2015/04/screenshot-2015-04-15-22-37-49.png?resize=740%2C297" alt="Grabbing the Home Team Names" srcset="http://i1.wp.com/www.racketracer.com/wp-content/uploads/2015/04/screenshot-2015-04-15-22-37-49.png?resize=300%2C120 300w, http://i1.wp.com/www.racketracer.com/wp-content/uploads/2015/04/screenshot-2015-04-15-22-37-49.png?resize=1024%2C411 1024w, http://i1.wp.com/www.racketracer.com/wp-content/uploads/2015/04/screenshot-2015-04-15-22-37-49.png?w=1886 1886w" sizes="(max-width: 1103px) 100vw, 1103px" data-recalc-dims="1"/></a><p class="wp-caption-text">Grabbing the Home Team Names</p></div>
<p>Afterwards once the API is created, all we need to do is call the API to receive a json format of the results. Since we can set the API to scrape the site every fifteen minutes, we probably get a couple hits when it’s in the fourth quarter of a game. If that’s not frequent enough, create two more APIs and run them at intervals. But NBA games usually have a lot of time-outs, especially when the games are close, so we can probably live with an update every 15 minutes.</p>
<p/>
<div>
<pre><span>def</span> <span>wiregame</span>(games):
    wire = []
    <span>for</span> game <span>in</span> games:
        gt = game[<span>'time'</span>][<span>'text'</span>]
        <span>#Find out if the game is actually playing</span>
        <span>if</span> <span>':'</span> <span>in</span> gt <span>and</span> <span>'PT'</span> <span>not</span> <span>in</span> gt:
            time, quarter = <span>str</span>(gt).split(<span>'-'</span>)
            home = <span>int</span>(game[<span>'home_score'</span>][<span>'text'</span>])
            away = <span>int</span>(game[<span>'away_score'</span>][<span>'text'</span>])
            <span>#Check if it's the 4th quarter, under 6 minutes to go, and socre differential under 7 points. </span>
            <span>if</span> quarter.strip() == <span>'4th'</span> <span>and</span> <span>int</span>(time[<span>0</span>]) &lt; <span>6</span> <span>and</span> <span>abs</span>(home-away) &lt; <span>7</span>:
                wire.append(gt + <span>": "</span> + game[<span>'home_team'</span>][<span>'text'</span>] + <span>" "</span> + 
                    <span>str</span>(home) + <span>" - "</span> + game[<span>'away_team'</span>][<span>'text'</span>] + <span>" "</span> + 
                    <span>str</span>(away) + <span>". GAME IS COMING DOWN TO THE WIRE."</span>) 
    <span>return</span> wire
</pre>
</div>
<p>The reason I have the values and then another nested dictionary [‘text’] in there is because each value also has a link. The team’s and score’s text on the ESPN page all have links to different pages so it creates a nested dictionary of a ‘href’ value and a ‘text’ value in case you want both.</p>
<p>Awesome, now that we have this, we can set up a twitter account to tweet the games when they’re getting real close! I use Tweepy to authenticate and wrap the twitter account.</p><p class="mailmunch-forms-in-post-middle"/>
<p/>
<div>
<pre><span>import</span> <span>json</span>
<span>import</span> <span>urllib</span>
<span>import</span> <span>tweepy</span>

<span>#Authenticate the login</span>
<span>def</span> <span>login</span>():
    CONSUMER_KEY = <span>'YOUR_CONSUMER_KEY'</span>
    CONSUMER_SECRET = <span>'YOUR_CONSUMER_SECRET'</span>
    oauth_token = <span>'YOUR_OAUTH_TOKEN'</span>
    oauth_token_secret = <span>'YOUR_OAUTH_TOKEN_SECRET'</span>
    auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)
    auth.set_access_token(oauth_token, oauth_token_secret)    
    <span>return</span> auth

<span>if</span> __name__==<span>'__main__'</span>:
    url = <span>"https://www.kimonolabs.com/api/5c8gx236?apikey=[YOUR API KEY]"</span>  
    games = callApi(url)
    msgs = wiregame(games)
    api = tweepy.API(login())
    <span>#If there's a game going on, update the status</span>
    <span>if</span> <span>len</span>(msgs) &gt; <span>0</span>:
        <span>print</span> msgs
        <span>for</span> msg <span>in</span> msgs:
            api.update_status(status=msg) 
    <span>else</span>:
        <span>print</span> <span>'NO GAMES'</span>
</pre>
</div>
<p>You can follow the tweets right <a title="NBA_WATCHTIME" href="https://twitter.com/NBA_WATCHTIME" target="_blank">here </a>.</p>
<p>So it’s a pretty cool application of making a scraper into an API. I actually have mainly used Kimono to scrape for one time use and save them as CSVs. But I think there’s a lot of different use cases for it. Kimono’s a little buggy sometimes, but overall I think it’s pretty good. Some websites that have scraping restrictions don’t work with Kimono either as well as websites with lots of javascript on the client-side. Currently there seems to be application errors when associated with craigslist (Possible legal restriction?). Anyhow, that doesn’t exactly help me with my future apartment search though.</p>
<p>IFTTT has a couple of nifty applications to automate some cross-platform devices. In it’s recipe book they have hundreds of different apps that you can post, update, tweet, etc. when another app posts, updates, or tweets. They have one for finding <a title="Bedrooms in SF" href="https://ifttt.com/recipes/90257-2br-apartment-in-sf" target="_blank">2br apartments in SF</a> but it might be nicer to customize the requirements a bit more. I don’t think I want to live in a hacker house no matter how cheap it is and definitely want to see some pictures on the postings.</p>
<p>I previously wrote a scraper with scrapy to parse through craigslist postings and save them into a database <a href="http://racketracer.com/2015/01/29/practical-scraping-using-scrapy/" target="_blank">here</a>. This time I’m not too interested in modifying my original code too much though and want to try out some more BeautifulSoup instead. I grabbed some of the code from <a href="http://www.danielforsyth.me/finding-the-best-ticket-price-simple-web-scraping-with-python/" target="_blank">Daniel Forsyth’s blog on finding tickets</a> on craigslist. What I want to specify is any kind of place that will guarantee me under 1800 per bedroom in the SOMA or Potrero Hill district. I don’t care about the number of rooms as long as it’s within reason and the postings have to have pictures and a latitude and longitudinal location.</p>
<div>
<pre><span>def</span> <span>apartments</span>(num, soup, seen):
    results = []
    <span>#Find the list of craigslist postings</span>
    <span>for</span> listing <span>in</span> soup.find_all(<span>'p'</span>,{<span>'class'</span>:<span>'row'</span>}):
        <span>#Check to see if they have pictures and coordinate locations as well as if they've been sent out before</span>
        <span>if</span> listing.find(<span>'span'</span>,{<span>'class'</span>:<span>'p'</span>}).text == <span>' pic map'</span> <span>and</span> listing.get(<span>"data-pid"</span>) <span>not</span> <span>in</span> seen:
            text = listing.text.split(<span>"  $"</span>)[<span>1</span>]
            price, beds = text.split(<span>" "</span>)[<span>0</span>], text.split(<span>" "</span>)[<span>2</span>][<span>0</span>]
            <span>#Divide to check if it's within the proper range on a per bedroom basis</span>
            <span>if</span> <span>float</span>(price)/<span>float</span>(beds) &lt; num <span>and</span> <span>float</span>(price)/<span>float</span>(beds) &gt; <span>float</span>(num)/<span>2</span>:
                results.append(listing.text)
                <span>#Add ID to the SET to make sure it doesn't get sent out twice</span>
                seen.add(listing.get(<span>"data-pid"</span>))
    <span>return</span> results
</pre>
</div>
<p>Cool stuff, here’s the rest of the code set up with an SMTP server to email the results directly to my email address. On the apartments module, I also tried to specify a low just in case people are putting up the 1 dollar bullshit that gets on Craigslist some of the time.</p>
<div>
<pre><span>import</span> <span>requests</span>
<span>from</span> <span>bs4</span> <span>import</span> BeautifulSoup
<span>from</span> <span>urlparse</span> <span>import</span> urljoin
<span>import</span> <span>smtplib</span>
<span>import</span> <span>csv</span>

<span>def</span> <span>connect</span>(email, password):
    server = smtplib.SMTP(<span>'smtp.gmail.com'</span>, <span>587</span>)
    server.ehlo()
    server.starttls()
    server.login(email, password)
    <span>return</span> server

<span>def</span> <span>email</span>(sender, receivers, listings, server):
    <span>for</span> listing <span>in</span> listings:
        message = <span>"""From: From Person &lt;from@fromdomain.com&gt;</span>
<span>        To: To Person &lt;to@todomain.com&gt;</span>
<span>        Subject: NEW CRAIGSLIST POSTING</span>
<span>        """</span> + listing.text
        <span>try</span>:
           server.sendmail(sender, receivers, message)         
           <span>print</span> <span>"Successfully sent email"</span>
        <span>except</span>:
           <span>print</span> <span>"Error: unable to send email"</span>

<span>def</span> <span>getSoup</span>(url):
    response = requests.get(url)
    <span>return</span> BeautifulSoup(response.content)

#Open a csv file containing a set of craigslist IDs previously sent out
<span>def</span> <span>openSet</span>(path):
    reader = csv.reader(<span>open</span>(path,<span>'rb'</span>))
    <span>return</span> <span>set</span>(<span>list</span>(reader)[<span>0</span>])
    
<span>if</span> __name__==<span>'__main__'</span>:
    URL = <span>'http://sfbay.craigslist.org/search/sfc/apa?hasPic=1&amp;nh=25&amp;nh=1&amp;bedrooms=2'</span>
    path = <span>'set1.csv'</span>
    soup = getSoup(URL)
    seen = openSet(path)
    listings = apartments(<span>1800</span>, soup, seen)
    <span>if</span> <span>len</span>(listings) &gt; <span>0</span>:
        server = connect(<span>"YOUR_EMAIL"</span>, <span>"YOUR_PASSWORD"</span>)
        email(<span>"YOUR_EMAIL"</span>, [<span>'jayfeng1@uw.edu'</span>], listings, server)
    cw = csv.writer(<span>open</span>(<span>"set1.csv"</span>,<span>'wb'</span>))
    cw.writerow(listings)
</pre>
</div>
<p>Hopefully this will inspire some people started on the right track to customize some scrapers a little more. I love scraping, it’s absolutely the best and let’s people pursue their dreams and goals. Finding cheaper housing is good. But IFTTT has done a pretty good job on craigslist. If you look more into craigslist’s filters for housing, they can actually specify pictures or not, specific titles, and neighborhoods that you might want to live in. Once you have it drilled down, all you need is an email blast for each new listing. So I like where IFTTT is heading. But I also like where Kimono is going to because they can also get into this notification marketplace too with their customized APIs. If I had the money to invest I think I would.</p>
<p class="mailmunch-forms-after-post"/>
<p id="jp-relatedposts" class="jp-relatedposts">
	<h3 class="jp-relatedposts-headline"><em>Related</em></h3>
</p>
		    	<p class="clear"/>
		    	
		    			    
		    </div>
		    
			</div></body></html>