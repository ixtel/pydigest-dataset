<html><body><div><article class="col-md-10 col-md-offset-1">
            


<p>Ever heard people at your office talking about AUC, ROC, or TPR but been too
shy to ask what the heck they're talking about? Well lucky for you we're going
to be diving into the wonderful world of binary classification evaluation today. In
particular, we'll be discussing <a href="http://en.wikipedia.org/wiki/Receiver_operating_characteristic">ROC curves</a>.</p>
<p>ROC curves are a great technique that have been around for a while and is still 
one of the tried and true industry standards. So sit back, relax, and get ready
to dive into a world of <a href="http://en.wikipedia.org/wiki/Three-letter_acronym">3 letter acronyms!</a></p>
<h3>What is it?</h3>
<p>Receiving Operating Characteristic, or ROC, is a visual way for inspecting
the performance of a binary classifier (0/1). In particular, it's comparing the
rate at which your classifier is making correct predictions (True Positives or TP) and
the rate at which your classifier is making false alarms (False Positives or FP). 
When talking about True Positive Rate (TPR) or False Positive Rate (FPR) we're referring to the definitions below:</p>
<p>$$TPR = TruePositives/(True Positives + False Negatives)$$</p>
<p>$$FPR = FalsePositives/(FalsePositives + True Negatives)$$</p>
<p>You might have heard of True Positives and True Negatives referred to as <a href="http://en.wikipedia.org/wiki/Sensitivity_and_specificity">Sensitivity and Specificity</a>. No matter what you call it, the big point here is we're measuring the trade off between 
the rate at which you can correctly predict something, with the rate at which you
make an embarrassing blunder and predict something that doesn't happen.</p>
<p/><center>
    <iframe src="https://www.youtube.com/embed/_IRQ0RjJ20Q" frameborder="0" allowfullscreen="">VIDEO</iframe>
    <p>Don't let your classifier embarrass you.</p>
</center>
<h3>A brief history primer</h3>
<p>ROC curves were first used during WWII to analyze radar effectiveness. In the 
early days of radar, it was sometimes hard to tell a bird from a plane. The 
British pioneered using ROC curves to optimize the way that they relied on 
radar for detecting incoming German planes.</p>
<p><img alt="" src="../static/img/british-aa-spotter.jpg"/>
</p><center>Blast, birds again.</center>
<h3>ROC curves: the basics</h3>
<p>Sound confusing? Let's go over a few simple rules for reading an ROC curve. To 
do this, I'm going to present some "pre-canned" charts that will show extreme 
situations that should make it easier to understand what other ROC curves are "saying".</p>
<h4>Guessing</h4>
<p>The first example is the simplest: a diagonal line. A diagonal line indicates 
that the classifier is just making completely random guesses. Since your 
classifier is only going to be correct 50% of the time, it stands to reason that
your TPR and FPR will also be equal.</p>
<p><img alt="" src="../static/img/roc-guessing.png"/></p>
<p>Often times, ROC charts will include the random ROC curve to provide the user 
with a benchmark for what a naive classifier would do. Any curves above the line
are better than guessing, while those below the line...well you're better off
guessing.</p>
<h4>A Perfect Classifier</h4>
<p>We know what a totally random classifier looks like, but what about a 
PERFECT classifier--i.e. something that makes every prediction correctly. Well 
if you're lucky enough to have a perfect classifier, then you'll also have a 
perfect trade-off between TPR and FPR (meaning you'll have a TPR of 1 and an
FPR of 0). In that case, your ROC curve looks something like this.</p>
<p><img alt="" src="../static/img/roc-perfect.png"/>
</p><center>Note the "random curve" is included as a benchmark as a dotted line.</center>
<h4>Worse than guessing</h4>
<p>So we know what a random classifier looks like and what a perfect classifier
looks like, but what about a bad classifier? A bad classifier (i.e. something
that's <em>worse than guessing</em>) will appear below the random line. This, my 
friend, is absolute garbage. Throw it away...now!</p>
<p><img alt="" src="../static/img/roc-bad.png"/></p>
<h4>Better than guessing</h4>
<p>A much more interesting activity is attempting to decipher the difference between
an "OK" and a "Good" classifier. The chart below shows an example of a very 
mediocre classifier. Context is everything of course, but there's not much lift
here. In addition, be <em>very wary</em> of lines that dip or are very geometric 
looking. I've found that in practice, this can mean that there's an irregularity
with your data, or you're making a very bad assumption in your model.</p>
<p><img alt="" src="../static/img/roc-ok.png"/></p>
<h4>Pretty good</h4>
<p>Ahh this is looking a little better. Below you can see a nice "hump shaped" (it's a technical term)
curve that's continually increasing. It sort of looks like it's being yanked
up into that top left (the perfect) spot of the chart.</p>
<p><img alt="" src="../static/img/roc-pretty-good.png"/></p>
<h3>Area under the curve (AUC)</h3>
<p>So it turns out that the "hump shaped-ness" actually has a name: AUC or
Area Under the Curve. One can't really give an overview of ROC curves without mentioning AUC. The good
news is it's exactly what it sounds like--the amount of space underneath the ROC curve. You 
can think of the AUC as sort of a holistic number that represents how well
your TPR and FPR is looking in aggregate.</p>
<p>To make it super simple:</p>
<ul>
<li>AUC=0 -&gt; BAD</li>
<li>AUC=1 -&gt; GOOD</li>
</ul>
<p>So in the context of an ROC curve, the more "up and left" it looks, the larger
the AUC will be and thus, the better your classifier is.  Comparing AUC values is also really useful when comparing different models, as we can select the model with the high AUC value, rather than just look at the curves. </p>
<p><img alt="" src="../static/img/roc-auc.png"/></p>
<h3>Calculating an ROC Curve in Python</h3>
<p><code>scikit-learn</code> makes it super easy to calculate ROC Curves. But first things 
first: to make an ROC curve, we first need a classification model to evaluate.
For this example, I'm going to make a synthetic dataset and then build a 
logistic regression model using <code>scikit-learn</code>.</p>
<pre><code>from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression

X, y = make_classification(n_samples=10000, n_features=10, n_classes=2, n_informative=5)
Xtrain = X[:9000]
Xtest = X[9000:]
ytrain = y[:9000]
ytest = y[9000:]

clf = LogisticRegression()
clf.fit(Xtrain, ytrain)
</code></pre>
<p>Ok, now that we have our model we can calculate the ROC curve. Pretty easy--from
scikit-learn import <code>roc_curve</code>, pass in the actual y values from our test set
and the predicted probabilities for those same records.</p>
<p>The results will yield your FPR and TPR. Pass those into a <code>ggplot</code> and BAM!
You've got yourself a nice looking ROC curve.</p>
<pre><code>from sklearn import metrics
import pandas as pd
from ggplot import *

preds = clf.predict_proba(Xtest)[:,1]
fpr, tpr, _ = metrics.roc_curve(ytest, preds)

df = pd.DataFrame(dict(fpr=fpr, tpr=tpr))
ggplot(df, aes(x='fpr', y='tpr')) +\
    geom_line() +\
    geom_abline(linetype='dashed')
</code></pre>
<p><img alt="" src="../static/img/roc-python.png"/>
</p><center>Voil√†! Your very own ROC curve</center>
<p>Finally to calculate the AUC:</p>
<pre><code>auc = metrics.auc(fpr,tpr)
ggplot(df, aes(x='fpr', ymin=0, ymax='tpr')) +\
    geom_area(alpha=0.2) +\
    geom_line(aes(y='tpr')) +\
    ggtitle("ROC Curve w/ AUC=%s" % str(auc))
</code></pre>
<p><img alt="" src="../static/img/roc-with-auc-python.png"/>
We get 0.900.  Recalling from earlier, AUC is bounded between 0 and 1, so this is pretty good.</p>
<h3>Calculating an ROC Curve in R</h3>
<p>Making ROC curves in R is easy as well. I highly recommend using the <a href="http://rocr.bioinf.mpi-sb.mpg.de/ROCR.pdf">ROCR</a>
package. It does all of the hard work for you and makes some pretty nice 
looking charts.</p>
<p>For the model, we're going to build a classifier that uses a logistic 
regression model to predict if a record from the <code>diamonds</code> dataset is
over $2400.</p>
<pre><code>library(ggplot2)

diamonds$is_expensive &lt;- diamonds$price &gt; 2400
is_test &lt;- runif(nrow(diamonds)) &gt; 0.75
train &lt;- diamonds[is_test==FALSE,]
test &lt;- diamonds[is_test==TRUE,]

summary(fit &lt;- glm(is_expensive ~ carat + cut + clarity, data=train))
</code></pre>
<p>Using ROCR, making the charts is relatively simple.</p>
<pre><code>library(ROCR)

prob &lt;- predict(fit, newdata=test, type="response")
pred &lt;- prediction(prob, test$is_expensive)
perf &lt;- performance(pred, measure = "tpr", x.measure = "fpr")
# I know, the following code is bizarre. Just go with it.
auc &lt;- performance(pred, measure = "auc")
auc &lt;- auc@y.values[[1]]

roc.data &lt;- data.frame(fpr=unlist(perf@x.values),
                       tpr=unlist(perf@y.values),
                       model="GLM")
ggplot(roc.data, aes(x=fpr, ymin=0, ymax=tpr)) +
    geom_ribbon(alpha=0.2) +
    geom_line(aes(y=tpr)) +
    ggtitle(paste0("ROC Curve w/ AUC=", auc))
</code></pre>
<p><img alt="" src="../static/img/roc-with-auc-r.png"/></p>
<h3>Closing Thoughts</h3>
<p>Well that about does it for ROC curves. For more reading material, check out
these resources below:</p>


        </article>
    </div></body></html>