<html><body><div><div class="content html_format">
      <i>Данная статья представляет собой перевод введения в машинное обучение, представленное на официальном сайте <a href="http://scikit-learn.org/stable/tutorial/basic/tutorial.html">scikit-learn</a>.</i>
<p>
В этой части мы поговорим о терминах </p><a href="https://ru.wikipedia.org/wiki/%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5">машинного обучения</a><p>, которые мы используем для работы с scikit-learn, и приведем простой пример обучения.

</p><h4>Машинное обучение: постановка вопроса</h4><p>
В общем, задача машинного обучения сводится к получению набора </p><a href="https://ru.wikipedia.org/wiki/Выборка">выборок данных</a><p> и, в последствии, к попыткам предсказать свойства неизвестных данных. Если каждый набор данных — это не одиночное число, а например, многомерная сущность (multi-dimensional entry или </p><a href="http://en.wikipedia.org/wiki/Multivariate_random_variable">multivariate</a><p> data), то он должен иметь несколько признаков или фич.
</p><p>
Машинное обчение можно разделить на несколько больших категорий: 
</p><ul>
<li><a href="https://ru.wikipedia.org/wiki/Обучение_с_учителем">обучение с учителем</a> (или управляемое обучение). Здесь данные представлены вместе с дополнительными признаками, которые мы хотим предсказать. (<a href="http://scikit-learn.org/stable/supervised_learning.html#supervised-learning">Нажмите сюда</a>, чтобы перейти к странице Scikit-Learn обучение с учителем). Это может быть любая из следующих задач:</li>
</ul>
<ol>
<li><a href="https://ru.wikipedia.org/wiki/Задача_классификации">классификация</a>: выборки данных принадлежат к двум или более классам и мы хотим научиться на уже размеченных данных предсказывать класс неразмеченной выборки. Примером задачи классификации может стать распознавание рукописных чисел, цель которого — присвоить каждому входному набору данных одну из конечного числа дискретных категорий. Другой способ понимания классификации — это понимание ее в качестве дискретной (как противоположность непрерывной) формы управляемого обучения, где у нас есть ограниченное количество категорий, предоставленных для N выборок; и мы пытаемся их пометить правильной категорией или классом.</li>
<li><a href="https://ru.wikipedia.org/wiki/Регрессионный_анализ">регрессионный анализ</a>: если желаемый выходной результат состоит из одного или более непрерывных переменных, тогда мы сталкиваемся с регрессионным анализом. Примером решения такой задачи может служить предсказание длинны лосося как результата функции от его возраста и веса.</li>
</ol>

<ul>
<li><a href="https://ru.wikipedia.org/wiki/Обучение_без_учителя">обучение без учителя </a>(или самообучение). В данном случае обучающая выборка состоит из набора входных данных Х без каких-либо соответствующих им значений. Целью подобных задач может быть определение групп схожих элементов внутри данных. Это называется <a href="https://ru.wikipedia.org/wiki/Кластерный_анализ">кластеризацией</a> или кластерным анализом. Также задачей может быть установление распределения данных внутри пространства входов, называемое густотой ожидания (<a href="http://en.wikipedia.org/wiki/Density_estimation">density estimation</a>). Или это может быть выделение данных из высоко размерного пространства в двумерное или трехмерное с целью визуализации данных. (<a href="http://scikit-learn.org/stable/unsupervised_learning.html#unsupervised-learning">Нажмите сюда</a>, чтобы перейти к странице Scikit-Learn обучение без учителя).</li>
</ul>
<a name="habracut"/>
<h4>Обучающая выборка и контрольная выборка</h4><p>
Машинное обучение представляет собой обучение выделению некоторых свойств выборки данных и применение их к новым данным. Вот почему общепринятая практика оценки алгоритма в Машинном обучении — это разбиение данных вручную на два набора данных. Первый из них — это обучающая выборка, на ней изучаются свойства данных. Второй — </p><b>контрольная выборка</b><p>, на ней тестируются эти свойства.

</p><h4>Загрузка типовой выборки</h4><p>
Scikit-learn устанавливается вместе с несколькими стандартными выборками данных, например,</p><a href="http://en.wikipedia.org/wiki/Iris_flower_data_set"> iris</a><p> и </p><a href="http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits">digits</a><p> для классификации, и </p><a href="http://archive.ics.uci.edu/ml/datasets/Housing">boston house prices</a><p> dataset для регрессионного анализа.
</p><p>
Далее мы запускам Python интерпретатор из командной строки и загружаем выборки iris и digits. Установим условные обозначения: $ означает запуск интерпретатора Python, а &gt;&gt;&gt; обозначает запуск командной строки Python:

</p><pre><code class="python">$ python
&gt;&gt;&gt; from sklearn import datasets
&gt;&gt;&gt; iris = datasets.load_iris()
&gt;&gt;&gt; digits = datasets.load_digits()</code></pre><p>
Набор данных — это объект типа «словарь», который содержит все данные и некоторые метаданных о них. Эти данные хранятся с расширением .data, например, массивы n_samples, n_features. При машинном обучении с учителем одна или более зависимых переменных хранятся с расширением .target. Для получения более полной информации о наборах данных перейдите в </p><a href="http://scikit-learn.org/stable/datasets/index.html#datasets">соответствующий раздел</a><p>.
</p><p>
Например, набор данных digits.data дает доступ к фичам, которые можно использовать для классификации числовых выборок:

</p><pre><code class="python">&gt;&gt;&gt; print(digits.data)  
[[  0.   0.   5. ...,   0.   0.   0.]
 [  0.   0.   0. ...,  10.   0.   0.]
 [  0.   0.   0. ...,  16.   9.   0.]
 ...,
 [  0.   0.   1. ...,   6.   0.   0.]
 [  0.   0.   2. ...,  12.   0.   0.]
 [  0.   0.  10. ...,  12.   1.   0.]]</code></pre><p>
а digits.target дает возможность определить в числовой выборке, какой цифре соответствует каждое числовое представление, чему мы и будем обучаться:

</p><pre><code class="python">&gt;&gt;&gt; digits.target
array([0, 1, 2, ..., 8, 9, 8])</code></pre>
<h4>Форма массива данных</h4><p>
Обычно, данные представлены в виде двухмерного массива, такую форму имеют n_samples, n_features, хотя исходные данные могут иметь другую форму. В случае с числами, каждая исходная выборка — это представление формой (8, 8), к которому можно получить доступ, используя:

</p><pre><code class="python">&gt;&gt;&gt; digits.images[0]
array([[  0.,   0.,   5.,  13.,   9.,   1.,   0.,   0.],
       [  0.,   0.,  13.,  15.,  10.,  15.,   5.,   0.],
       [  0.,   3.,  15.,   2.,   0.,  11.,   8.,   0.],
       [  0.,   4.,  12.,   0.,   0.,   8.,   8.,   0.],
       [  0.,   5.,   8.,   0.,   0.,   9.,   8.,   0.],
       [  0.,   4.,  11.,   0.,   1.,  12.,   7.,   0.],
       [  0.,   2.,  14.,   5.,  10.,  12.,   0.,   0.],
       [  0.,   0.,   6.,  13.,  10.,   0.,   0.,   0.]])</code></pre>
<a href="http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#example-classification-plot-digits-classification-py">Следующий простой пример с этим набором данных</a><p> иллюстрирует, как, исходя из поставленной задачи, можно сформировать данные для использования в scikit-learn.

</p><h4>Обучение и прогнозирование</h4><p>
В случае с числовым набором данных цель обучения — это предсказать, принимая во внимание представление данных, какая цифра изображена. У нас есть образцы каждого из десяти возможных классов (числа от 0 до 9), на которым мы обучаем алгоритм оценки (estimator), чтобы он мог предсказать класс, к которому принадлежит неразмеченный образец.
</p><p>
В scikit-learn алгоритм оценки для классификатора — это Python объект, который исполняет методы fit(X, y) и predict(T). Пример алгоритма оценки — это класс sklearn.svm.SVC выполняет классификацию </p><a href="https://ru.wikipedia.org/wiki/Метод_опорных_векторов">методом опорных векторов</a><p>. Конструктор алгоритма оценки принимает в качестве аргументов параметры модели, но для сокращения времени, мы будем рассматривать этот алгоритм как черный ящик:

</p><pre><code class="python">&gt;&gt;&gt; from sklearn import svm
&gt;&gt;&gt; clf = svm.SVC(gamma=0.001, C=100.)</code></pre>

<h4>Выбор параметров для модели</h4><p>
В этом примере мы установили значение gamma вручную. Также можно автоматически определить подходящие значения для параметров, используя такие инструменты как </p><a href="http://scikit-learn.org/stable/modules/grid_search.html#grid-search">grid search</a><p> и </p><a href="http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation">cross validation</a><p>.
</p><p>
Мы назвали экземпляр нашего алгоритма оценки clf, так как он является классификатором. Теперь он должен быть применен к модели, т.е. он должен обучится на модели. Это осуществляется путем прогона нашей обучающей выборки через метод fit. В качестве обучающей выборки мы можем использовать все представления наших данных, кроме последнего. Мы сделали эту выборку с помощью синтаксиса Python [:-1], что создало новый массив, содержащий все, кроме последней, сущности из digits.data:

</p><pre><code class="python">&gt;&gt;&gt; clf.fit(digits.data[:-1], digits.target[:-1])  
SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,
  gamma=0.001, kernel='rbf', max_iter=-1, probability=False,
  random_state=None, shrinking=True, tol=0.001, verbose=False)</code></pre><p>
Теперь можно предсказать новые значения, в частности, мы можем спросить классификатор, какое число содержится в последнем представлении в наборе данных digits, которое мы не использовали в обучении классификатора:
</p><pre><code class="python">&gt;&gt;&gt; clf.predict(digits.data[-1])
array([8])</code></pre><p>
Соответствующее изображение представлено ниже:

</p><img src="https://habrastorage.org/getpro/habr/post_images/8cf/8ff/85c/8cf8ff85cc3c5b71775f7ffcaccf86af.png" alt="image"/>
<p>
Как вы можете видеть, это сложная задача: представление в плохом разрешении. Вы согласны с классификатором?
</p><p>
Полное решение этой задачи классификации доступно в качестве примера, который вы можете запустить и изучить: </p><a href="http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#example-classification-plot-digits-classification-py">Recognizing hand-written digits</a><p>.

</p><h4>Сохранение модели</h4><p>
В scikit модель можно сохранить, используя встроенный модуль, названный </p><a href="http://docs.python.org/library/pickle.html">pickle</a><p>:

</p><pre><code class="python">&gt;&gt;&gt; from sklearn import svm
&gt;&gt;&gt; from sklearn import datasets
&gt;&gt;&gt; clf = svm.SVC()
&gt;&gt;&gt; iris = datasets.load_iris()
&gt;&gt;&gt; X, y = iris.data, iris.target
&gt;&gt;&gt; clf.fit(X, y)  
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,
  kernel='rbf', max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False)

&gt;&gt;&gt; import pickle
&gt;&gt;&gt; s = pickle.dumps(clf)
&gt;&gt;&gt; clf2 = pickle.loads(s)
&gt;&gt;&gt; clf2.predict(X[0])
array([0])
&gt;&gt;&gt; y[0]
0</code></pre><p>
В частном случае применения scikit, может быть полезнее заметить pickle на библиотеку joblib (joblib.dump &amp; joblib.load), которая более эффективна для работы с большим объемом данных, но она позволяет сохранять модель только на диске, а не в строке:

</p><pre><code class="python">&gt;&gt;&gt; from sklearn.externals import joblib
&gt;&gt;&gt; joblib.dump(clf, 'filename.pkl') </code></pre><p>
Потом можно загрузить сохраненную модель(возможно в другой Python процесс) с помощью:
</p><pre><code class="python">&gt;&gt;&gt; clf = joblib.load('filename.pkl') </code></pre><p>
Обратите внимание, что joblib.dump возвращает список имен файлов. Каждый отдельный массив numpy, содержащийся в clf объекте, сеарилизован как отдельный файл в файловой системе. Все файлы должны находиться в одной папке, когда вы снова загружаете модель с помощью joblib.load.
</p><p>
Обратите внимание, что у pickle есть некоторые проблемы с безопасностью и сопровождением. Для получения более детальной информации о хранении моделей в scikit-learn обратитесь к секции </p><a href="http://scikit-learn.org/stable/modules/model_persistence.html#model-persistence">Model persistence</a><p>.
      </p><p class="clear"/>
    </div>

    
  </div></body></html>