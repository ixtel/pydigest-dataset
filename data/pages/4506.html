<html><body><div><div class="col-sm-12 post-content"> <p class="page-post-summary"> Trying to find an online course for my last college class was (avoidably) difficult. Each institution was listed one at a time in a drop down and then presented a huge table to match registration numbers. Using web scraping could make this so much easier. </p> <p><strong>requests.py</strong> (<a href="http://docs.python-requests.org/en/latest/">source</a>) is a convenience wrapper around <a href="https://docs.python.org/2/library/urllib.html"><code>urllib</code></a> to make structured HTTP request with Python easier. Unlike more advanced methods of mining that potentially do full DOM rendering with JavaScript support, such as <a href="http://selenium.googlecode.com/git/docs/api/py/api.html">Selenium</a> or <a href="http://jeanphix.me/Ghost.py/">Ghost.py</a>, <code>requests</code> keeps it simple (and fast!) by focusing on getting you the data and allowing us to process it another way.</p> <h3>Contents</h3> <ol> <li><a href="/blog/mining-for-credits-web-scraping-with-requests.html#dataSource">The Data Source</a></li> <li><a href="/blog/mining-for-credits-web-scraping-with-requests.html#theEnvironment">The Environment</a></li> <li><a href="/blog/mining-for-credits-web-scraping-with-requests.html#whyRequests">Why requests.py</a></li> <li><a href="/blog/mining-for-credits-web-scraping-with-requests.html#helperLibraries">Helper Libraries</a></li> <li><a href="/blog/mining-for-credits-web-scraping-with-requests.html#theFlow">Defining a Flow</a></li> <li><a href="/blog/mining-for-credits-web-scraping-with-requests.html#ourEnvironment">Our Environment</a></li> <li><a href="blog/mining-for-credits-web-scraping-with-requests.html#envAnalysis">Analysis</a></li> <li><a href="blog/mining-for-credits-web-scraping-with-requests.html#envPython">Python Scripts</a></li> <li><a href="/blog/mining-for-credits-web-scraping-with-requests.html#testRun">Test Run</a></li> <li><a href="/blog/mining-for-credits-web-scraping-with-requests.html#finalRun">Final Run</a></li> <li><a href="/blog/mining-for-credits-web-scraping-with-requests.html#conclusion">Conclusion</a></li> </ol> <p><a name="dataSource"/></p> <h2>The Data Source</h2> <p>Before any works is done setting up an automation environment to mine data, you have to ask yourself a few questions:</p> <ol> <li><strong>Is there enough data to justify automation?</strong></li> </ol> <p>Setting up a web scraping environment is pretty cool, and it's really neat to see the raw data coming back into "your world" so you can manipulate and present it how you want. However, before spending all the time and effort in the preparation stages, ask yourself if there's enough data needed to justify the investment. </p> <ol> <li><strong>Will the information be in a "minable" format?</strong></li> </ol> <p>The time is worth it, and there's lots of data...but, does that mean we can actually get to it? Not all web pages are data access friendly. Content is obscured in JavaScript code, retrieved from APIs, hidden behind pay walls, and some will make you solve a quiz or <a href="http://www.captcha.net/">captcha</a> before you can read. Sometimes these things make it near impossible to get the information automatically. It's their choice, which is fine, just do a little research beforehand to ensure it's worth the time before hitting a dead end.</p> <ol> <li><strong>Coding vs. Manually searching time commitment.</strong></li> </ol> <p>The time it takes to set up an automated mining environment is roughly the same for each project, but the return value scales with the <em>volume of pages you're trying to keep.</em> After a few attempts, getting a sense of when and when-not to use these tools will come easier. For now, just use them until you're familiar with their limitations :)</p> <ol> <li><strong>Will automation raise flags in the system? Or, other limitations.</strong></li> </ol> <p>Assuming everything above is good-to-go, the last thing you need to analyze before proceeding is whether or not automation will keep you from getting what you need. These can include grabbing the data too quickly, too many concurrent sessions, constantly logging in (use sessions and state), too many failed page loads, etc. Some websites don't want you to scrape their data and do everything they can to keep that from happening. If your activities start throwing too many flags you may not be able to retrieve the data at all; even by hand. Analyze the situation if you need to, check headers and skim the page source for trackers. Doing a few minutes of analysis could save the headache of getting your Selenium Grid shutdown.</p> <p><a name="theEnvironment"/></p> <h2>The Environment</h2> <p>A fun thing about web scraping is that it's like getting the information by hand but scaled dozens of times faster. A fun thing about automation is that you can scale it dozens of times again over multiple machines and Internet connections. The Environment for your scripts will determine how fast you can get the data, how easy it is to store and retrieve, and the majority of the difficulty in setting up the program to mine.</p> <p>To start don't worry about multiple machines, or even multiple browser instances on the same machine. Focus on getting one page at a time working correctly and then determine if you need more data faster.</p> <p>Our environment will follow this principle. To start we're using one browser instance and going one page at a time.</p> <p><a name="whyRequests"/></p> <h2>Why requests.py</h2> <p>The primary reason I try to use requests before heavier libraries in data mining is its low footprint and raw speed. Without the incurred overhead of DOM and JavaScript manipulation it's much easier to point at an endpoint and collect the data for later.</p> <p>Data mining based on raw HTTP requests is much easier to deploy on low power and headless machines, such as the <a href="http://www.amazon.com/gp/product/B00T2U7R7I/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=B00T2U7R7I&amp;linkCode=as2&amp;tag=blakinteprog-20&amp;linkId=VNURUCLCH36MLGBX">RaspberryPi</a> or a small <a href="https://www.digitalocean.com/?refcode=70ebeb6d7781">DigitalOcean</a> instance.</p> <p>The module also assists in keeping track of cookies and session state, and is capable of being serialized to disk. The ability to serialize the mining state allows tests to stop mid-run and start them again from that exact position later. It also allows you to clone instances after you've logged in, or loaded a heavy resource, to bypass session based limitations on the client or server side.</p> <p><a name="helperLibraries"/></p> <h2>Helper Libraries</h2> <p>When data mining it's always important to have a few helper libraries that can assist the collection methods with parsing and analysis. Because <a href="http://docs.python-requests.org/en/latest/">requests</a> excels at grabbing HTML/XML/JSON a tool like <a href="https://pythonhosted.org/pyquery/">pyQuery</a> can help coerce, analyze, and query those responses from a different angle. pyQuery behaves a lot like jQuery in the browser, allowing CSS-based selectors to find HTML elements within an <a href="https://docs.python.org/2/library/xml.etree.elementtree.html"><code>xmltree</code></a>.</p> <p>You could use <a href="http://sowingseasons.com/blog/introduction-to-whoosh.html">Whoosh</a> to create a search index of the data, or another kind of database binding like <a href="http://mongoengine.org/">MongoEngine</a> for document storage or <a href="http://mongoengine.org/">SQLAlchemy</a> as relational information.</p> <p>The point is there's all this data that's coming in, and it has to go somewhere. If you want to look at the data later a search index or database might be advantageous than storing it back in plain-text like the sources it came from.</p> <p><a name="theFlow"/></p> <h2>Defining a Flow</h2> <p>The flow is how you want your application to behave once it's running. Having a clear idea of loops and additional requests that may or may not need to be made is critical because you don't know where the mining might go (sometimes). If you're following links it might be a good idea to set a maximum depth before returning, or if there's a list of a lot of pages it would be wise to save incrementally in case of a crash.</p> <p>Understanding what could go wrong, how long the process will take, and how the data gets from their pages to your crawler will help define a better flow. The work spent in the design process will make up for itself if something goes wrong and you don't have to start the crawl over from scratch.</p> <p>Below is just an example diagram of what a Ghost.py / Selenium program's flow might be.</p> <p><img alt="Example program flow of a data mining application with Ghost.py" src="/mediafiles/image/ghost_py_flow-left.png"/></p> <p>Utilizing requests helps cut back on resources by omitting DOM, CSS, JavaScript and Webkit backed objects.</p> <p><a name="ourEnvironment"/></p> <h2>Our Environment</h2> <p><a name="envAnalysis"/></p> <h3>Analysis</h3> <p>The initial page that contains our data is <a href="https://uvaps.uvu.edu/prod/SZPTRANSARTIC.SZTransHist">here</a>. It's a Utah Valley University (<a href="http://www.uvu.edu/">UVU</a>) hosted page with options in a <code>select</code> field for different colleges and universities they accept credit from.</p> <p>Selecting any given institution will load a separate page, but in the same tab. Each page consists of some header HTML, boiler plate transfer credit notification and a table that shows which classes at that school transfer to UVU. This is what we care about.</p> <p><img alt="Transfer Credits Table" src="/mediafiles/image/transfer_credit_table.jpg"/></p> <p>Data presented in a table format makes it <em>extremely</em> easy to get. Unlike nested <code>div</code>s which may or may not have classes and ids, a table will at least give structure with rows and cells. You can always traverse the <code>tr</code> tags and map the <code>td</code>'s within to an underlying attribute.</p> <p>After looking at the page source, we can see the generated table is dirty yet parsable.</p> <pre><code class="html">&lt;/TR&gt;
    &lt;TD CLASS="dbdefault"&gt;&lt;p class="leftaligntext"&gt;&lt;B&gt;AMC&lt;/B&gt;&lt;/p&gt;&lt;/TD&gt;
    &lt;TD CLASS="dbdefault"&gt;&lt;p class="centeraligntext"&gt;&amp;nbsp;&lt;/p&gt;&lt;/TD&gt;
    &lt;TD CLASS="dbdefault"&gt;&lt;p class="leftaligntext"&gt;&lt;B&gt;232&lt;/B&gt;&lt;/p&gt;&lt;/TD&gt;
    &lt;TD CLASS="dbdefault"&gt;&lt;p class="centeraligntext"&gt;&amp;nbsp;&lt;/p&gt;&lt;/TD&gt;
    &lt;TD CLASS="dbdefault"&gt;&lt;p class="leftaligntext"&gt;&lt;B&gt;Marketing Concepts&lt;/B&gt;&lt;/p&gt;&lt;/TD&gt;
    &lt;TD CLASS="dblabel" scope="row" &gt;&lt;p class="centeraligntext"&gt;&amp;nbsp;&lt;/p&gt;&lt;/TD&gt;
    &lt;TD CLASS="dblabel" scope="row" &gt;&lt;p class="centeraligntext"&gt;&amp;nbsp;&lt;/p&gt;&lt;/TD&gt;
    &lt;TD CLASS="dbdefault"&gt;&lt;p class="leftaligntext"&gt;&lt;B&gt;MGMT&lt;/B&gt;&lt;/p&gt;&lt;/TD&gt;
    &lt;TD CLASS="dbdefault"&gt;&lt;p class="centeraligntext"&gt;&amp;nbsp;&lt;/p&gt;&lt;/TD&gt;
    &lt;TD CLASS="dbdefault"&gt;&lt;p class="leftaligntext"&gt;&lt;B&gt;1900&lt;/B&gt;&lt;/p&gt;&lt;/TD&gt;
    &lt;TD CLASS="dbdefault"&gt;&lt;p class="centeraligntext"&gt;&amp;nbsp;&lt;/p&gt;&lt;/TD&gt;
    &lt;TD CLASS="dbdefault"&gt;&lt;p class="leftaligntext"&gt;&lt;B&gt;Business Elective&lt;/B&gt;&lt;/p&gt;&lt;/TD&gt;
    &lt;TD CLASS="dbdefault"&gt;&lt;p class="centeraligntext"&gt;&amp;nbsp;&lt;/p&gt;&lt;/TD&gt;
    &lt;TD CLASS="dbdefault"&gt;&lt;p class="leftaligntext"&gt;&amp;nbsp;&lt;/p&gt;&lt;/TD&gt;
&lt;/TR&gt;
</code></pre> <p>The first school I chose was easy, it had something like ten classes that transfer. With a minimum of 0 transfer classes, a small number around 10, I wanted to know what a big table would look like. So, selecting another public in-state school, <a href="http://www.usu.edu/">Utah State University</a>, I was able to get a pretty clear idea. This school...</p> <ul> <li>Took <code>1.7 minutes</code> to load.</li> <li>Has a page size of <code>8.92 MiB</code>.</li> <li>Has <code>141,537 lines</code> in the HTML source.</li> <li>Has <code>9,254 courses</code> listed in the transfer table.</li> <li>...and the worse thing ever, it's not cached.</li> </ul> <p>What a nightmare.</p> <p>Going back to the original page and opening the source, I noticed that each <code>option.value</code> was an ID code. Each one was either <code>\d{6}</code> or <code>U\d{5}</code>. Interesting. There wasn't any (obvious) correlation between the ID and the institution name, but luckily they were all unique. This is an excellent indicator for data mining as it's promising to map a source (the ID + school name) to a destination (the course list).</p> <p><a name="envPython"/></p> <h3>Python Scripts</h3> <h4><strong>Grabbing the dependencies.</strong></h4> <pre><code class="bash">pip install requests pyquery
</code></pre> <h4><strong>Testing the dependencies.</strong></h4> <p>Create a new Python file called <code>miner.py</code> and test check the imports. Pick a URL you're familiar with and ensure you can do a GET with expected/predictable results.</p> <pre><code class="python">import re
import requests

from pyquery import PyQuery as pq

PRIMARY_URL = 'http://sowingseasons.com'

resp = requests.get(PRIMARY_URL)

assert resp.status_code == 200 and 'blog' in resp.content
</code></pre> <p>Testing PyQuery</p> <pre><code class="python">
d = pq(resp.content)

colleges = d('option')

&gt;&gt;&gt; pq(colleges[0]).attr.value, colleges[0].text
U00059 ACT College

&gt;&gt;&gt; pq(colleges[0]).attr.value, colleges[0].text
U00059 ACT College
</code></pre> <p>Sweet! :)</p> <h4><strong>Creating our miner.</strong></h4> <p>Collecting the colleges.</p> <pre><code class="python">colleges = []
for college in d('option'): # colleges in &lt;select&gt;
    colleges.append(        # list of all the colleges
        # a tuple of COLLEGE-ID, NAME
        (pq(college).attr.value, college.text.strip(),))
</code></pre> <p>"Clicking" the submit button for a given school is done with a <code>requests.post</code>. There are two things you need to look for to use this pattern. The first is in the HTML, determine if there is a <code>&lt;form&gt;</code> and where the <code>action</code> attribute points to. This will be the URL that accepts the data sent from the form; which in our case is just the drop down list. The second is the POST payload. You can find this using a tool like <code>fiddler</code> (development proxy) or if you're using Chrome in their DevTools. Hitting the <strong>Network</strong> tab before hitting submit will record incoming and outgoing HTTP requests on that page. After clicking submit should yield a row with a POST Method. If you single click the POST item extra information pops up on the right, and we're interested in the <strong>Headers</strong>. (see below) At the very bottom is <strong>Form Data</strong>. These are the parameters we need to send as a payload using <code>requests</code> to get the responses we want.</p> <p><img alt="Form Data from Chrome's DevTools" src="/mediafiles/image/request_headers.png"/></p> <p>In our use case there's a single field called <code>sbgi</code> and it appears to contain the institution ID that was mapped to its name in the drop down field. Perfect.</p> <h4><strong>Test the HTTP post.</strong></h4> <pre><code class="python">resp = requests.post(
    PRIMARY_URL,
    data = {
        # colleges[0]    =&gt; ('U00059', 'ACT College')
        # colleges[0][0] =&gt; 'U00059'
        'sbgi': colleges[0][0]
    }
)

# make sure the page response was successful
assert resp.status_code == 200

# if it worked this should contain 'ACT College'
# pq(resp.content) is loading the raw HTML into the pyQuery analyzer
# then, ('h2') is asking for the first &lt;h2&gt; tag on the page, and
# subsequently printing out its text-node value.
d = pq(resp.content)

print d('h2').text()
</code></pre> <p><strong>Output:</strong></p> <pre><code class="bash">Transfer Articulation  For: ACT College
[Finished in 4.2s]
</code></pre> <p>Woohoo! It worked. Now we just need to grab the information from the table. I'm switching the college from <code>U00059 ACT College</code> to <code>000718 Barry University</code> for a small transfer table that has more than a single row of classes that match.</p> <p>I just did a simple filter over the colleges and grabbed the first one with <code>barry university</code> in the second column.</p> <p><strong>Example:</strong></p> <pre><code class="python">barry = filter(lambda x: x[1].lower() == 'barry university', colleges)[0]
</code></pre> <h4><strong>Get what's needed from the page.</strong></h4> <p>Unfortunately their page is bad, and they should feel bad. They didn't close their tags! Because the HTML is poorly structured you have to wiggle what you want from the page in a "decent" format. Querying for <code>&lt;tr&gt;</code> tags on the resulting page says there's one, doing a Find on the source says there's five. However, all the information we need is in <code>&lt;td&gt;</code> tags, even if they all have the same class names.</p> <p>Querying the table for <code>&lt;td&gt;</code> and splitting the cells into equal size chunks cuts down a lot of the cruft from the page.</p> <pre><code class="python"># Split a python list into equal sized chunks
# source: 
#    http://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks-in-python
#
def chunks(iterable, size):
    for x in xrange(0, len(iterable), size):
        yield iterable[x:x+size]

# query for &lt;table class="bordertable"&gt;
# query for &lt;td class="dbdefault"&gt; under the &lt;table&gt;

cells = d('table.bordertable &gt; td.dbdefault')

# 1. map each of the cells into a pyquery container for parsering
# 2. filter over the containers for those that contain text
# 3. split the resulting text-based cells into rows of 6 cells each
# 4. get the source and destination school attributes from the row
# 5. print them out.

for c in chunks(filter(lambda x: x.text(), map(pq, cells)), 6):
    src_sub, src_crs, src_title, des_sub, des_crs, des_title = [x.text() for x in c]

    print src_sub, src_crs, des_sub, des_crs
</code></pre> <p><strong>Output:</strong></p> <pre><code class="bash">ART 309 GF 3900
ENG 103 ENGL 1900
ENG 111 ENGL 1010
ENG 112 ENGL 2010
HIS 102 HIST 151G
MAT 107 MATH 1900
MUS 109 MUSC 1110
MUS 123 MUSC 156R
MUS 202 MUSC 1900
ORI 100 EL 1900
PHI 154 HH 1900
PHO 203 ART 1050
PSY 382 SS 1900
SES 210 EL 1900
SES 220 EXSC 2500
SOC 201 SOC 1010
SPE 101 COMM 1050
THE 214 HH 1900

</code></pre> <h4><strong>Put it in a function.</strong></h4> <p>Defining a function that takes a <code>Response.content</code> string with the class we're looking for would be something like this. (below) The method I chose is really simple, but there's lots of room for more advanced functionality for searching and cross referencing which courses combined from different schools could become one at UVU. All I want to do is find <code>statistics</code> classes that transfer to my school.</p> <pre><code class="python">def class_on_page(page, the_class):
    html = pq(page)
    uni = html('h2').text().split(':')[-1].strip()
    cells = html('table.bordertable &gt; td.dbdefault')
    for c in chunks(filter(lambda x: x.text(), map(pq, cells)), 6):
        table_row = [x.text().lower() for x in c]
        if the_class.lower() in ' '.join(table_row):
            print uni, table_row
</code></pre> <p><strong>Example:</strong></p> <pre><code class="python">&gt;&gt;&gt; class_on_page(resp.content, 'statistics')
Alamo Colleges ['elementary statisctical method', 'math', '2040', 'principles of statistics ql', 'math', '2412']
Alamo Colleges ['statistical methods psychology', 'psy', '1950', 'ld psy 3010 statistics', 'psyc', '2319']
</code></pre> <p>Perfect!</p> <h4><strong>Define the "Flow."</strong></h4> <p>Like the example above we need to define our flow. Based on the analysis we know that:</p> <ol> <li>Different institutions can be grouped by keywords in their name, or by their ID.</li> <li>Each results page is generated from a single POST request, from values in the "Select Institution" drop down. </li> <li>There are no included JavaScript libraries on these pages. It's probably CGI and can be mined without DOM rendering.</li> <li>The pages can be very small and fast, or very big and super slow. We need to adjust for that.</li> <li>We don't need all the data, just the classes we're looking to transfer.</li> <li>We don't need to save the data for later (in a database or document store).</li> </ol> <p>Roughly translated to pseudo code it would look like this,</p> <pre><code class="bash">load the course home page
grab all the colleges from the select drop down

for each college in colleges:
    load the college page with a POST
    analyze the response content

    if the class we're looking for is in the content:
        log the information that implies this

we're done! ~ analyze the "logs" somehow

</code></pre> <p><a name="testRun"/></p> <h2>Test Run</h2> <p>With the above logic I rearranged the code we've made so far into a simple loop. I tested ten schools from the middle of the list, ASU through Alberta College of Art.</p> <p><strong>Output:</strong></p> <pre><code>Loading.. Alabama State University
Loading.. Alamance Community College
Loading.. Alameda College
Loading.. Alamo Colleges
Alamo Colleges ['elementary statisctical method', 'math', '2040', 'principles of statistics ql', 'math', '2412']
Alamo Colleges ['statistical methods psychology', 'psy', '1950', 'ld psy 3010 statistics', 'psyc', '2319']
Loading.. Alamo Colleges - NE Lakeview
Loading.. Alaska Career College
Loading.. Alaska Pacific University
Loading.. Albany Coll Of Pharmacy &amp; Hlth
Albany Coll Of Pharmacy &amp; Hlth ['elementary statistics', 'math', '2040', 'principles of statistics ql']
Loading.. Albany Technical College
Loading.. Alberta College Of Art-Canada

</code></pre> <p>It works! But, the results took a lot longer than I would have liked. 10 schools took over a minute...and we have 3500+ to check! Luckily this is an IO bound problem which enables us to utilize Python's threads.</p> <p><strong>Example:</strong></p> <pre><code class="python">import Queue
from threading import Thread

q = Queue.Queue()

def worker():
    while True:
        college = q.get()
        print 'Loading..', college[1]
        resp = requests.post(PRIMARY_URL, data = {'sbgi': college[0]})
        class_on_page(resp.content, 'statistics')
        q.task_done()

for x in xrange(30): # number of threads
    t = Thread(target=worker)
    t.daemon = True
    t.start()

for college in colleges[150:160]:
    q.put(college)

q.join()
</code></pre> <p><strong>Output:</strong></p> <pre><code class="bash">
Loading.. Alabama State University
Loading.. Alamance Community College
Loading.. Alameda College
Loading.. Alamo Colleges
Loading.. Alamo Colleges - NE Lakeview
Loading.. Alaska Career College
Loading.. Alaska Pacific University
Loading.. Albany Coll Of Pharmacy &amp; Hlth
Loading.. Albany Technical College
Loading.. Alberta College Of Art-Canada
Albany Coll Of Pharmacy &amp; Hlth ['elementary statistics', 'math', '2040', 'principles of statistics ql']
Alamo Colleges ['elementary statisctical method', 'math', '2040', 'principles of statistics ql', 'math', '2412']
Alamo Colleges ['statistical methods psychology', 'psy', '1950', 'ld psy 3010 statistics', 'psyc', '2319']

</code></pre> <p>There's a lot of potential for maximizing performance with threads and requests, but we'll cover that another day. I really just wanted to try and get the information a little faster than one at a time. <a name="finalRun"/></p> <h2>Final Run</h2> <p>On my Raspberry Pi 2 with <strong>10 threads</strong> the first <strong>100 institutions</strong> took <strong>45.1 seconds</strong>. Raising the count to <strong>50 threads</strong> crashed the Python kernel, which wasn't very reassuring but not too surprising either. Running outside an iPython Notebook might have helped, but I was thirsty for some results.</p> <p>I re-loaded the script to my 4.1 Ghz i7 with <strong>75 threads</strong> for another test of <strong>100 schools</strong>, and it took just under <strong>8 seconds</strong>. These were the results I wanted, and they seemed super promising. Without hesitation I increased the thread count to <strong>150</strong> and was promptly shut down by my ISP with a 5 minute timeout. :)</p> <p>I didn't follow my own warning with item four on the "Is Mining Worth It?" list and tried to get too much data too fast. There were other indicators this wasn't a good idea when we were looking at the POST headers and noticed the pages weren't being cached when we requested the same one a couple times. Don't get carried away like I did with getting information too quickly or you might not get it at all. Whether it takes 5 minutes or 30 is insignificant if it would have taken hours upon hours manually before.</p> <p>After what seemed like an eternity I introduced a <code>time.sleep</code> with a random value. This prevents all the threads firing at once during the initial queue pops for all the threads we initialized. It's important to remember the <code>time.sleep</code> function pauses the thread, not the process. By adding the call inside the <code>worker</code> function we can stagger introduce our requests to the remote server.</p> <pre><code class="python">time.sleep(random.randint(1, 10) / float(random.randint(1, 3)))
</code></pre> <p>With the sleepy fix and <strong>80 threads</strong> it took <strong>2 minutes 43 seconds</strong> to pull the information of 1000 schools. With our data set (3507 requests) that would put our total mining time at just under 10 minutes. This example is highly dependent on UVU's server, I believe the implementation and management of this "service" is very poor.</p> <p><a name="conclusion"/></p> <h2>Conclusion</h2> <p>In practice I think it goes to show how super easy it is to mine static resources on the web with the <code>requests</code> library. If your information can be derived from simple GET and POST requests without any JavaScript interaction it's really the way to go. Tools like pyQuery that allow for CSS-like selectors of the "DOM" are also very helpful and can easily minimize potential regular expression parsing of the data.</p> <p>These two libraries are intuitive to use and easy to pick up. Their results speak for themselves which is why it's my go-to combination when trying to scrape basic information from the web.</p> </div> </div></body></html>