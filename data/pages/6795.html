<html><body><div><div class="post-text" itemprop="text">

<p>Based on <a href="http://pybrain.org/docs/index.html#tutorials" rel="nofollow">PyBrain's tutorials</a> I managed to knock together the following code:</p>

<pre><code>#!/usr/bin/env python2
# coding: utf-8

from pybrain.structure import FeedForwardNetwork, LinearLayer, SigmoidLayer, FullConnection
from pybrain.datasets import SupervisedDataSet
from pybrain.supervised.trainers import BackpropTrainer

n = FeedForwardNetwork()

inLayer = LinearLayer(2)
hiddenLayer = SigmoidLayer(3)
outLayer = LinearLayer(1)

n.addInputModule(inLayer)
n.addModule(hiddenLayer)
n.addOutputModule(outLayer)

in_to_hidden = FullConnection(inLayer, hiddenLayer)
hidden_to_out = FullConnection(hiddenLayer, outLayer)

n.addConnection(in_to_hidden)
n.addConnection(hidden_to_out)

n.sortModules()

ds = SupervisedDataSet(2, 1)
ds.addSample((0, 0), (0,))
ds.addSample((0, 1), (1,))
ds.addSample((1, 0), (1,))
ds.addSample((1, 1), (0,))

trainer = BackpropTrainer(n, ds)
# trainer.train()
trainer.trainUntilConvergence()

print n.activate([0, 0])[0]
print n.activate([0, 1])[0]
print n.activate([1, 0])[0]
print n.activate([1, 1])[0]
</code></pre>

<p>It's supposed to learn XOR function, but the results seem quite random:</p>

<blockquote>
  <p>0.208884929522</p>
  
  <p>0.168926515771</p>
  
  <p>0.459452834043</p>
  
  <p>0.424209192223</p>
</blockquote>

<p>or</p>

<blockquote>
  <p>0.84956138664</p>
  
  <p>0.888512762786</p>
  
  <p>0.564964077401</p>
  
  <p>0.611111147862</p>
</blockquote>
    </div>
    </div></body></html>