<html><body><div><div class="post-body entry-content" id="post-body-35552852328990016" itemprop="description articleBody"><p>

In my last </p><a href="http://byterot.blogspot.co.uk/2015/06/five-crazy-abstractions-my-deep-learning-word2doc-model-just-did-NLP-gensim.html" target="_blank">blog post</a><p>, I outlined a few interesting results from a word2wec model trained on half a million news documents. This was pleasantly met with some positive reactions, some of which not necessarily due to the scientific rigour of the report but due to awareness effect of such "populist treatment of the subject" on the community. On the other hand, there were more than some negative reactions. Some believing I was "cherry-picking" and reporting only a handful of interesting results out of an ocean of mediocre performances. Others rejecting my claim that training on a small dataset in any language can produce very encouraging results. And yet others literally threatening me so that I would release the code despite I reiterating the code is small and not the point.

</p><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container"><tbody>
<tr><td><a href="http://img-new.cgtrader.com/items/66020/large_am_i_the_only_one_around_here_lithophane_3d_model_stl_191c19e9-4d83-4326-9fdb-969e8ff3483e.jpg" imageanchor="1"><img border="0" src="http://img-new.cgtrader.com/items/66020/large_am_i_the_only_one_around_here_lithophane_3d_model_stl_191c19e9-4d83-4326-9fdb-969e8ff3483e.jpg"/></a></td></tr>
<tr><td class="tr-caption">Am I the only one here thinking word2vec is freaking awesome?!</td></tr>
</tbody></table>
<p>
So I am back. And this time I have trained the model on a </p><em>very small</em><p> corpus of Rock artists obtained from Wikipedia, as part of my </p><strong>Rock History project</strong><p>. And I have built an API on top of the model so that you could play with the model and try out different combinations to your heart's content - [but please be easy on the API it is a small instance only] :) </p><strong>strictly no bots</strong><p>. And that's not all: I am releasing the code and the dataset (which is only 36K Wiki entries).
</p><p>
But now, my turn to RANT for a few paragraphs.
</p><p>
First of all, quantification of the performance of an unsupervised learning algo in a highly subjective field is very hard, time-consuming and potentially non-repeatable. Google in their </p><a href="http://arxiv.org/pdf/1506.05869.pdf" target="_blank">latest paper</a><p> on seq2seq had to resort to reporting mainly man-machine conversations. I feel in these subjects crowdsourcing the quantification is probably the best approach. Hence you would help by giving a rough accuracy score according to your experience.

</p>
<p>
On the other hand, sorry, those who were expecting to see a formal paper - perhaps in laTex format - you completely missed the point. As others said, there are plenty of hardcode papers out there, feel free to knock yourselves down. My point was to evangelise to a much wider audience. And, if you liked what you saw, go and try it for yourself.
</p><p>
Finally, alluding to "cognition" turned a lot of eyebrows but as Nando de Freitas puts it when asked about intelligence, whenever we build an intelligent machine, we will look at it as bogus not containing the "real intelligence" and we will discard it as not AI. So the world of Artifical Intelligence is a world of moving targets essentially because intelligence has been very difficult to define.
</p><p>
For me, word2vec is a breath of fresh air in a world of arbitrary, highly engineered and complex NLP algorithms which can breach the gap forming a meaningful relationship between tokens of your corpus. And I feel it is more a tool enhancing other algorithms rather than the end product. But even on its own, it generates fascinating results. For example in this tiny corpus, it was not only able to find the match between the name of the artists, but it can successfully find matches between similar bands - able to be used it as a </p><strong>Recommender system</strong><p>. And then, even adding the vector of artists generates interesting fusion genres which tend to correspond to real bands influenced by them.
</p><h2 id="api">
API</h2>
<i>BEWARE: Tokens are <b>case-sensitive</b>. So u2 and U2 not the same.</i>
<p>
The API is basically a simple RESTful flask on top of the model:
</p><pre><code>http://localhost:5000/api/v1/rock/similar?pos=&lt;pos&gt;&amp;neg=&lt;neg&gt;
</code></pre><p>
where </p><code>pos</code><p> and </p><code>neg</code><p> are comma separated list of zero to many 'phrases' (</p><code>pos</code><p> for similar, and </p><code>neg</code><p> for opposite) - that are English words, or multi-word tokens including name of the bands or phrases that have a Wiki entry (such as albums or songs) - list if which can be found here </p><a href="https://www.blogger.com/!!"/><p>.</p><p>
For example:
</p><pre><code>http://localhost:5000</code>/api/v1/rock/similar?pos=Captain%20Beefheart</pre>


<p>
You can add vectors of words, for example to mix genres:
</p><pre><code>http://localhost:5000</code>/api/v1/rock/similar?pos=Daft%20Punk,Tool&amp;min_freq=50</pre><p>
or add an artist with an adjective for example a softer Bob Dylan:
</p><pre><code>http://localhost:5000/api/v1/rock/similar?pos=Bob%20Dylan,soft&amp;min_freq=50
</code></pre><p>
Or subtract:
</p><pre><code>http://localhost:5000</code>/api/v1/rock/similar?pos=Bob%20Dylan&amp;neg=U2</pre><p>
But the tokens do not have to be a band name or artist names:
</p><pre><code>http://localhost:5000</code>/api/v1/rock/similar?pos=drug</pre><p>
If you pass a non-existent or misspelling (it is case-sensitive!) of a name or word, you will get an error:
</p><pre><code>http://</code>localhost:5000/api/v1/rock/similar?pos=radiohead</pre>
<pre><code>
{
  result: "Not in vocab: radiohead"
}
</code></pre>
<pre><code>
</code></pre><p>
You may pass minimum frequency of the word in the corpus to filter the output to remove the noice:
</p><pre><code>http://localhost:5000/api/v1/rock/similar?pos=Daft%20Punk,Tool&amp;min_freq=50
</code></pre>
<h2 id="code">
Code</h2><p>
The code on </p><a href="https://github.com/aliostad/WikiRockWord2Vec" target="_blank">github</a><p>Â as I said is tiny. Perhaps the most complex part of the code is the Dictionary Tokenisation which is one of the tools I have built to tokenise the text without breaking multi-word phrases and I have found it very useful allowing to produce much more meaningful results.
</p><p>
The code is shared under </p><a href="http://opensource.org/licenses/MIT" target="_blank">MIT license</a><p>.
</p><p>
To build the model, uncomment the line in wiki_rock_train.py, specifying the location of corpus:

</p><pre><code>train_and_save('data/wiki_rock_multiword_dic.txt', 'data/stop-words-english1.txt', '&lt;THE_LOCATION&gt;/wiki_rock_corpus/*.txt')

</code></pre>
<h2 id="dataset">
Dataset</h2><p>
As mentioned earlier, dataset/corpus is the text from 36K Rock music artist entries on the Wikipedia. This list was obtained by scraping the links from the "</p><a href="https://en.wikipedia.org/wiki/List_of_rock_genres" target="_blank">List of rock genres</a><p>". Dataset can be downloaded from </p><a href="https://drive.google.com/file/d/0By4PF7Jis9FzTTFpS1VVVzB4NFk/view?usp=sharing" target="_blank">here</a><p>. For information on the Copyright of the Wikipedia text and its terms of use please see </p><a href="https://en.wikipedia.org/wiki/Wikipedia:Copyrights" target="_blank">here</a><p>.
</p><p/>
</div>
</div></body></html>