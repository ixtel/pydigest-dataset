<html><body><div><div class="entry-content"><p>In my <a href="/blog/2014/10/31/install-apache-spark-on-ubuntu-14-dot-04/">previous post</a>, I wrote about installation of Spark and Scala interactive shell. Here in this post we’ll see how to do the same in Python.</p>

<p>Similar to Scala iteractive shell, there is an interactive shell available for Python. You can run it with the below command from spark root folder:</p>

<figure class="code"/>


<p>
Now you can enjoy Spark using Python interactive shell.</p>

<p>This shell might be sufficient for experimentations and developments. However, for production level, we should use a stand alone application.  I talked about a stand alone Spark application in Scala in one of my previous <a href="/blog/2014/04/01/a-standalone-spark-application-in-scala/">post</a>. Here comes the same written in Python – you can find more about it in <a href="https://spark.apache.org/docs/latest/quick-start.html#self-contained-applications">Spark official site</a> – and known as a self contained PySpark application.</p>

<p>First, <a href="/blog/2014/10/31/install-apache-spark-on-ubuntu-14-dot-04/">refer this post</a> to build Spark using sbt assembly. Add Pyspark lib in system Python path as follows:</p>

<figure class="code"/>


<p>Add the following exports in end of bashrc file</p>

<figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">export SPARK_HOME=&lt;path to Spark home&gt;
</span><span class="line">export PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH</span></code></pre></td></tr></table></div></figure>


<p>Don’t forget to export the SPARK_HOME. Restart <code>BASH</code> once it is done.</p>

<figure class="code"/>


<p>PySpark depends on the <code>py4j</code> Python package. It helps Python interpreter to dynamically access the Spark object from the JVM.Install py4j python package using following comand:</p>

<figure class="code"/>


<p>PySpark should be avaible in system path by now. After writing the Python code, one can simply run the code using <code>python</code> command then it runs in local Spark instance with default configurations. For Spark applications, it is better to use the spark submit script.</p>

<figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">./bin/spark-submit --master local[8] &lt;python_file.py&gt;</span></code></pre></td></tr></table></div></figure>


<p>
For more details about spark submit <a href="https://spark.apache.org/docs/latest/configuration.html">refer here</a>. From the site we can observe that the configuration values can be passed at run time. It can also be changed in the conf/spark-defaults.conf file. After configuring the spark config file the changes also get reflected while running pyspark applications using simple ‘python’ command.</p>

<p>The reason for why there is no <code>pip install</code> for pyspark can be found in this <a href="https://issues.apache.org/jira/browse/SPARK-1267">jira ticket</a>.</p>

<p>If you are a fan of ipython, then you have the option to run PySpark ipython notebook. Refer this <a href="http://blog.cloudera.com/blog/2014/08/how-to-use-ipython-notebook-with-apache-spark/">blog post</a> for more detail.</p>
</div>


  </div></body></html>