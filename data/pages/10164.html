<html><body><div><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-model" class="anchor" href="#model" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Model</h1>

<p>This is an implementation of the deep residual network used for cifar10 as described in <a href="http://arxiv.org/abs/1512.03385">He et. al.,
"Deep Residual Learning for Image Recognition"</a>.  The model is
structured as a very deep network with skip connections designed to have convolutional parameters
adjusting to residual activations.  The training protocol uses minimal pre-processing (mean
subtraction) and very simple data augmentation (shuffling, flipping, and cropping).  All model
parameters (even batch norm parameters) are updated using simple stochastic gradient descent with
weight decay.  The learning rate is dropped only twice (at 90 and 135 epochs).</p>

<h3><a id="user-content-acknowledgments" class="anchor" href="#acknowledgments" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Acknowledgments</h3>

<p>Many thanks to Dr. He and his team at MSRA for their helpful input in replicating the model as
described in their paper.</p>

<h3><a id="user-content-model-script" class="anchor" href="#model-script" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Model script</h3>

<p>The model train script is included (<a href="/apark263/cfmz/blob/master/cifar10_msra.py">cifar10_msra.py</a>).</p>

<h3><a id="user-content-trained-weights" class="anchor" href="#trained-weights" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Trained weights</h3>

<p>The trained weights file can be downloaded from AWS</p>



<h3><a id="user-content-performance" class="anchor" href="#performance" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Performance</h3>

<p>Training this model with the options described below should be able to achieve above 93.6% top-1
accuracy using only mean subtraction, random cropping, and random flips.</p>

<h2><a id="user-content-instructions" class="anchor" href="#instructions" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Instructions</h2>

<p>This script was tested with <a href="https://github.com/NervanaSystems/neon/tree/v1.2.1">neon version 1.2.1</a>.
Make sure that your local repo is synced to this commit and run the <a href="http://neon.nervanasys.com/docs/latest/user_guide.html#installation">installation
procedure</a> before proceeding.
Commit SHA for v1.2.1 is  <code>c460e6c12cc4ea6e7453c0335afadf1f5110a4f7</code></p>

<p>In addition, we use the branch that implements the merge sum layer type.</p>

<p>This example uses the <code>ImageLoader</code> module to load the images for consumption while applying random
cropping, flipping, and shuffling.  Prior to beginning training, you need to write out the padded
cifar10 images into a macrobatch repository.  From your top-level neon direcotry, run:</p>

<pre><code>neon/data/batch_writer.py \
    --set_type cifar10 \
    --data_dir &lt;path-to-save-batches&gt; \
    --macro_size 10000 \
    --target_size 40
</code></pre>

<p>Note that it is good practice to choose your <code>data_dir</code> to be local to your machine in order to
avoid having <code>ImageLoader</code> module perform reads over the network.</p>

<p>Once the batches have been written out, you may initiate training:</p>

<pre><code>cifar10_msra.py -r 0 -vv \
    --log &lt;logfile&gt; \
    --epochs 180 \
    --save_path &lt;model-save-path&gt; \
    --eval_freq 1 \
    --backend gpu \
    --data_dir &lt;path-to-saved-batches&gt; \
    --depth &lt;n&gt;
</code></pre>

<p>The depth argument is the <code>n</code> value discussed in the paper which represents the number of repeated
residual models at each filter depth.  Since there are 3 stages at each filter depth, and each
residual module consists of 2 convolutional layers, there will be <code>6n</code> total convolutional layers
in the residual part of the network, plus 2 additional layers (input convolutional, and output
linear), making the total network <code>6n+2</code> layers deep.  For depth arguments of 3, 5, 9, 18, we get
network depths of 20, 32, 56, and 110.</p>

<p>If you just want to run evaluation, you can use the much simpler script that loads the serialized
model and evaluates it on the validation set:</p>

<pre><code>cifar10_eval.py -vv --model_file &lt;model-save-path&gt;
</code></pre>

<h2><a id="user-content-benchmarks" class="anchor" href="#benchmarks" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Benchmarks</h2>

<p>Machine and GPU specs:</p>

<pre><code>Intel(R) Core(TM) i7-4790 CPU @ 3.60GHz
Ubuntu 14.04.2 LTS
GPU: GeForce GTX TITAN X
CUDA Driver Version 7.0
</code></pre>

<p>The memory usage and per-epoch training time of each network configuration, along with final
validation error is shown in the table below.  We observed that the error rates were consistently
lower than what was cited in the original paper.  Our hypothesis is that this may be due to our
inclusion of a final batch norm transformation at the output affine layer.</p>

<table><thead>
<tr>
<th>Model Depth</th>
<th>GPU Memory Footprint</th>
<th>Seconds per Epoch</th>
<th>Validation Error %</th>
</tr>
</thead><tbody>
<tr>
<td>20</td>
<td>521 MiB</td>
<td>11</td>
<td>8.29</td>
</tr>
<tr>
<td>32</td>
<td>636 MiB</td>
<td>18</td>
<td>7.26</td>
</tr>
<tr>
<td>56</td>
<td>860 MiB</td>
<td>30</td>
<td>6.31</td>
</tr>
<tr>
<td>110</td>
<td>1277 MiB</td>
<td>60</td>
<td>6.00</td>
</tr>
</tbody></table>

<p>The total amount of time to train the 56 layer network for 180 epochs was about 90 minutes with the
described machine and GPU specifications.</p>

<p>The evolution of validation misclassification error for the various layer depths can be seen in the
figures below.</p>

<p><a href="/apark263/cfmz/blob/master/val_error.png" target="_blank"><img src="/apark263/cfmz/raw/master/val_error.png" alt="validation error"/></a></p>

<p><a href="/apark263/cfmz/blob/master/val_error_zoom.png" target="_blank"><img src="/apark263/cfmz/raw/master/val_error_zoom.png" alt="validation error zoom"/></a></p>
</article>
  </div></body></html>