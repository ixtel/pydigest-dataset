<html><body><div><div class="post-body entry-content" id="post-body-2072781516592907992" itemprop="description articleBody">
<i>Seeing is believing. </i>
<p>
Of course, there is a whole host of Machine Learning techniques available, thanks to the researchers, and to Open Source developers for turning them into libraries. And I am not quite a complete stranger to this field, I have been, on and off, working on Machine Learning over the last 8 years. But, nothing, absolutely nothing for me has ever come close to what blew my mind recently with word2vec: so effortless yet you feel </p><b>like</b><p> the model knows so much that it has obtained </p><i>cognitive coherence of the vocabulary</i><p>. Until neuroscientists nail cognition, I am happy to </p><i>foolishly</i><p> take that as some early form of machine cognition.

</p><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container"><tbody>
<tr><td><a href="http://4.bp.blogspot.com/-o6QmaB5arZI/VX1qOexFIjI/AAAAAAAAI9s/R5dfzns6e5o/s1600/Singularity_Dance%2B%25281%2529.png" imageanchor="1"><img border="0" src="http://4.bp.blogspot.com/-o6QmaB5arZI/VX1qOexFIjI/AAAAAAAAI9s/R5dfzns6e5o/s640/Singularity_Dance%2B%25281%2529.png"/></a></td></tr>
<tr><td class="tr-caption"><span>Singularity Dance - Wiki</span></td></tr>
</tbody></table>
<p>
But, no, don't take my word for it! If you have a corpus of 100s of thousand documents (or even 10s of thousands), feed it and see it for yourselves. What language? Doesn't really matter! My money is on that you will get results that equally blow your tops off.

</p><h2>
What is word2vec?</h2><p>
word2vec is a Deep Learning technique first </p><a href="http://arxiv.org/pdf/1301.3781v3.pdf" target="_blank">described</a><p> by Tomas Mikolov only 2 years ago but due to its simplicity of algorithm and yet surprising robustness of the results, it has been widely </p><a href="https://radimrehurek.com/gensim/models/word2vec.html" target="_blank">implemented</a><p> and adopted. This technique basically trains a model based on a neighborhood window of words in a corpus and then projects the result onto [an arbitrary number of] </p><i>n</i><p> dimensions where each word is a vector in the </p><i>n</i><p> dimensional space. Then the words can be compared using the </p><a href="https://en.wikipedia.org/wiki/Cosine_similarity" target="_blank">cosine similarity</a><p> of their vectors. And what is much more interesting is the </p><b>arithmetics</b><p>: vectors can be added or subtracted for example vector of </p><span>Queen</span><p> is almost equal to </p><span>King</span><p> + </p><span>Woman</span><p> - </p><span>Man</span><p>. In other words, if you remove Man from the King and add Woman to it, </p><i>logically</i><p> you get Queen and but this model is able to represent it </p><i>mathematically</i><p>.
</p><p>
LeCun recently </p><a href="http://arxiv.org/abs/1502.01710" target="_blank">proposed</a><p> a variant of this approach in which he uses characters and not words. Altogether this is a fast moving space and likely to bring about significant change in the state of the art in Natural Language Processing.

</p><h2>
Enough of this, show us ze resultz!</h2><p>
OK, sure. For those interested, I have brought the methods after the results.



</p><h3>
1) Human - Animal = Ethics</h3><p>
Yeah, as if it knows! So if you remove the animal traits from human, what remains is Ethics. And in word2vec terms, subtracting the vector of </p><span>Human</span><p> by the vector of </p><span>Animal</span><p> results in a vector which is closest to </p><span>Ethics</span><p> (0.51). The other similar words to the </p><span>Human - Animal</span><p> vector are the words below: </p><span>spirituality</span><p>,  </p><span>knowledge</span><p> and </p><span>piety</span><p>. Interesting, huh?

</p><h3>
2) Stock Market ≈ Thermometer</h3><p>
In my model the word Thermometer has a similarity of 0.72 to the Stock Market vector and the 6th similar word to it - most of closer words were other names for the stock market. It is not 100% clear to me how it was able to make such abstraction but perhaps proximity of Thermometer to the words increase/decrease or up/down, etc could have resulted in the similarity. In any case, likening Stock Market to Thermometer is a higher level abstraction.

</p><h3>
3) Library - Books = Hall</h3><p>
What remains of a library if you were to remove the books? word2vec to the rescue. The similarity is 0.49 and next words are: </p><span>Building</span><p> and </p><span>Dorm</span><p>.  </p><span>Hall</span><span>'s</span><p> vector is already similar to that of </p><span>Library</span><p> (so the subtraction's effect could be incidental) but </p><span>Building</span><p> and </p><span>Dorm</span><p> are not. Now </p><span>Library - Book</span><p> (and not </p><span>Books</span><p>) is closest to </p><span>Dorm</span><p> with 0.51 similarity.

</p><h3>
4) Obama + Russia - USA = Putin</h3><p>
This is a classic case similar to </p><span>King+Woman-Man</span><p> but it was interesting to see that it works. In fact finding leaders of most countries was successful using this method. For example, </p><span>Obama + Britain - USA</span><p> finds </p><span>David Cameron</span><p> (0.71).

</p><h3>
5) Iraq - Violence = Jordan</h3><p>
So a country that is most similar to Iraq after taking its violence is Jordan, its neighbour. Iraq's vector itself is most similar to that of Syria - for obvious reasons. After Jordan, next vectors are </p><span>Lebanon</span><p>, </p><span>Oman</span><p> and </p><span>Turkey</span><p>.
</p><p>
Not enough? Hmm there you go with another two...

</p><h3>
Bonus) President - Power = Prime Minister</h3><p>
Kinda obvious, isn't it? But of course we know it depends which one is Putin which one is Medvedev :)

</p><h3>
Bonus 2) Politics - Lies = Germans??</h3><p>
OK, I admit I don't know what this one really means but according to my model, German politicians do not lie!
</p><p>
<br/></p><p>
Now the </p><i>boring stuff</i><p>...

</p><h2>
Methods</h2><p>
I used a corpus of publicly available online news and articles. Articles extracted from a number of different Farsi online websites and on average they contained ~ 8KB of text. The topics ranged from local and global Politics, Sports, Arts and Culture, Science and Technologies, Humanities and Religion, Health, etc.
</p><p>
The processing pipeline is illustrated below:

</p><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container"><tbody>
<tr><td><a href="http://4.bp.blogspot.com/-OIOAvbgCFUU/VX1omPNG3GI/AAAAAAAAI9g/2Xz8JroEVDc/s1600/word2vec%2Bpipeline%2B-%2Bcrop.png" imageanchor="1"><img border="0" src="http://4.bp.blogspot.com/-OIOAvbgCFUU/VX1omPNG3GI/AAAAAAAAI9g/2Xz8JroEVDc/s640/word2vec%2Bpipeline%2B-%2Bcrop.png"/></a></td></tr>
<tr><td class="tr-caption">Figure 1 - Processing Pipeline</td></tr>
</tbody></table><p>
For word segmentation, an approach was used to join named entities using a dictionary of ~ 40K multi-part words and named entities.
</p><p>
Gensim's </p><a href="https://radimrehurek.com/gensim/models/word2vec.html" target="_blank">word2vec</a><p> implementation was used to train the model. The default </p><i><span>n=100</span></i><p> and </p><i><span>window=5</span></i><p> worked very well but to find the optimum values, another study needs to be conducted.
</p><p>
In order to generate the results presented in this post, </p><span>most_similar</span><p> method was used. No significant difference between using </p><span>most_similar</span><p> and </p><span>most_similar_cosmul</span><p> was found.
</p><p>
A significant problem was discovered where words with spelling mistake in the corpus or infrequent words generate sparse vectors which result in a very high score of similar with some words. I used frequency of the word in the corpus to filter out such occasions.

</p><h2>
Conclusion</h2>
<p>
word2vec is relatively simple algorithm with surprisingly remarkable performance. Its implementation are available in a variety of Open Source libraries, including Python's Gensim. Based on the preliminary results, it appears that word2vec is able to make higher levels abstractions which nudges towards cognitive abilities.</p>
<p>
<br/></p><p>
Despite its remarkable it is not quite clear how this ability can be used in an application, although in its current form, it can be readily used in finding antonym/synonym, spelling correction and stemming.

</p><p/>
</div>
</div></body></html>