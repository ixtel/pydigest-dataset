<html><body><div><div id="content-container">
	    
	    



<h1>The Python Concurrency Story, Part 2</h1>

<p class="originally">Originally published in the <a href="/python-newsletter/">Advanced Python Newsletter</a></p>


<p>In <a href="/python-concurrency-story-pt1/">Part One</a>,
we talked about the key dilemma of programming Python in a concurrent
world. In a nutshell, for CPU-bound tasks, Python threads will not help. So
what do we do instead?</p>

<p>Broadly speaking, there are two kinds of real-world concurrency
problems you will face:</p>

<ul>
    <li>Tasks where having more than one CPU can help it run faster,
    and</li>
    <li>tasks where having extra CPUs won't help much.</li>
</ul>

<p>In this essay, you're going to learn about the first one: tasks
that ARE empowered by having extra CPUs. We do this by writing Python
code to run across several processes. In the best case, having N CPUs
will let you run the program N times faster. </p>

<p>(The second kind, which we will cover in part 3, has to do with
juggling multiple tasks that are not CPU-bound. For something like
this, having multiple CPUs or cores doesn't really help. To win <em>that</em>
game, you have to get the absolute most you can out of a single
thread... which modern Python gives you some great tools for.)</p>

<h2>Two Approaches to Concurrent Programming</h2>

<p>Among Python programmers, one thing separating the good from the
great is an understanding of the concurrency primitives provided by
modern operating systems - processes and threads - and the different
patterns for working with them. This is fundamentally independent of
language. You may know some of it from your formal education, or
your experience implementing concurrent applications in other
languages.</p>

<p>Broadly, there are two main approaches to implementing concurrent
systems: <em>Shared Memory</em>, and <em>Message Passing</em>.  The
key difference: shared memory means your program is designed for two
or more threads to read from and write to the same block of memory -
dancing without stepping on each other's toes. In the message passing
approach, any chunk of memory is only accessed by a single
thread. When two or more need to synchronize, interact or communicate,
data is sent from one to another - i.e., a message is passed.</p>

<p>Now, the two threads can be in the same process, or in two
different processes - each of which has its own main thread. So you can
use shared memory or message passing in either.<sup><a href="#note_1">1</a></sup> However, typically, people will used
shared memory only in single-process, multi-threaded systems. It's
possible to walk the shared memory path via <code>mmap</code>-like
constructs or IPC for a multiprocess program, but is not as
performant, and can be complex to implement.<sup><a href="#note_2">2</a></sup></p>

<p>I recommend you not go down that path, though, if you can avoid
it. Those of you who have done it know why: it's very difficult to do
well, and too easy to create subtle race conditions, deadlocks, and
other bugs. Sometimes you <em>can't</em> avoid sharing memory. But if
you can use message-passing instead, do it.</p>

<p>For multiprocess programs, message passing tends to be a more
natural fit anyway. For two reasons: first, shared memory is not so
performant and elegant in multiprocess, compared to doing so in a
single-process, multithreaded program. More importantly, many of the
kinds of programs you'd want to implement using multiple processes
lend themselves naturally to a message-passing architecture,
especially in Python.</p>

<h2>Writing Multicore Python</h2>

<p>Python essentially forces you to use multiple processes to leverage
more than one CPU. <sup><a href="#note_3">3</a></sup> There are ways
to share memory across processes, and modern Python makes it
relatively easy to do. But we'll focus on the message-passing approach
here.</p>

<p>We really have it easy now. Once upon a time, if you wanted to make
Python use multiple CPUs well, you had to do something horrible, hacky
and unportable using <code>os.fork</code>. But now, we have the nice
<code>multiprocessing</code> module in the standard library. It's a
nice Pythonic interface to working with multiple processes, making
many hard things easy<sup><a href="#note_4">4</a></sup>. There are
plenty of articles out there with nice, neat toy examples. But how
about we look at something more realistic?
</p>

<p>Let me introduce you to <a href="https://github.com/migrateup/thumper">Thumper</a>.  It's a
python program that generates thumbnails of gigantic image
libraries. This is what you want to use if your webapp fetches a new
batch of 100,000 images at 2am each night, and you need
thumbnails of them ready to view in a reasonable amount of time. This
is a perfect use case for multiple CPUs: generating a thumbnail is
CPU-bound, and the calculation is completely independent for two
different images.</p>

<p>Feast your eyes on this beauty:</p>

<img src="/static/python-concurrency-story/thumper-multiprocessing-all.svg"/>

<p>This comes from repeatedly running thumper on a data set of
NASA TIFF images, on an AWS c3.2xlarge instance. The horizontal is the
number of worker processes spawned.</p>

<p>There's a LOT going on in this graph. I'm working on an Advanced
Python Mastery course, which will delve into much of it, granting my
students amazing Batman-like concurrent programming superpowers. For
now, we are going to focus on Thumper's implementation - and how it
bypasses the GIL to scale with the number of available CPU
cores.</p>

<p>The core is quite simple, thanks to Pillow - the modern, maintained
fork of PIL (Python Imaging Library):</p>


<div class="highlight"><pre><ol><li><p class="line"><span class="c"># PIL is actually Pillow. Confusing, I know,</span>
</p></li><li><p class="line"><span class="c"># but nicely backwards compatible.</span>
</p></li><li><p class="line"><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
</p></li><li><p class="line"><span class="k">def</span> <span class="nf">create_thumbnail</span><span class="p">(</span><span class="n">src_path</span><span class="p">,</span> <span class="n">dest_path</span><span class="p">,</span> <span class="n">thumbnail_width</span><span class="p">,</span> <span class="n">thumbnail_height</span><span class="p">):</span>
</p></li><li><p class="line">    <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">src_path</span><span class="p">)</span>
</p></li><li><p class="line">    <span class="n">image</span><span class="o">.</span><span class="n">thumbnail</span><span class="p">((</span><span class="n">thumbnail_width</span><span class="p">,</span> <span class="n">thumbnail_height</span><span class="p">))</span>
</p></li><li><p class="line">    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">dest_path</span><span class="p">),</span> <span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</p></li><li><p class="line">    <span class="n">image</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">dest_path</span><span class="p">)</span>
</p></li></ol></pre></div>



<p>Very straightforward. Now, we want to farm this out to several
worker processes. Here are some considerations:</p>

<ul>
    <li>How many processes? This is a surprisingly hard question, as
    we will see, and it turns out to be a moving target. So we want
    flexibility - the ability to specify how many at runtime.</li>
    <li>How do we keep from creating too many processes, or too
    few?</li>
    <li>How can we effeciently wait until one worker process is done,
    then immediately hand it a new image to thumbnail?</li>
</ul>

<h2>Python's multiprocessing Pools</h2>

<p>The <code>multiprocessing</code> module has a very useful
abstraction, in the form of the <code>Pool</code> class. You create it
like this:</p>


<div class="highlight"><pre><ol><li><p class="line"><span class="kn">import</span> <span class="nn">multiprocessing</span>
</p></li><li><p class="line"><span class="n">pool</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="o">.</span><span class="n">Pool</span><span class="p">(</span><span class="n">num_processes</span><span class="p">)</span>
</p></li></ol></pre></div>



<p>Whew, that was hard! The word "multiprocessing" has way too many
characters... so much typing. Thankfully that's behind us, and what's
left is to dispatch images to each of these
workers. The <code>Pool</code> provides several different
ways to do this. Each takes a callable - which will be
<code>create_thumbnail</code>, in our case - and its
arguments.</p>

<p>Some of the different pool methods are:</p>
<dl>
    <dt>apply_async</dt>
    <dd>Takes a function and some arguments, and sends it to
    <em>one</em> of the worker processes to be run. Replies
    immediately with a result object, which is kind of like a future -
    you can get the returned value from it once it's done.</dd>
    <dt>map and map_async</dt>
    <dd>Similar to Python's built-in <code>map()</code> function, except the work
    is farmed out over the child processes. <code>map</code> will block until all
    jobs are done; <code>map_async</code> immediately returns a result
    object. Limitation: the callable can only take one argument.</dd>
    <dt>imap and imap_async</dt>
    <dd>Like <code>map</code> and <code>map_async</code>, but returns an iterator instead of a
    completed sequence. Could have been called <code>lazy_map</code>.</dd>
    <dt>starmap and starmap_async</dt>
    <dd>Like <code>map</code> and <code>map_async</code>, except its
    callable can take many arguments.</dd>
</dl>

<p>These are most<sup><a href="#note_5">5</a></sup>
of the methods you will use, and as you can see, they are mostly
variants of similar ideas. Since our <code>create_thumbnail</code>
function takes multiple arguments, Thumper uses <code>starmap_async</code>.</p>


<div class="highlight"><pre><ol><li><p class="line"><span class="c"># src_dir is the folder containing full-size images.</span>
</p></li><li><p class="line"><span class="c"># dest_dir is where we are going to write the thumbnails,</span>
</p></li><li><p class="line"><span class="c"># in a parallel file hierarchy.</span>
</p></li><li><p class="line">    
</p></li><li><p class="line"><span class="k">def</span> <span class="nf">gen_child_args</span><span class="p">():</span>
</p></li><li><p class="line">    <span class="k">for</span> <span class="p">(</span><span class="n">dirpath</span><span class="p">,</span> <span class="n">dirnames</span><span class="p">,</span> <span class="n">filenames</span><span class="p">)</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">walk</span><span class="p">(</span><span class="n">src_dir</span><span class="p">):</span>
</p></li><li><p class="line">        <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">filenames</span><span class="p">:</span>
</p></li><li><p class="line">            <span class="n">src_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dirpath</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
</p></li><li><p class="line">            <span class="n">dest_path</span> <span class="o">=</span> <span class="n">find_dest_path</span><span class="p">(</span><span class="n">src_dir</span><span class="p">,</span> <span class="n">dest_dir</span><span class="p">,</span> <span class="n">src_path</span><span class="p">)</span>
</p></li><li><p class="line">            <span class="k">yield</span> <span class="p">(</span><span class="n">src_path</span><span class="p">,</span> <span class="n">dest_path</span><span class="p">,</span> <span class="n">thumbnail_width</span><span class="p">,</span> <span class="n">thumbnail_height</span><span class="p">)</span>
</p></li><li><p class="line"><span class="n">pool</span><span class="o">.</span><span class="n">starmap_async</span><span class="p">(</span><span class="n">create_thumbnail</span><span class="p">,</span> <span class="n">gen_child_args</span><span class="p">())</span>
</p></li></ol></pre></div>



<p>As you can infer, the second argument of
<code>startmap_async</code> is an iterable. Since we want Thumper to
literally work with millions of images, I coded a memory-efficient
generator that creates the arugment tuples as needed, rather than
calculating a huge list (one tuple for each image).</p>

<h2>Understanding Performance</h2>

<p>How well does it work? It actually takes some thought at first to
understand. Imagine you are using a machine with eight CPUs (or
cores). How many will you want Thumper to use?

Theoretically, it is going to be eight - the number of cores. It may
be another value, depending on (a) what else is going on in the
system, and (b) what's going on in your application. It's common to
find that you'll get the exact same performance using fewer processes
than the number of CPUs on the system. (That means you've hit a point
where raw CPU is no longer the bottleneck.)  And believe it or not,
I've also seen cases where using too many workers can cause contention
for resources and slow things down.</p>

<p>In my own experience, the only way to know for sure is to test:
repeating execution varying only the number of CPUs exercised. That's
what I did above. Let's focus on elapsed time, because minimizing that
is the whole point:</p>

<img src="/static/python-concurrency-story/thumper-multiprocessing-elapsed.svg"/>

<p>This was done on a c3.2xlarge EC2 instance, a hefty fellow
with eight CPUs. The horizontal axis is the number of worker processes
spawned by Thumper; the vertical is the total elapsed time to
thumbnail all images, so lower is better.</p>

<p>Looking at the graph, as you add worker processes, elapsed
time keeps decreasing until you hit six. Why not eight? Or seven? There are many
possible reasons, which can be very application specific. In general,
you have scaled out as much as you can via CPU, and have now hit
another bottleneck. Perhaps you are using all available memory, and
have started paging. Perhaps you are now saturating CPU cache. Perhaps
you are hitting an I/O throughput limit, where you can't load any more
bytes of images per second.</p>

<p>As you can see, learning the classes, methods and functions
provided by <code>multiprocessing</code> is only the first step.</p>

<p>For programming in general - in any language - scaling out across
CPUs will only get you so far. Plus, throwing more CPUs at a problem
gets expensive. If you <em>really</em> want to get the most out of the
machine, you also have to master getting the most out of a single
thread. And that's what we are going to talk about in part 3. The main
key here - the one secret that will let you really kick a single
thread's butt - is...</p>

<p>Well, I'll tell you when I publish part 3. <a href="/python-newsletter/">Make sure you're subscribed</a> to get the
announcement.</p>

<section id="footnotes">
    <ol>
	<li id="note_1"><p>For a single-process, multithreaded
	program, message passing can be implemented through a global,
	thread-safe queue or other data structure; multiple processes can do
	this through IPC.</p>
	</li>

	<li id="note_2"><p>Unless you use Python's
	<code>multiprocessing</code> module. But, let's not get ahead of
	ourselves.</p>
	</li>

	<li id="note_3"><p>This is oversimplified. Using C
	extensions can let you leverage multiple CPUs in about the way C can,
	at least within a certain limited portion of your Python
	application. And the story is different for implementations other than
	CPython.</p>

	<p>For most Python code that's being written today - in the form of
	pure Python applications, using the official language interpreter -
	using more than one CPU means using more than one process.</p>
	</li>


	<li id="note_4"><p>Or at least, less hard. And with fewer
	sharp pointy jagged edges.</p>
	</li>

	<li id="note_5"><p>Plus a few others, like <code>apply</code>
	- which is just <code>apply_async</code>, except it blocks before
	returning. Can't think of a reason to use this in a program that uses
	multiprocessing? Me neither.</p>
	</li>
    </ol>
</section>



	</div> 

    </div></body></html>