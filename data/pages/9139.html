<html><body><div><div class="entry-content">
		<p>If you are following some Machine Learning news, you certainly saw the work done by Ryan Dahl on <a href="http://tinyclouds.org/colorize/" target="_blank">Automatic Colorization</a> (<a href="https://news.ycombinator.com/item?id=10864801" target="_blank">Hacker News comments</a>, <a href="https://www.reddit.com/r/MachineLearning/comments/4007ma/colorizing_black_and_white_photos_with_deep/" target="_blank">Reddit comments</a>). This amazing work uses pixel <a href="http://arxiv.org/abs/1411.5752" target="_blank">hypercolumn</a> information extracted from the VGG-16 network in order to colorize images. <a href="http://samim.io/" target="_blank">Samim </a>also used the network to process Black &amp; White video frames and produced the amazing video below:</p>
<p><iframe src="https://www.youtube.com/embed/_MJU8VK2PI4?feature=oembed" frameborder="0" allowfullscreen="">VIDEO</iframe></p>
<p class="yt watch-title-container"><em><span id="eow-title" class="watch-title " dir="ltr" title="Colorizing Black&amp;White Movies with Neural Networks">Colorizing Black&amp;White Movies with Neural Networks (video by Samim, network by Ryan)</span></em></p>
<p>But how does this hypercolumns works ? How to extract them to use on such variety of pixel classification problems ? The main idea of this post is to use the VGG-16 pre-trained network together with Keras and Scikit-Learn in order to extract the pixel hypercolumns and take a superficial look at the information present on it. I’m writing this because I haven’t found anything in Python to do that and this may be really useful for others working on pixel classification, segmentation, etc.</p>
<h2>Hypercolumns</h2>
<p>Many algorithms using features from CNNs (Convolutional Neural Networks) usually use the last FC (fully-connected) layer features in order to extract information about certain input. However, the information in the last FC layer may be too coarse spatially to allow precise localization (due to sequences of maxpooling, etc.), on the other side, the first layers may be spatially precise but will lack semantic information. To get the best of both worlds, the authors of the <a href="http://arxiv.org/pdf/1411.5752v2.pdf" target="_blank">hypercolumn paper</a> define the hypercolumn of a pixel as the vector of activations of all CNN units “above” that pixel.</p>
<figure id="attachment_2945" class="wp-caption aligncenter"><a href="http://blog.christianperone.com/wp-content/uploads/2016/01/hypercolumn.png" rel="attachment wp-att-2945"><img class="wp-image-2945 size-full" src="http://blog.christianperone.com/wp-content/uploads/2016/01/hypercolumn.png" alt="Hypercolumn Extraction" srcset="http://blog.christianperone.com/wp-content/uploads/2016/01/hypercolumn-300x202.png 300w, http://blog.christianperone.com/wp-content/uploads/2016/01/hypercolumn-768x517.png 768w, http://blog.christianperone.com/wp-content/uploads/2016/01/hypercolumn.png 861w" sizes="(max-width: 861px) 100vw, 861px"/></a><figcaption class="wp-caption-text"><strong>Hypercolumn Extraction</strong> (<em>by Hypercolumns for Object Segmentation and Fine-grained Localization</em>)</figcaption></figure>
<p>The first step on the extraction of the hypercolumns is to feed the image into the CNN (<em>Convolutional Neural Network</em>) and extract the feature map activations for each location of the image. The tricky part is when the feature maps are smaller than the input image, for instance after a pooling operation, the authors of the paper then do a bilinear upsampling of the feature map in order to keep the feature maps on the same size of the input. There are also the issue with the FC (<em>fully-connected</em>) layers, because you can’t isolate units semantically tied only to one pixel of the image, so the FC activations are seen as 1×1 feature maps, which means that all locations shares the same information regarding the FC part of the hypercolumn. All these activations are then concatenated to create the hypercolumn. For instance, if we take the VGG-16 architecture to use only the first 2 convolutional layers after the max pooling operations, we will have a hypercolumn with the size of:</p>
<p><strong>64 filters</strong> (<em>first conv layer before pooling</em>)</p>
<p><strong>+</strong></p>
<p><strong>128 filters</strong> (<em>second conv layer before pooling</em> ) = <strong>192 features</strong></p>
<p>This means that each pixel of the image will have a 192-dimension hypercolumn vector. This hypercolumn is really interesting because it will contain information about the first layers (where we have a lot of spatial information but little semantic) and also information about the final layers (with little spatial information and lots of semantics). Thus this hypercolumn will certainly help in a lot of pixel classification tasks such as the one mentioned earlier of automatic colorization, because each location hypercolumn carries the information about what this pixel semantically and spatially represents. This is also very helpful on segmentation tasks (you can see more about that on the original paper introducing the hypercolumn concept).</p>
<p>Everything sounds cool, but how do we extract hypercolumns in practice ?</p>
<h2>VGG-16</h2>
<p>Before being able to extract the hypercolumns, we’ll setup the VGG-16 pre-trained network, because you know, the price of a good GPU (I can’t even imagine many of them) here in Brazil is very expensive and I don’t want to sell my kidney to buy a GPU.</p>
<figure id="attachment_2954" class="wp-caption aligncenter"><a href="http://blog.christianperone.com/wp-content/uploads/2016/01/vgg16arch.png" rel="attachment wp-att-2954"><img class="wp-image-2954 size-large" src="http://blog.christianperone.com/wp-content/uploads/2016/01/vgg16arch-1024x263.png" alt="VGG16 Network Architecture (by Zhicheng Yan et al.)" srcset="http://blog.christianperone.com/wp-content/uploads/2016/01/vgg16arch-300x77.png 300w, http://blog.christianperone.com/wp-content/uploads/2016/01/vgg16arch-768x197.png 768w, http://blog.christianperone.com/wp-content/uploads/2016/01/vgg16arch-1024x263.png 1024w, http://blog.christianperone.com/wp-content/uploads/2016/01/vgg16arch.png 1460w" sizes="(max-width: 604px) 100vw, 604px"/></a><figcaption class="wp-caption-text">VGG16 Network Architecture (by Zhicheng Yan et al.)</figcaption></figure>
<p>To setup a pretrained VGG-16 network on Keras, you’ll need to download the weights file <a href="https://drive.google.com/file/d/0Bz7KyqmuGsilT0J5dmRCM0ROVHc/view?usp=sharing" target="_blank">from here</a> (vgg16_weights.h5 file with approximately 500MB) and then setup the architecture and load the downloaded weights using Keras (<em>more information about the weights file and architecture <a href="https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3" target="_blank">here</a></em>):</p>
<pre class="EnlighterJSRAW" data-enlighter-language="python">from matplotlib import pyplot as plt

import theano
import cv2
import numpy as np
import scipy as sp

from keras.models import Sequential
from keras.layers.core import Flatten, Dense, Dropout
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.layers.convolutional import ZeroPadding2D
from keras.optimizers import SGD

from sklearn.manifold import TSNE
from sklearn import manifold
from sklearn import cluster
from sklearn.preprocessing import StandardScaler

def VGG_16(weights_path=None):
    model = Sequential()
    model.add(ZeroPadding2D((1,1),input_shape=(3,224,224)))
    model.add(Convolution2D(64, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(64, 3, 3, activation='relu'))
    model.add(MaxPooling2D((2,2), stride=(2,2)))

    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(128, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(128, 3, 3, activation='relu'))
    model.add(MaxPooling2D((2,2), stride=(2,2)))

    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(256, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(256, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(256, 3, 3, activation='relu'))
    model.add(MaxPooling2D((2,2), stride=(2,2)))

    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(512, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(512, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(512, 3, 3, activation='relu'))
    model.add(MaxPooling2D((2,2), stride=(2,2)))

    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(512, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(512, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(512, 3, 3, activation='relu'))
    model.add(MaxPooling2D((2,2), stride=(2,2)))

    model.add(Flatten())
    model.add(Dense(4096, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(4096, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(1000, activation='softmax'))

    if weights_path:
        model.load_weights(weights_path)

    return model</pre>
<p>As you can see, this is a very simple code to declare the VGG16 architecture and load the pre-trained weights (together with Python imports for the required packages). After that we’ll compile the Keras model:</p>
<pre class="EnlighterJSRAW" data-enlighter-language="python">model = VGG_16('vgg16_weights.h5')
sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(optimizer=sgd, loss='categorical_crossentropy')</pre>
<p>Now let’s test the network using an image:</p>
<pre class="EnlighterJSRAW" data-enlighter-language="python">im_original = cv2.resize(cv2.imread('madruga.jpg'), (224, 224))
im = im_original.transpose((2,0,1))
im = np.expand_dims(im, axis=0)
im_converted = cv2.cvtColor(im_original, cv2.COLOR_BGR2RGB)
plt.imshow(im_converted)</pre>
<p><a href="http://blog.christianperone.com/wp-content/uploads/2016/01/madruga.png" rel="attachment wp-att-2956"><img class="size-full wp-image-2956 aligncenter" src="http://blog.christianperone.com/wp-content/uploads/2016/01/madruga.png" alt="Image used"/></a></p>
<p>Image used</p>
<p>As we can see, we loaded the image, fixed the axes and then we can now feed the image into the VGG-16 to get the predictions:</p>
<pre class="EnlighterJSRAW" data-enlighter-language="python">out = model.predict(im)
plt.plot(out.ravel())</pre>
<p> </p>
<figure id="attachment_2958" class="wp-caption aligncenter"><a href="http://blog.christianperone.com/wp-content/uploads/2016/01/preds_ravel.png" rel="attachment wp-att-2958"><img class="wp-image-2958 size-full" src="http://blog.christianperone.com/wp-content/uploads/2016/01/preds_ravel.png" alt="Predictions" srcset="http://blog.christianperone.com/wp-content/uploads/2016/01/preds_ravel-300x198.png 300w, http://blog.christianperone.com/wp-content/uploads/2016/01/preds_ravel.png 388w" sizes="(max-width: 388px) 100vw, 388px"/></a><figcaption class="wp-caption-text">Predictions</figcaption></figure>
<p>As you can see, these are the final activations of the softmax layer, the class with the “jersey, T-shirt, tee shirt” category.</p>
<h2>Extracting arbitrary feature maps</h2>
<p>Now, to extract the feature map activations, we’ll have to being able to extract feature maps from arbitrary convolutional layers of the network. We can do that by compiling a Theano function using the <em>get_output()</em> method of Keras, like in the example below:</p>
<pre class="EnlighterJSRAW" data-enlighter-language="python">get_feature = theano.function([model.layers[0].input], model.layers[3].get_output(train=False), allow_input_downcast=False)
feat = get_feature(im)
plt.imshow(feat[0][2])</pre>
<p><a href="http://blog.christianperone.com/wp-content/uploads/2016/01/fmap.png" rel="attachment wp-att-2959"><img class="size-full wp-image-2959 aligncenter" src="http://blog.christianperone.com/wp-content/uploads/2016/01/fmap.png" alt="Feature Map"/></a></p>
<p>Feature Map</p>
<p>In the example above, I’m compiling a Theano function to get the 3 layer (a convolutional layer) feature map and then showing only the 3rd feature map. Here we can see the intensity of the activations. If we get feature maps of the activations from the final layers, we can see that the extracted features are more abstract, like eyes, etc. Look at this example below from the 15th convolutional layer:</p>
<pre class="EnlighterJSRAW" data-enlighter-language="null">get_feature = theano.function([model.layers[0].input], model.layers[15].get_output(train=False), allow_input_downcast=False)
feat = get_feature(im)
plt.imshow(feat[0][13])</pre>
<p><a href="http://blog.christianperone.com/wp-content/uploads/2016/01/absfeat.png" rel="attachment wp-att-2960"><img class="aligncenter wp-image-2960 size-full" src="http://blog.christianperone.com/wp-content/uploads/2016/01/absfeat.png" alt="More semantic feature maps" srcset="http://blog.christianperone.com/wp-content/uploads/2016/01/absfeat-150x150.png 150w, http://blog.christianperone.com/wp-content/uploads/2016/01/absfeat.png 254w" sizes="(max-width: 254px) 100vw, 254px"/></a></p>
<p>More semantic feature maps.</p>
<p>As you can see, this second feature map is extracting more abstract features. And you can also note that the image seems to be more stretched when compared with the feature we saw earlier, that is because the the first feature maps has 224×224 size and this one has 56×56 due to the downscaling operations of the layers before the convolutional layer, and that is why we lose a lot of spatial information.</p>
<h2>Extracting hypercolumns</h2>
<p>Now finally let’s extract the hypercolumns of arbitrary set of layers. To do that, we will define a function to extract these hypercolumns:</p>
<pre class="EnlighterJSRAW" data-enlighter-language="python">def extract_hypercolumn(model, layer_indexes, instance):
    layers = [model.layers[li].get_output(train=False) for li in layer_indexes]
    get_feature = theano.function([model.layers[0].input], layers,
                                  allow_input_downcast=False)
    feature_maps = get_feature(instance)
    hypercolumns = []
    for convmap in feature_maps:
        for fmap in convmap[0]:
            upscaled = sp.misc.imresize(fmap, size=(224, 224),
                                        mode="F", interp='bilinear')
            hypercolumns.append(upscaled)

    return np.asarray(hypercolumns)</pre>
<p>As we can see, this function will expect three parameters: the model itself, an list of layer indexes that will be used to extract the hypercolumn features and an image instance that will be used to extract the hypercolumns. Let’s now test the hypercolumn extraction for the first 2 convolutional layers:</p>
<pre class="EnlighterJSRAW" data-enlighter-language="python">layers_extract = [3, 8]
hc = extract_hypercolumn(model, layers_extract, im)</pre>
<p>That’s it, we extracted the hypercolumn vectors for each pixel. The shape of this “hc” variable is: (192L, 224L, 224L), which means that we have a 192-dimensional hypercolumn for each one of the 224×224 pixel (a total of 50176 pixels with 192 hypercolumn feature each).</p>
<p>Let’s plot the average of the hypercolumns activations for each pixel:</p>
<pre class="EnlighterJSRAW" data-enlighter-language="python">ave = np.average(hc.transpose(1, 2, 0), axis=2)
plt.imshow(ave)</pre>
<figure id="attachment_2963" class="wp-caption aligncenter"><a href="http://blog.christianperone.com/wp-content/uploads/2016/01/hc_38.png" rel="attachment wp-att-2963"><img class="size-full wp-image-2963" src="http://blog.christianperone.com/wp-content/uploads/2016/01/hc_38.png" alt="Hypercolumn average for layers 3 and 8."/></a><figcaption class="wp-caption-text">Hypercolumn average for layers 3 and 8.</figcaption></figure>
<p>Ad you can see, those first hypercolumn activations are all looking like edge detectors, let’s see how these hypercolumns looks like for the layers 22 and 29:</p>
<pre class="EnlighterJSRAW" data-enlighter-language="python">layers_extract = [22, 29]
hc = extract_hypercolumn(model, layers_extract, im)
ave = np.average(hc.transpose(1, 2, 0), axis=2)
plt.imshow(ave)
</pre>
<figure id="attachment_2964" class="wp-caption aligncenter"><a href="http://blog.christianperone.com/wp-content/uploads/2016/01/hc2229.png" rel="attachment wp-att-2964"><img class="size-full wp-image-2964" src="http://blog.christianperone.com/wp-content/uploads/2016/01/hc2229.png" alt="Hypercolumn average for the layers 22 and 29."/></a><figcaption class="wp-caption-text">Hypercolumn average for the layers 22 and 29.</figcaption></figure>
<p>As we can see now, the features are really more abstract and semantically interesting but with spatial information a little fuzzy.</p>
<p>Remember that you can extract the hypercolumns using all the initial layers and also the final layers, including the FC layers. Here I’m extracting them separately to show how they differ in the visualization plots.</p>
<h2>Simple hypercolumn pixel clustering</h2>
<p>Now, you can do a lot of things, you can use these hypercolumns to classify pixels for some task, to do automatic pixel colorization, segmentation, etc. What I’m going to do here just as an experiment, is to use the hypercolumns (from the VGG-16 layers 3, 8, 15, 22, 29) and then cluster it using KMeans with 2 clusters:</p>
<pre class="EnlighterJSRAW" data-enlighter-language="python">m = hc.transpose(1,2,0).reshape(50176, -1)
kmeans = cluster.KMeans(n_clusters=2, max_iter=300, n_jobs=5, precompute_distances=True)
cluster_labels = kmeans .fit_predict(m)

imcluster = np.zeros((224,224))
imcluster = imcluster.reshape((224*224,))
imcluster = cluster_labels

plt.imshow(imcluster.reshape(224, 224), cmap="hot")</pre>
<figure id="attachment_2967" class="wp-caption aligncenter"><a href="http://blog.christianperone.com/wp-content/uploads/2016/01/cluster2.png" rel="attachment wp-att-2967"><img class="size-full wp-image-2967" src="http://blog.christianperone.com/wp-content/uploads/2016/01/cluster2.png" alt="KMeans clustering using hypercolumns."/></a><figcaption class="wp-caption-text">KMeans clustering using hypercolumns.</figcaption></figure>
<p>Now you can imagine how useful hypercolumns can be to tasks like keypoints extraction, segmentation, etc. It’s a very elegant, simple and useful concept.</p>
<p>I hope you liked it !</p>
<p><em>– Christian S. Perone</em></p>
	</div>
	
	</div></body></html>