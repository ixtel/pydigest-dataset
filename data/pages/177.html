<html><body><div><div class="content html_format">
      <h4>Введение</h4><p>
Добрый день, уважаемые читатели.</p><p>
Недавно, бродя по просторам глобальной паутины, я наткнулся на </p><a href="https://www.tcsbank.ru/tournament/">турнир</a><p>, который проводился банком ТКС в начале этого года. Ознакомившись с заданиями, я решил проверить свои навыки в анализе данных на них.</p><p>
Начать проверку я решил с задачи о скоринге (Задание №3). Для ее решения я, как всегда, использовал Python с аналитическими модулями </p><a href="http://pandas.pydata.org/pandas-docs/stable/">pandas</a><p> и </p><a href="http://scikit-learn.org/stable/index.html">scikit-learn</a><p>.
</p><a name="habracut"/>
<h4>Описание данных и постановка задачи</h4><p>
Банк запрашивает кредитную историю заявителя в трех крупнейших российских кредитных бюро. Предоставляется выборка клиентов Банка в файле </p><a href="https://static.tcsbank.ru/documents/olymp/SAMPLE_CUSTOMERS.csv">SAMPLE_CUSTOMERS.CSV</a><p>. Выборка разделена на части «train» и «test». По выборке «train» известно значение целевой переменной bad — наличие "дефолта" (допущение клиентом просрочки 90 и более дней в течение первого года пользования кредитом). В файле </p><a href="https://static.tcsbank.ru/documents/olymp/SAMPLE_ACCOUNTS.csv">SAMPLE_ACCOUNTS.CSV</a><p> предоставлены данные из ответов кредитных бюро на все запросы по соответствующим клиентам.</p><p>
Формат данных </p><b>SAMPLE_CUSTOMERS</b><p> – информация о возможности дефолта определенного человека.</p><p>
Описание формата набора данных </p><b>SAMPLE_ACCOUNTS</b><p>:
</p><div class="spoiler"><b class="spoiler_title">Описание набора</b><div class="spoiler_text"><table>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
<tr>
<td><i>TCS_CUSTOMER_ID</i></td>
<td>Идентификатор клиента</td>
</tr>
<tr>
<td><i>BUREAU_CD</i></td>
<td>Код бюро, из которого получен счет</td>
</tr>
<tr>
<td><i>BKI_REQUEST_DATE</i></td>
<td>Дата, в которую был сделан запрос в бюро</td>
</tr>
<tr>
<td><i>CURRENCY</i></td>
<td>Валюта договора (ISO буквенный код валюты)</td>
</tr>
<tr>
<td><i>RELATIONSHIP</i></td>
<td>Тип отношения к договору</td>
</tr>
<tr>
<td rowspan="5"/>
<td>1 — Физическое лицо</td>
</tr>
<tr>
<td>2 — Дополнительная карта/Авторизованный пользователь</td>
</tr>
<tr>
<td>4 — Совместный</td>
</tr>
<tr>
<td>5 — Поручитель</td>
</tr>
<tr>
<td>9 — Юридическое лицо</td>
</tr>
<tr>
<td><i>OPEN_DATE</i></td>
<td>Дата открытия договора</td>
</tr>
<tr>
<td><i>FINAL_PMT_DATE</i></td>
<td>Дата финального платежа (плановая)</td>
</tr>
<tr>
<td><i>TYPE</i></td>
<td>Код типа договора</td>
</tr>
<tr>
<td rowspan="11"/>
<td>1 – Кредит на автомобиль</td>
</tr>
<tr>
<td>4 – Лизинг</td>
</tr>
<tr>
<td>6 – Ипотека</td>
</tr>
<tr>
<td>7 – Кредитная карта</td>
</tr>
<tr>
<td>9 – Потребительский кредит</td>
</tr>
<tr>
<td>10 – Кредит на развитие бизнеса</td>
</tr>
<tr>
<td>11 – Кредит на пополнение оборотных средств</td>
</tr>
<tr>
<td>12 – Кредит на покупку оборудования</td>
</tr>
<tr>
<td>13 – Кредит на строительство недвижимости</td>
</tr>
<tr>
<td>14 – Кредит на покупку акций (например, маржинальное кредитование)</td>
</tr>
<tr>
<td>99 – Другой</td>
</tr>
<tr>
<td><i>PMT_STRING_84M</i></td>
<td>Дисциплина (своевременность) платежей. Строка составляется из кодов состояний счета на моменты передачи банком данных по счету в бюро, первый символ — состояние на дату PMT_STRING_START, далее последовательно в порядке убывания дат.<br/>
</td>
</tr>
<tr>
<td rowspan="11"/>
<td>0 – Новый, оценка невозможна</td>
</tr>
<tr>
<td>X – Нет информации</td>
</tr>
<tr>
<td>1 – Оплата без просрочек</td>
</tr>
<tr>
<td>A – Просрочка от 1 до 29 дней</td>
</tr>
<tr>
<td>2 – Просрочка от 30 до 59 дней</td>
</tr>
<tr>
<td>3 – Просрочка от 60 до 89 дней</td>
</tr>
<tr>
<td>4 – Просрочка от 90 до 119 дней</td>
</tr>
<tr>
<td>5 – Просрочка более 120 дней</td>
</tr>
<tr>
<td>7 – Регулярные консолидированные платежи</td>
</tr>
<tr>
<td>8 – Погашение по кредиту с использованием залога</td>
</tr>
<tr>
<td>9 – Безнадёжный долг/ передано на взыскание/ пропущенный платеж</td>
</tr>
<tr>
<td><i>STATUS</i></td>
<td>Статус договора</td>
</tr>
<tr>
<td rowspan="7"/>
<td>00 – Активный</td>
</tr>
<tr>
<td>12 – Оплачен за счет обеспечения</td>
</tr>
<tr>
<td>13 – Счет закрыт</td>
</tr>
<tr>
<td>14 – Передан на обслуживание в другой банк</td>
</tr>
<tr>
<td>21 – Спор</td>
</tr>
<tr>
<td>52 – Просрочен</td>
</tr>
<tr>
<td>61 – Проблемы с возвратом</td>
</tr>
<tr>
<td><i>OUTSTANDING</i></td>
<td>Оставшаяся непогашенная задолженность. Сумма в рублях по курсу ЦБ РФ</td>
</tr>
<tr>
<td><i>NEXT_PMT</i><br/>
</td>
<td>Размер следующего платежа. Сумма в рублях по курсу ЦБ РФ</td>
</tr>
<tr>
<td><i>INF_CONFIRM_DATE</i></td>
<td>Дата подтверждения информации по счету</td>
</tr>
<tr>
<td><i>FACT_CLOSE_DATE</i><br/>
</td>
<td>Дата закрытия счета (фактическая)<br/>
</td>
</tr>
<tr>
<td><i>TTL_DELQ_5</i><br/>
</td>
<td>Количество просрочек до 5 дней<br/>
</td>
</tr>
<tr>
<td><i>TTL_DELQ_5_29</i><br/>
</td>
<td>Количество просрочек от 5 до 29 дней<br/>
</td>
</tr>
<tr>
<td><i>TTL_DELQ_30_59</i><br/>
</td>
<td>Количество просрочек от 30 до 59 дней<br/>
</td>
</tr>
<tr>
<td><i>TTL_DELQ_60_89</i><br/>
</td>
<td>Количество просрочек от 60 до 89 дней<br/>
</td>
</tr>
<tr>
<td><i>TTL_DELQ_30</i><br/>
</td>
<td>Количество просрочек до 30 дней<br/>
</td>
</tr>
<tr>
<td><i>TTL_DELQ_90_PLUS</i><br/>
</td>
<td>Количество просрочек 90+ дней<br/>
</td>
</tr>
<tr>
<td><i>PMT_FREQ</i><br/>
</td>
<td>Код частоты платежей<br/>
</td>
</tr>
<tr>
<td rowspan="9"> </td>
<td>1 – Еженедельно<br/>
</td>
</tr>
<tr>
<td>2 – Раз в две недели<br/>
</td>
</tr>
<tr>
<td>3 – Ежемесячно<br/>
</td>
</tr>
<tr>
<td>A — Раз в 2 месяца<br/>
</td>
</tr>
<tr>
<td>4 – Поквартально<br/>
</td>
</tr>
<tr>
<td>B — Раз в 4 месяца<br/>
</td>
</tr>
<tr>
<td>5 – Раз в полгода<br/>
</td>
</tr>
<tr>
<td>6 — Ежегодно<br/>
</td>
</tr>
<tr>
<td>7 – Другое<br/>
</td>
</tr>
<tr>
<td><i>CREDIT_LIMIT</i><br/>
</td>
<td>Кредитный лимит. Сумма в рублях по курсу ЦБ РФ<br/>
</td>
</tr>
<tr>
<td><i>DELQ_BALANCE</i><br/>
</td>
<td>Текущая просроченная задолженность. Сумма в рублях по курсу ЦБ РФ<br/>
</td>
</tr>
<tr>
<td><i>MAX_DELQ_BALANCE</i><br/>
</td>
<td>Максимальный объем просроченной задолженности. Сумма в рублях по курсу ЦБ РФ<br/>
</td>
</tr>
<tr>
<td><i>CURRENT_DELQ</i><br/>
</td>
<td>Текущее количество дней просрочки<br/>
</td>
</tr>
<tr>
<td><i>PMT_STRING_START</i><br/>
</td>
<td>Дата начала строки PMT_STRING_84M<br/>
</td>
</tr>
<tr>
<td><i>INTEREST_RATE</i><br/>
</td>
<td>Процентная ставка по кредиту<br/>
</td>
</tr>
<tr>
<td><i>CURR_BALANCE_AMT</i><br/>
</td>
<td>Общая выплаченная сумма, включая сумму основного долга, проценты, пени и штрафы. Сумма в рублях по курсу ЦБ РФ<br/>
</td>
</tr>
</table>
</div></div>
<p>
Задача состоит в том, чтобы на выборке </p><i>«train»</i><p> необходимо построить модель, определяющую вероятность «дефолта», и проставить вероятности ее по клиентам из выборки </p><i>«test»</i><p>. Для оценки модели будет использоваться характеристика </p><b>Area Under ROC Curve</b><p> (также указано в условиях задачи).

</p><h4>Предварительная обработка данных</h4><p>
Для начала загрузим исходные файлы и посмотрим на них:
</p><pre><code class="python">from pandas import read_csv, DataFrame
from sklearn.metrics import roc_curve
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.cross_validation import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.decomposition import PCA
import ml_metrics, string, re, pylab as pl

SampleCustomers = read_csv("https://static.tcsbank.ru/documents/olymp/SAMPLE_CUSTOMERS.csv", ';')
SampleAccounts = read_csv("https://static.tcsbank.ru/documents/olymp/SAMPLE_ACCOUNTS.csv",";",decimal =',')

print SampleAccounts
</code></pre>
<img src="https://habrastorage.org/getpro/habr/post_images/428/028/3df/4280283df2545a36cc1c3220e32badea.png" alt="image"/>

<pre><code class="python">SampleCustomers.head()</code></pre>
<table>
<tr>
<th/>
<th>tcs_customer_id</th>
<th>bad</th>
<th>sample_type</th>
</tr>
<tr>
<th>0</th>
<td> 1</td>
<td>NaN</td>
<td> test</td>
</tr>
<tr>
<th>1</th>
<td> 2</td>
<td> 0</td>
<td> train</td>
</tr>
<tr>
<th>2</th>
<td> 3</td>
<td> 1</td>
<td> train</td>
</tr>
<tr>
<th>3</th>
<td> 4</td>
<td> 0</td>
<td> train</td>
</tr>
<tr>
<th>4</th>
<td> 5</td>
<td> 0</td>
<td> train</td>
</tr>
</table>
<p>
Из условий задачи можно предположить, что набор SampleAccounts содержит несколько записей по одному заемщику давайте проверим это:

</p><pre><code class="python">SampleAccounts.tcs_customer_id.drop_duplicates().count(), SampleAccounts.tcs_customer_id.count()</code></pre><p>
Наше предположение оказалось верным. Уникальных заемщиков 50000 из 280942 записей. Это связано с тем, что у одно заемщика быть несколько кредитов и по каждому из них в разных бюро моте быть разная информация. Следовательно, надо выполнить преобразования над SampleAccounts, чтобы одному заемщику соответствовала одна строка.</p><p>
Теперь давайте получим список все уникальных кредитов по каждому заемщику:

</p><pre><code class="python">SampleAccounts[['tcs_customer_id','open_date','final_pmt_date','credit_limit','currency']].drop_duplicates()</code></pre><p>
Следовательно, когда мы получили список кредитов, мы сможем вывести какую-либо общую информацию по каждому элементу списка. Т.е. можно было бы взять связку из перечисленных выше полей и сделать ее индексом, относительно которого мы бы производили дальнейшие манипуляции, но, к сожалению, тут нас подстерегает один неприятный момент. Он заключается в том, что поле 'final_pmt_date' в наборе данных имеет незаполненные значения. Давайте попробуем избавиться от них.</p><p>
У нас в наборе есть поле фактическая дата закрытия кредита, следовательно, если она есть, а поле 'final_pmt_date' не заполнено, то можно в него записать данное значение. Для остальных же просто запишем 0.

</p><pre><code class="python">SampleAccounts.final_pmt_date[SampleAccounts.final_pmt_date.isnull()] = SampleAccounts.fact_close_date[SampleAccounts.final_pmt_date.isnull()].astype(float)
SampleAccounts.final_pmt_date.fillna(0, inplace=True)
</code></pre><p>
Теперь, когда от пустых значений мы избавились, давайте получим самую свежую дату обращения в какое-либо из бюро по каждому из кредитов. Это пригодиться нам для определения его атрибутов, таких как статус договора, тип и т.д.

</p><pre><code class="python">sumtbl = SampleAccounts.pivot_table(['inf_confirm_date'],  ['tcs_customer_id','open_date','final_pmt_date','credit_limit','currency'], aggfunc='max')
sumtbl.head(15)
</code></pre>
<table>
<tr>
<th/>
<th/>
<th/>
<th/>
<th/>
<th>inf_confirm_date</th>
</tr>
<tr>
<th>tcs_customer_id</th>
<th>open_date</th>
<th>final_pmt_date</th>
<th>credit_limit</th>
<th>currency</th>
<th/>
</tr>
<tr>
<td rowspan="8">1</td>
<td>39261</td>
<td>39629</td>
<td>19421 </td>
<td>RUB</td>
<td> 39924</td>
</tr>
<tr>
<td>39505</td>
<td>39870</td>
<td>30000 </td>
<td>RUB</td>
<td> 39862</td>
</tr>
<tr>
<td>39644</td>
<td>40042</td>
<td>11858 </td>
<td>RUB</td>
<td> 40043</td>
</tr>
<tr>
<td>39876</td>
<td>41701</td>
<td>300000</td>
<td>RUB</td>
<td> 40766</td>
</tr>
<tr>
<td>39942</td>
<td>40308</td>
<td>19691 </td>
<td>RUB</td>
<td> 40435</td>
</tr>
<tr>
<td>40421</td>
<td>42247</td>
<td>169000</td>
<td>RUB</td>
<td> 40756</td>
</tr>
<tr>
<td>40428</td>
<td>51386</td>
<td>10000 </td>
<td>RUB</td>
<td> 40758</td>
</tr>
<tr>
<td>40676</td>
<td>41040</td>
<td>28967 </td>
<td>RUB</td>
<td> 40764</td>
</tr>
<tr>
<td rowspan="2">2</td>
<td>40472</td>
<td>40618</td>
<td>7551 </td>
<td>RUB</td>
<td> 40661</td>
</tr>
<tr>
<td>40652</td>
<td>40958</td>
<td>21186 </td>
<td>RUB</td>
<td> 40661</td>
</tr>
<tr>
<td rowspan="2">3</td>
<td>39647</td>
<td>40068</td>
<td>22694 </td>
<td>RUB</td>
<td> 40069</td>
</tr>
<tr>
<td>40604</td>
<td>0 </td>
<td>20000 </td>
<td>RUB</td>
<td> 40624</td>
</tr>
<tr>
<td rowspan="3">4</td>
<td>38552</td>
<td>40378</td>
<td>75000 </td>
<td>RUB</td>
<td> 40479</td>
</tr>
<tr>
<td>39493</td>
<td>39797</td>
<td>5000 </td>
<td>RUB</td>
<td> 39823</td>
</tr>
<tr>
<td>39759</td>
<td>40123</td>
<td>6023 </td>
<td>RUB</td>
<td> 40125</td>
</tr>
</table><p>
Теперь добавим полученные нами даты к основному набору:

</p><pre><code class="python">SampleAccounts = SampleAccounts.merge(sumtbl, 'left', 
                                     left_on=['tcs_customer_id','open_date','final_pmt_date','credit_limit','currency'], 
                                     right_index=True,
                                     suffixes=('', '_max'))</code></pre><p>
Итак, далее мы разобьем столбы, в которых параметры строго определены, таким образом, чтобы каждому значению из этих полей соответствовал отдельный столбец. По условию столбцами с заданными значениями будут:
</p><ul>
<li>pmt_string_84m</li>
<li>pmt_freq</li>
<li>type</li>
<li>status</li>
<li>relationship</li>
<li>bureau_cd</li>
</ul><p>
Код для их преобразования приведен ниже:

</p><pre><code class="python"># преобразуем pmt_string_84m
vals = list(xrange(10)) + ['A','X']
PMTstr = DataFrame([{'pmt_string_84m_%s' % (str(j)): str(i).count(str(j)) for j in vals} for i in SampleAccounts.pmt_string_84m])
SampleAccounts = SampleAccounts.join(PMTstr).drop(['pmt_string_84m'], axis=1)

# преобразуем pmt_freq
SampleAccounts.pmt_freq.fillna(7, inplace=True)
SampleAccounts.pmt_freq[SampleAccounts.pmt_freq == 0] = 7
vals = list(range(1,8)) + ['A','B']
PMTstr = DataFrame([{'pmt_freq_%s' % (str(j)): str(i).count(str(j)) for j in vals} for i in SampleAccounts.pmt_freq])
SampleAccounts = SampleAccounts.join(PMTstr).drop(['pmt_freq'], axis=1)

# преобразуем type
vals = [1,4,6,7,9,10,11,12,13,14,99]
PMTstr = DataFrame([{'type_%s' % (str(j)): str(i).count(str(j)) for j in vals} for i in SampleAccounts.type])
SampleAccounts = SampleAccounts.join(PMTstr).drop(['type'], axis=1)

# преобразуем status
vals = [0,12, 13, 14, 21, 52,61]
PMTstr = DataFrame([{'status_%s' % (str(j)): str(i).count(str(j)) for j in vals} for i in SampleAccounts.status])
SampleAccounts = SampleAccounts.join(PMTstr).drop(['status'], axis=1)

# преобразуем relationship
vals = [1,2,4,5,9]
PMTstr = DataFrame([{'relationship_%s' % (str(j)): str(i).count(str(j)) for j in vals} for i in SampleAccounts.relationship])
SampleAccounts = SampleAccounts.join(PMTstr).drop(['relationship'], axis=1)

# преобразуем bureau_cd
vals = [1,2,3]
PMTstr = DataFrame([{'bureau_cd_%s' % (str(j)): str(i).count(str(j)) for j in vals} for i in SampleAccounts.bureau_cd])
SampleAccounts = SampleAccounts.join(PMTstr).drop(['bureau_cd'], axis=1)
</code></pre><p>
Следующим шагом, преобразуем поле 'fact_close_date', в котором содержится дата последнего фактического платежа, чтобы в нем содержалось только 2 значения:
</p><ul>
<li>0 — не было последнего платежа</li>
<li>1 — последний платеж был</li>
</ul><p>
Данную замену я сделал потому, что изначально поле было заполнено наполовину.

</p><pre><code class="python">SampleAccounts.fact_close_date[SampleAccounts.fact_close_date.notnull()] = 1
SampleAccounts.fact_close_date.fillna(0, inplace=True)
</code></pre><p>
Теперь из нашего набора данных нам надо вытащить свежие данные по всем кредитам. В этом нам поможет поле </p><i>«inf_confirm_date_max»</i><p>, полученное выше. В него мы добавили крайнюю дату обновления информации по кредиту во всех бюро:

</p><pre><code class="python">PreFinalDS = SampleAccounts[SampleAccounts.inf_confirm_date == SampleAccounts.inf_confirm_date_max].drop_duplicates()</code></pre><p>
После вышеописанных действий наша выборка существенно сократилась, но теперь нам надо обобщить всю информацию по кредиту и заемщику полученную ранее. Для этого произведем группировку нашего набора данных:

</p><pre><code class="python">PreFinalDS = PreFinalDS.groupby(['tcs_customer_id','open_date','final_pmt_date','credit_limit','currency']).max().reset_index()</code></pre><p>
Наши данные почти готовы к началу анализа. Осталось выполнить еще несколько действий:
</p><ol>
<li>Убрать ненужные столбцы</li>
<li>Привести все кредитные лимиты в рубли</li>
<li>Посчитать какое количество кредитов у каждого заемщика по информации от бюро</li>
</ol><p>
Начнем с очистки таблицы от ненужных столбцов:

</p><pre><code class="python">PreFinalDS = PreFinalDS.drop(['bki_request_date',
                              'inf_confirm_date',
                              'pmt_string_start',
                              'interest_rate',
                              'open_date',
                              'final_pmt_date',
                              'inf_confirm_date_max'], axis=1)
</code></pre><p>
Далее переведем все кредитные лимиты к рублям. Для простоты я взял курсы валют на текущий момент. Хотя правильнее наверное было бы брать курс на момент открытия счета. Еще один нюанс, в том, что для анализа нам надо убрать текстовое поле </p><i>«сurrency»</i><p>, поэтому после перевода валют в рубли мы проведем с этим полем манипуляцию, которые мы провели с полями выше:

</p><pre><code class="python">curs = DataFrame([33.13,44.99,36.49,1], index=['USD','EUR','GHF','RUB'], columns=['crs'])
PreFinalDS = PreFinalDS.merge(curs, 'left', left_on='currency', right_index=True)
PreFinalDS.credit_limit = PreFinalDS.credit_limit * PreFinalDS.crs

#выделяем значения в отдельные столбцы
vals = ['RUB','USD','EUR','CHF']
PMTstr = DataFrame([{'currency_%s' % (str(j)): str(i).count(str(j)) for j in vals} for i in PreFinalDS.currency])
PreFinalDS = PreFinalDS.join(PMTstr).drop(['currency','crs'], axis=1)
</code></pre><p>
Итак перед заключительной группировкой добавим к нашему набору поле заполненное единицами. Т.е. когда мы выполним последнюю группировку, сумма по нему даст количество кредитов у заемщика:

</p><pre><code class="python">PreFinalDS['count_credit'] = 1</code></pre><p>
Теперь, когда у нас в наборе данных все данные количественные, можно заполнить пробелы в данных 0 и выполнить заключительную группировку по клиенту:
</p><pre><code class="python">PreFinalDS.fillna(0, inplace=True)
FinalDF = PreFinalDS.groupby('tcs_customer_id').sum()
FinalDF
</code></pre>

<h4>Предварительный анализ</h4>
<p>
Ну что же первичная обработка данных завершена и можно приступить к их анализу. Для начала разделим наши данные на обучающую и тестовую выборки. В этом нам поможет столбец </p><i>«sample_type»</i><p> из SampleCustomers, по нему как раз сделано такое разделение.</p><p>
Для того чтобы разбить наш обработанный DataFrame, достаточно объединить его с SampleCustomers поиграться фильтрами:

</p><pre><code class="python">SampleCustomers.set_index('tcs_customer_id', inplace=True)
UnionDF = FinalDF.join(SampleCustomers)
trainDF = UnionDF[UnionDF.sample_type == 'train'].drop(['sample_type'], axis=1)
testDF = UnionDF[UnionDF.sample_type == 'test'].drop(['sample_type'], axis=1)
</code></pre><p>
Далее давайте посмотрим, как признаки коррелирует между собой, для этого построим матрицу с коэффициентами корреляции признаков. С помощью pandas это можно сделать одной командой:

</p><pre><code class="python">CorrKoef = trainDF.corr()</code></pre><p>
После действия выше CorrKoef будет содержать матрицу размеров 61x61. </p><p>
Строками и столбцами ее будут соответствующие имена полей, а на их пересечении — значение коэффициента корреляции. Например:
</p><table>
<tr>
<td/>
<th>fact_close_date</th>
</tr>
<tr>
<th>status_13</th>
<td>0.997362</td>
</tr>
</table><p>
Возможен случай, когда коэффициента корреляции нет. Это значит, что в данные поля скорее всего заполнены только одним одинаковым значением и их можно опустить при анализе. Проверим:

</p><pre><code class="python">FieldDrop = [i for i in CorrKoef if CorrKoef[i].isnull().drop_duplicates().values[0]]
</code></pre><p>
На выходе мы получили список полей которые можно удалить:
</p><ul>
<li>pmt_string_84m_6</li>
<li>pmt_string_84m_8</li>
<li>pmt_freq_5</li>
<li>pmt_freq_A</li>
<li>pmt_freq_B</li>
<li>status_12</li>
</ul><p>
Следующим шагом мы найдем поля которые коррелируют между собой (у которых коэффициент корреляции больше 90%), используя нашу матрицу:

</p><pre><code class="python">CorField = []
for i in CorrKoef:
    for j in CorrKoef.index[CorrKoef[i] &gt; 0.9]:
        if i &lt;&gt; j and j not in CorField and i not in CorField:
            CorField.append(j)
            print "%s--&gt;%s: r^2=%f" % (i,j, CorrKoef[i][CorrKoef.index==j].values[0])
</code></pre><p>
На выходе получим следующие:
</p><p>
fact_close_date--&gt;status_13: r^2=0.997362</p><p>
ttl_delq_5_29--&gt;ttl_delq_30: r^2=0.954740</p><p>
ttl_delq_5_29--&gt;pmt_string_84m_A: r^2=0.925870</p><p>
ttl_delq_30_59--&gt;pmt_string_84m_2: r^2=0.903337</p><p>
ttl_delq_90_plus--&gt;pmt_string_84m_5: r^2=0.978239</p><p>
delq_balance--&gt;max_delq_balance: r^2=0.986967</p><p>
pmt_freq_3--&gt;relationship_1: r^2=0.909820</p><p>
pmt_freq_3--&gt;currency_RUB: r^2=0.910620</p><p>
pmt_freq_3--&gt;count_credit: r^2=0.911109
</p><p>
Итак, исходя из связей которые мы получили на предыдущем шаге, мы можем добавить в список удаления следующие поля:

</p><pre><code class="python">FieldDrop =FieldDrop + ['fact_close_date','ttl_delq_30',
                        'pmt_string_84m_5',
                        'pmt_string_84m_A',
                        'pmt_string_84m_A',
                        'max_delq_balance',
                        'relationship_1',
                        'currency_RUB',
                        'count_credit']
newtr = trainDF.drop(FieldDrop, axis=1)
</code></pre>

<h4>Построение и выбор модели</h4>
<p>
Ну что же первичные данные обработаны и теперь можно перейти к построению модели.</p><p>
Отделим признак класса от обучающей выборки:

</p><pre><code class="python">target = newtr.bad.values
train = newtr.drop('bad', axis=1).values
</code></pre><p>
Теперь давайте уменьшим размерность нашей выборки, дабы взять только значимые параметры. Для этого воспользуемся </p><a href="http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%B3%D0%BB%D0%B0%D0%B2%D0%BD%D1%8B%D1%85_%D0%BA%D0%BE%D0%BC%D0%BF%D0%BE%D0%BD%D0%B5%D0%BD%D1%82">методом главных компонент</a><p> и его реализацией </p><a href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA">PCA()</a><p> в модуле </p><i>sklearn</i><p>. В параметре мы передаем количество компонент, которые мы хотим сохранить(я выбрал 20, т.к. при них результаты моделей практически не отличались от результатов по исходным данным)

</p><pre><code class="python">coder = PCA(n_components=20)
train = coder.fit_transform(train)
</code></pre><p>
Пришло время для определения моделей классификации. Возьмем несколько различных алгоритмов и сравним результаты их работы при помощи характеристики </p><a href="http://ru.wikipedia.org/wiki/ROC-%D0%BA%D1%80%D0%B8%D0%B2%D0%B0%D1%8F">Area Under ROC Curve</a><p> (</p><b>auc</b><p>). Для моделирования будут рассмотрены следующие алгоритмы:
</p>

<pre><code class="python">models = []
models.append(RandomForestClassifier(n_estimators=165, max_depth=4, criterion='entropy'))
models.append(GradientBoostingClassifier(max_depth =4))
models.append(KNeighborsClassifier(n_neighbors=20))
models.append(GaussianNB())
</code></pre><p>
Итак модели выбраны. Давайте сейчас разобьем нашу обучающую выборку на 2 подвыборки: тестовую и обучающую. Данное действие нужно чтобы мы могли посчитать характеристику auc для наших моделей. Разбиение можно провести функцией </p><a href="http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html">train_test_split()</a><p> из модуля </p><i>sklearn</i><p>:

</p><pre><code class="python">TRNtrain, TRNtest, TARtrain, TARtest = train_test_split(train, target, test_size=0.3, random_state=0)</code></pre><p>
Осталось осталось обучить наши модели и оценить результат.</p><p>
Для расчета характеристики </p><b>auc</b><p> есть 2 пути:
</p><ol>
<li>Стандартными средствами модуля <i>sklearn</i> при помощи функции <a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score">roc_auc_score</a> или <a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html#sklearn.metrics.auc">auc</a></li>
<li>С помощью стороннего пакета <a href="https://github.com/benhamner/Metrics">ml_metrics</a> и функции auc()</li>
</ol><p>
Я воспользуюсь вторым способом, т.к. первый был показан в предыдущей статье. Пакет </p><b>ml_metrics</b><p> является очень полезным дополнением к sklearn, т.к. в нем присутствуют некоторые метрики, которых нет в sklearn.</p><p>
Итак, построим ROC кривые и посчитаем их площади:

</p><pre><code class="python">plt.figure(figsize=(10, 10)) 
for model in models:
    model.fit(TRNtrain, TARtrain)
    pred_scr = model.predict_proba(TRNtest)[:, 1]
    fpr, tpr, thresholds = roc_curve(TARtest, pred_scr)
    roc_auc = ml_metrics.auc(TARtest, pred_scr)
    md = str(model)
    md = md[:md.find('(')]
    pl.plot(fpr, tpr, label='ROC fold %s (auc = %0.2f)' % (md, roc_auc))

pl.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6))
pl.xlim([0, 1])
pl.ylim([0, 1])
pl.xlabel('False Positive Rate')
pl.ylabel('True Positive Rate')
pl.title('Receiver operating characteristic example')
pl.legend(loc="lower right")
pl.show()
</code></pre>
<img src="https://habrastorage.org/getpro/habr/post_images/e76/550/a20/e76550a20afe303651244c1ff6d82eb4.png" alt="image"/><p>
Итак, по результатам анализа наших моделей можно сказать, что лучше всего себя показал градиентный бустинг, его точность порядка 69%. Соответственно для обучения тестовой выборки мы выберем его. Давайте заполним информацию в тестовой выборке, предварительно обработав ее до нужного формата:

</p><pre><code class="python">#приводим тестовую выборку к нужному формату
FieldDrop.append('bad')
test = testDF.drop(FieldDrop, axis=1).values
test = coder.fit_transform(test)
#обучаем модель
model = models[1]
model.fit(train, target)
#записываем результат
testDF.bad = model.predict(test)
</code></pre>
<h4>Заключение</h4><p>
В качестве заключения хотелось бы отметить, что полученная точность модели в 69%, является недостаточно хорошей, но большей точности я добиться не смог. Хотелось бы отметить, тот факт, что при построении модели по полной размерности, т.е. без учета коррелируемых столбцов и сокращения размерности, она дала так же 69% точности (это можно легко проверить используя набор trainDF для обучения модели)</p><p>
В данной статье, я постарался показать все основные этапы анализа данных от первичной обработки сырых данных до построения модели классификатора. Кроме того, хотелось бы отметить, что в анализируемые модели не был включен метод опорных векторов, это связано с тем, что после нормализации данных точность модели опустилась до 51% и лучший результат который мне удалось получить с ним был в районе 60%, при значительных затратах по времени.</p><p>
Также хотелось бы отметить, что, к сожалению на тестовой выборке результат проверить не удалось, т.к. не уложился в сроки проведения турнира.

      
      </p><p class="clear"/>
    </div>

    
  </div></body></html>