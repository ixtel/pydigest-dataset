<html><body><div><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-autograd--" class="anchor" href="#autograd--" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Autograd  <a href="https://travis-ci.org/HIPS/autograd"><img src="https://camo.githubusercontent.com/c1f238e8b7f06ef1f6e4497101e246e113518949/68747470733a2f2f7472617669732d63692e6f72672f484950532f6175746f677261642e7376673f6272616e63683d6d6173746572" alt="Test status" data-canonical-src="https://travis-ci.org/HIPS/autograd.svg?branch=master"/></a></h1>

<p>Autograd can automatically differentiate native Python and Numpy code. It can
handle a large subset of Python's features, including loops, ifs, recursion and
closures, and it can even take derivatives of derivatives of derivatives. It
uses reverse-mode differentiation (a.k.a. backpropagation), which means it can
efficiently take gradients of scalar-valued functions with respect to
array-valued arguments. The main intended application is gradient-based
optimization. For more information, check out the <a href="/HIPS/autograd/blob/master/docs/tutorial.md">tutorial</a>
and the <a href="/HIPS/autograd/blob/master/examples">examples directory</a>.</p>

<p>Example use:</p>

<div class="highlight highlight-source-python"><pre><span class="pl-k">&gt;&gt;</span><span class="pl-k">&gt;</span> <span class="pl-k">import</span> autograd.numpy <span class="pl-k">as</span> np  <span class="pl-c"># Thinly-wrapped numpy</span>
<span class="pl-k">&gt;&gt;</span><span class="pl-k">&gt;</span> <span class="pl-k">from</span> autograd <span class="pl-k">import</span> grad    <span class="pl-c"># The only autograd function you may ever need</span>
<span class="pl-k">&gt;&gt;</span><span class="pl-k">&gt;</span>
<span class="pl-k">&gt;&gt;</span><span class="pl-k">&gt;</span> <span class="pl-k">def</span> <span class="pl-en">tanh</span>(<span class="pl-smi">x</span>):                 <span class="pl-c"># Define a function</span>
<span class="pl-c1">...</span>     y <span class="pl-k">=</span> np.exp(<span class="pl-k">-</span>x)
<span class="pl-c1">...</span>     <span class="pl-k">return</span> (<span class="pl-c1">1.0</span> <span class="pl-k">-</span> y) <span class="pl-k">/</span> (<span class="pl-c1">1.0</span> <span class="pl-k">+</span> y)
<span class="pl-c1">...</span>
<span class="pl-k">&gt;&gt;</span><span class="pl-k">&gt;</span> grad_tanh <span class="pl-k">=</span> grad(tanh)       <span class="pl-c"># Obtain its gradient function</span>
<span class="pl-k">&gt;&gt;</span><span class="pl-k">&gt;</span> grad_tanh(<span class="pl-c1">1.0</span>)               <span class="pl-c"># Evaluate the gradient at x = 1.0</span>
<span class="pl-c1">0.39322386648296376</span>
<span class="pl-k">&gt;&gt;</span><span class="pl-k">&gt;</span> (tanh(<span class="pl-c1">1.0001</span>) <span class="pl-k">-</span> tanh(<span class="pl-c1">0.9999</span>)) <span class="pl-k">/</span> <span class="pl-c1">0.0002</span>  <span class="pl-c"># Compare to finite differences</span>
<span class="pl-c1">0.39322386636453377</span></pre></div>

<p>We can continue to differentiate as many times as we like:</p>

<div class="highlight highlight-source-python"><pre><span class="pl-k">&gt;&gt;</span><span class="pl-k">&gt;</span> <span class="pl-k">def</span> <span class="pl-en">elementwise_grad</span>(<span class="pl-smi">fun</span>):                   <span class="pl-c"># A wrapper for broadcasting</span>
<span class="pl-c1">...</span>     <span class="pl-k">return</span> grad(<span class="pl-k">lambda</span> <span class="pl-smi">x</span>: np.sum(fun(x)))    <span class="pl-c"># (closures are no problem)</span>
<span class="pl-c1">...</span>
<span class="pl-k">&gt;&gt;</span><span class="pl-k">&gt;</span> grad_tanh   <span class="pl-k">=</span> elementwise_grad(tanh)
<span class="pl-k">&gt;&gt;</span><span class="pl-k">&gt;</span> grad_tanh_2 <span class="pl-k">=</span> elementwise_grad(grad_tanh)    <span class="pl-c"># 2nd derivative</span>
<span class="pl-k">&gt;&gt;</span><span class="pl-k">&gt;</span> grad_tanh_3 <span class="pl-k">=</span> elementwise_grad(grad_tanh_2)  <span class="pl-c"># 3rd derivative</span>
<span class="pl-k">&gt;&gt;</span><span class="pl-k">&gt;</span> grad_tanh_4 <span class="pl-k">=</span> elementwise_grad(grad_tanh_3)  <span class="pl-c"># etc.</span>
<span class="pl-k">&gt;&gt;</span><span class="pl-k">&gt;</span> grad_tanh_5 <span class="pl-k">=</span> elementwise_grad(grad_tanh_4)
<span class="pl-k">&gt;&gt;</span><span class="pl-k">&gt;</span> grad_tanh_6 <span class="pl-k">=</span> elementwise_grad(grad_tanh_5)
<span class="pl-k">&gt;&gt;</span><span class="pl-k">&gt;</span>
<span class="pl-k">&gt;&gt;</span><span class="pl-k">&gt;</span> <span class="pl-k">import</span> matplotlib.pyplot <span class="pl-k">as</span> plt
<span class="pl-k">&gt;&gt;</span><span class="pl-k">&gt;</span> x <span class="pl-k">=</span> np.linspace(<span class="pl-k">-</span><span class="pl-c1">7</span>, <span class="pl-c1">7</span>, <span class="pl-c1">200</span>)
<span class="pl-k">&gt;&gt;</span><span class="pl-k">&gt;</span> plt.plot(x, tanh(x),
<span class="pl-c1">...</span>          x, grad_tanh(x),
<span class="pl-c1">...</span>          x, grad_tanh_2(x),
<span class="pl-c1">...</span>          x, grad_tanh_3(x),
<span class="pl-c1">...</span>          x, grad_tanh_4(x),
<span class="pl-c1">...</span>          x, grad_tanh_5(x),
<span class="pl-c1">...</span>          x, grad_tanh_6(x))
<span class="pl-k">&gt;&gt;</span><span class="pl-k">&gt;</span> plt.show()</pre></div>

<p><a href="/HIPS/autograd/blob/master/examples/tanh.png" target="_blank"><img src="/HIPS/autograd/raw/master/examples/tanh.png"/></a></p>

<p>See the <a href="/HIPS/autograd/blob/master/examples/tanh.py">tanh example file</a> for the code.</p>

<h2><a id="user-content-documentation" class="anchor" href="#documentation" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Documentation</h2>

<p>You can find a tutorial <a href="/HIPS/autograd/blob/master/docs/tutorial.md">here.</a></p>

<h2><a id="user-content-end-to-end-examples" class="anchor" href="#end-to-end-examples" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>End-to-end examples</h2>



<p><a href="/HIPS/autograd/blob/master/examples/fluidsim/animated.gif" target="_blank"><img src="/HIPS/autograd/raw/master/examples/fluidsim/animated.gif"/></a></p>



<h2><a id="user-content-how-to-install" class="anchor" href="#how-to-install" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>How to install</h2>

<p>Just run <code>pip install autograd</code></p>

<h2><a id="user-content-authors" class="anchor" href="#authors" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Authors</h2>

<p>Autograd was written by <a href="http://users.physics.harvard.edu/%7Emaclaurin/">Dougal Maclaurin</a>,
<a href="http://mlg.eng.cam.ac.uk/duvenaud/">David Duvenaud</a>
and <a href="http://www.mit.edu/%7Emattjj/">Matt Johnson</a>,
and we're actively
developing it. Please feel free to submit any bugs or feature requests.
We'd also love to hear about your experiences with autograd in general.
Drop us an email!</p>

<p>We want to thank Jasper Snoek and the rest of the HIPS group (led by Prof. Ryan
P. Adams) for helpful contributions and advice; Barak Pearlmutter for
foundational work on automatic differentiation and for guidance on our
implementation; and Analog Devices Inc. (Lyric Labs) and Samsung Advanced Institute
of Technology for their generous support.</p>
</article>
  </div></body></html>