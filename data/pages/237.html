<html><body><div><div class="content html_format">
      <h4>Введение</h4><p>
Добрый день, уважаемые читатели.</p><p>
В прошлых статьях, на практических примерах, мной были показаны способы решения задач классификации (</p><a href="http://habrahabr.ru/post/204500/">задача кредитного скоринга</a><p>) и основ анализа текстовой информации (</p><a href="http://habrahabr.ru/post/205360/">задача о паспортах</a><p>). Сегодня же мне бы хотелось коснуться другого класса задач, а именно </p><a href="http://www.machinelearning.ru/wiki/index.php?title=%D0%A0%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D1%8F">восстановления регрессии</a><p>. Задачи данного класса, как правило, используются при </p><a href="http://www.machinelearning.ru/wiki/index.php?title=%D0%9F%D1%80%D0%BE%D0%B3%D0%BD%D0%BE%D0%B7%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5">прогнозировании</a><p>.</p><p>
Для примера решения задачи прогнозирования, я взял набор данных </p><a href="http://archive.ics.uci.edu/ml/datasets/Energy+efficiency">Energy efficiency</a><p> из крупнейшего репозитория </p><a href="http://www.machinelearning.ru/wiki/index.php?title=UCI">UCI</a><p>. В качестве инструментов по традиции будем использовать Python c аналитическими пакетами </p><a href="http://pandas.pydata.org/pandas-docs/stable/">pandas</a><p> и </p><a href="http://scikit-learn.org/stable/">scikit-learn</a><p>.
</p><a name="habracut"/>
<h4>Описание набора данных и постановка задачи</h4><p>
Дан </p><a href="http://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx">набор данных</a><p>, котором описаны следующие атрибуты помещения:
</p><table>
<tr>
<th>Поле</th>
<th>Описание</th>
<th>Тип</th>
</tr>
<tr>
<th>X1</th>
<td>Относительная компактность</td>
<td>FLOAT</td>
</tr>
<tr>
<th>X2</th>
<td>Площадь</td>
<td>FLOAT</td>
</tr>
<tr>
<th>X3</th>
<td>Площадь стены</td>
<td>FLOAT</td>
</tr>
<tr>
<th>X4</th>
<td>Площадь потолка</td>
<td>FLOAT</td>
</tr>
<tr>
<th>X5</th>
<td>Общая высота</td>
<td>FLOAT</td>
</tr>
<tr>
<th>X6</th>
<td>Ориентация</td>
<td>INT</td>
</tr>
<tr>
<th>X7</th>
<td>Площадь остекления</td>
<td>FLOAT</td>
</tr>
<tr>
<th>X8</th>
<td>Распределенная площадь остекления</td>
<td>INT</td>
</tr>
<tr>
<th>y1</th>
<td>Нагрузка при обогреве</td>
<td>FLOAT</td>
</tr>
<tr>
<th>y2</th>
<td>Нагрузка при охлаждении</td>
<td>FLOAT</td>
</tr>
</table><p>
В нем </p><img src="https://habrastorage.org/getpro/habr/post_images/9fc/b2d/c08/9fcb2dc08ba54b120d59e0393a6af2f1.png" title="LaTeX:X1 ... X8"/><p> — характеристики помещения на основании которых будет проводиться анализ, а </p><img src="https://habrastorage.org/getpro/habr/post_images/61e/ffd/b17/61effdb1703ab84e602bf8766c0106e3.png" title="LaTeX:y1,y2"/><p> — значения нагрузки, которые надо спрогнозировать.

</p><h4>Предварительный анализ данных</h4><p>
Для начала загрузим наши данные и посмотрим на них:

</p><pre><code class="python">from pandas import read_csv, DataFrame
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score
from sklearn.cross_validation import train_test_split

dataset = read_csv('EnergyEfficiency/ENB2012_data.csv',';')
dataset.head()
</code></pre>
<table>
<tr>
<th/>
<th>X1</th>
<th>X2</th>
<th>X3</th>
<th>X4</th>
<th>X5</th>
<th>X6</th>
<th>X7</th>
<th>X8</th>
<th>Y1</th>
<th>Y2</th>
</tr>
<tr>
<th>0</th>
<td> 0.98</td>
<td> 514.5</td>
<td> 294.0</td>
<td> 110.25</td>
<td> 7</td>
<td> 2</td>
<td> 0</td>
<td> 0</td>
<td> 15.55</td>
<td> 21.33</td>
</tr>
<tr>
<th>1</th>
<td> 0.98</td>
<td> 514.5</td>
<td> 294.0</td>
<td> 110.25</td>
<td> 7</td>
<td> 3</td>
<td> 0</td>
<td> 0</td>
<td> 15.55</td>
<td> 21.33</td>
</tr>
<tr>
<th>2</th>
<td> 0.98</td>
<td> 514.5</td>
<td> 294.0</td>
<td> 110.25</td>
<td> 7</td>
<td> 4</td>
<td> 0</td>
<td> 0</td>
<td> 15.55</td>
<td> 21.33</td>
</tr>
<tr>
<th>3</th>
<td> 0.98</td>
<td> 514.5</td>
<td> 294.0</td>
<td> 110.25</td>
<td> 7</td>
<td> 5</td>
<td> 0</td>
<td> 0</td>
<td> 15.55</td>
<td> 21.33</td>
</tr>
<tr>
<th>4</th>
<td> 0.90</td>
<td> 563.5</td>
<td> 318.5</td>
<td> 122.50</td>
<td> 7</td>
<td> 2</td>
<td> 0</td>
<td> 0</td>
<td> 20.84</td>
<td> 28.28</td>
</tr>
</table><p>
Теперь давайте посмотрим не связаны ли между собой какие-либо атрибуты. Сделать это можно рассчитав коэффициенты корреляции для всех столбцов. Как это сделать было описано в предыдущей </p><a href="http://habrahabr.ru/post/204500/">статье</a><p>:

</p><pre><code class="python">dataset.corr()
</code></pre>
<table>
<tr>
<th/>
<th>X1</th>
<th>X2</th>
<th>X3</th>
<th>X4</th>
<th>X5</th>
<th>X6</th>
<th>X7</th>
<th>X8</th>
<th>Y1</th>
<th>Y2</th>
</tr>
<tr>
<th>X1</th>
<td> 1.000000e+00</td>
<td>-9.919015e-01</td>
<td>-2.037817e-01</td>
<td>-8.688234e-01</td>
<td> 8.277473e-01</td>
<td> 0.000000</td>
<td> 1.283986e-17</td>
<td> 1.764620e-17</td>
<td> 0.622272</td>
<td> 0.634339</td>
</tr>
<tr>
<th>X2</th>
<td>-9.919015e-01</td>
<td> 1.000000e+00</td>
<td> 1.955016e-01</td>
<td> 8.807195e-01</td>
<td>-8.581477e-01</td>
<td> 0.000000</td>
<td> 1.318356e-16</td>
<td>-3.558613e-16</td>
<td>-0.658120</td>
<td>-0.672999</td>
</tr>
<tr>
<th>X3</th>
<td>-2.037817e-01</td>
<td> 1.955016e-01</td>
<td> 1.000000e+00</td>
<td>-2.923165e-01</td>
<td> 2.809757e-01</td>
<td> 0.000000</td>
<td>-7.969726e-19</td>
<td> 0.000000e+00</td>
<td> 0.455671</td>
<td> 0.427117</td>
</tr>
<tr>
<th>X4</th>
<td>-8.688234e-01</td>
<td> 8.807195e-01</td>
<td>-2.923165e-01</td>
<td> 1.000000e+00</td>
<td>-9.725122e-01</td>
<td> 0.000000</td>
<td>-1.381805e-16</td>
<td>-1.079129e-16</td>
<td>-0.861828</td>
<td>-0.862547</td>
</tr>
<tr>
<th>X5</th>
<td> 8.277473e-01</td>
<td>-8.581477e-01</td>
<td> 2.809757e-01</td>
<td>-9.725122e-01</td>
<td> 1.000000e+00</td>
<td> 0.000000</td>
<td> 1.861418e-18</td>
<td> 0.000000e+00</td>
<td> 0.889431</td>
<td> 0.895785</td>
</tr>
<tr>
<th>X6</th>
<td> 0.000000e+00</td>
<td> 0.000000e+00</td>
<td> 0.000000e+00</td>
<td> 0.000000e+00</td>
<td> 0.000000e+00</td>
<td> 1.000000</td>
<td> 0.000000e+00</td>
<td> 0.000000e+00</td>
<td>-0.002587</td>
<td> 0.014290</td>
</tr>
<tr>
<th>X7</th>
<td> 1.283986e-17</td>
<td> 1.318356e-16</td>
<td>-7.969726e-19</td>
<td>-1.381805e-16</td>
<td> 1.861418e-18</td>
<td> 0.000000</td>
<td> 1.000000e+00</td>
<td> 2.129642e-01</td>
<td> 0.269841</td>
<td> 0.207505</td>
</tr>
<tr>
<th>X8</th>
<td> 1.764620e-17</td>
<td>-3.558613e-16</td>
<td> 0.000000e+00</td>
<td>-1.079129e-16</td>
<td> 0.000000e+00</td>
<td> 0.000000</td>
<td> 2.129642e-01</td>
<td> 1.000000e+00</td>
<td> 0.087368</td>
<td> 0.050525</td>
</tr>
<tr>
<th>Y1</th>
<td> 6.222722e-01</td>
<td>-6.581202e-01</td>
<td> 4.556712e-01</td>
<td>-8.618283e-01</td>
<td> 8.894307e-01</td>
<td>-0.002587</td>
<td> 2.698410e-01</td>
<td> 8.736759e-02</td>
<td> 1.000000</td>
<td> 0.975862</td>
</tr>
<tr>
<th>Y2</th>
<td> 6.343391e-01</td>
<td>-6.729989e-01</td>
<td> 4.271170e-01</td>
<td>-8.625466e-01</td>
<td> 8.957852e-01</td>
<td> 0.014290</td>
<td> 2.075050e-01</td>
<td> 5.052512e-02</td>
<td> 0.975862</td>
<td> 1.000000</td>
</tr>
</table><p>
Как можно заметить из нашей матрицы, коррелируют между собой следующие столбы (Значение коэффициента корреляции больше 95%):
</p><ul>
<li>y1 --&gt; y2</li>
<li>x1 --&gt; x2</li>
<li>x4 --&gt; x5</li>
</ul><p>
Теперь давайте выберем, какие столбцы их наших пар мы можем убрать из нашей выборки. Для этого, в каждой паре, выберем столбцы, которые в большей степени оказывают влияние на прогнозные значения </p><i>Y1</i><p> и </p><i>Y2</i><p> и оставим их, а остальные удалим.</p><p>
Как можно заметить и матрицы с коэффициентами корреляции на </p><i><b>y1</b></i><p>,</p><i><b>y2</b></i><p> больше значения оказывают </p><b><i>X2</i></b><p> и </p><b><i>X5</i></b><p>, нежели X1 и X4, таким образом мы можем последние столбцы мы можем удалить.

</p><pre><code class="python">dataset = dataset.drop(['X1','X4'], axis=1)
dataset.head()
</code></pre><p>
Помимо этого, можно заметить, что поля </p><i><b>Y1</b></i><p> и </p><i><b>Y2</b></i><p> очень тесно коррелируют между собой. Но, т. к. нам надо спрогнозировать оба значения мы их оставляем «как есть».

</p><h4>Выбор модели</h4><p>
Отделим от нашей выборки прогнозные значения:

</p><pre><code class="python">trg = dataset[['Y1','Y2']]
trn = dataset.drop(['Y1','Y2'], axis=1)
</code></pre><p>
После обработки данных можно перейти к построению модели. Для построения модели будем использовать следующие методы:
</p><p>
Теорию о данным методам можно почитать в </p><a href="http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5_(%D0%BA%D1%83%D1%80%D1%81_%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D0%B9%2C_%D0%9A.%D0%92.%D0%92%D0%BE%D1%80%D0%BE%D0%BD%D1%86%D0%BE%D0%B2)">курсе лекций К.В.Воронцова по машинному обучению</a><p>.</p><p>
Оценку будем производить с помощью </p><a href="http://ru.wikipedia.org/wiki/%CA%EE%FD%F4%F4%E8%F6%E8%E5%ED%F2_%E4%E5%F2%E5%F0%EC%E8%ED%E0%F6%E8%E8">коэффициента детерминации</a><p> (</p><i>R-квадрат</i><p>). Данный коэффициент определяется следующим образом:

</p><img src="https://habrastorage.org/getpro/habr/post_images/18a/6ec/f5f/18a6ecf5f82456d28c0f84e6e89fb661.png" title="LaTeX:R^2 = 1 - \frac{V(y|x)}{V(y)} = 1 - \frac{\sigma^2}{\sigma_y^2}"/>
<p>
, где </p><img src="https://habrastorage.org/getpro/habr/post_images/9b9/735/84d/9b973584d4d32f949149aad7ca99885a.png" alt="image"/><p> — условная дисперсия зависимой величины </p><i>у</i><p> по фактору </p><i>х</i><p>.</p><p>
Коэффициент принимает значение на промежутке </p><img src="https://habrastorage.org/getpro/habr/post_images/8b6/206/f36/8b6206f36960f49e50a8f3135524b7c1.png" title="LaTeX:[0,1]"/><p> и чем он ближе к 1 тем сильнее зависимость.</p><p>
Ну что же теперь можно перейти непосредственно к построению модели и выбору модели. Давайте поместим все наши модели в один список для удобства дальнейшего анализа:

</p><pre><code class="python">	models = [LinearRegression(), # метод наименьших квадратов
	          RandomForestRegressor(n_estimators=100, max_features ='sqrt'), # случайный лес
	          KNeighborsRegressor(n_neighbors=6), # метод ближайших соседей
	          SVR(kernel='linear'), # метод опорных векторов с линейным ядром
	          LogisticRegression() # логистическая регрессия
	          ]
</code></pre><p>
Итак модели готовы, теперь мы разобьем наши исходные данные на 2 подвыборки: </p><i>тестовую</i><p> и </p><i>обучающую</i><p>. Кто читал мои предыдущие статьи знает, что сделать это можно с помощью функции train_test_split() из пакета scikit-learn:

</p><pre><code class="python">Xtrn, Xtest, Ytrn, Ytest = train_test_split(trn, trg, test_size=0.4)
</code></pre><p>
Теперь, т. к. нам надо спрогнозировать 2 параметра </p><img src="https://habrastorage.org/getpro/habr/post_images/61e/ffd/b17/61effdb1703ab84e602bf8766c0106e3.png" title="LaTeX:y1,y2"/><p>, надо построить регрессию для каждого из них. Кроме этого, для дальнейшего анализа, можно записать полученные результаты во временный </p><i>DataFrame</i><p>. Сделать это можно так:

</p><pre><code class="python">	#создаем временные структуры
	TestModels = DataFrame()
	tmp = {}
	#для каждой модели из списка
	for model in models:
	    #получаем имя модели
	    m = str(model)
	    tmp['Model'] = m[:m.index('(')]    
	    #для каждого столбцам результирующего набора
	    for i in  xrange(Ytrn.shape[1]):
	        #обучаем модель
	        model.fit(Xtrn, Ytrn[:,i]) 
	        #вычисляем коэффициент детерминации
	        tmp['R2_Y%s'%str(i+1)] = r2_score(Ytest[:,0], model.predict(Xtest))
	    #записываем данные и итоговый DataFrame
	    TestModels = TestModels.append([tmp])
	#делаем индекс по названию модели
	TestModels.set_index('Model', inplace=True)
</code></pre><p>
Как можно заметить из кода выше, для расчета коэффициента </p><img src="https://habrastorage.org/getpro/habr/post_images/879/d5c/f0f/879d5cf0fa87faddaefd28fc0906ad27.png" title="LaTeX:R^2"/><p> используется функция r2_score().</p><p>
Итак, данные для анализа получены. Давайте теперь построим графики и посмотрим какая модель показала лучший результат:

</p><pre><code class="python">fig, axes = plt.subplots(ncols=2, figsize=(10,4))
TestModels.R2_Y1.plot(ax=axes[0], kind='bar', title='R2_Y1')
TestModels.R2_Y2.plot(ax=axes[1], kind='bar', color='green', title='R2_Y2')
</code></pre>
<img src="https://habrastorage.org/getpro/habr/post_images/7e7/a6e/0cc/7e7a6e0cc7c2b999338cb464ba68b073.png" alt="image"/>

<h4>Анализ результатов и выводы</h4><p>
Из графиков, приведенных выше, можно сделать вывод, что лучше других с задачей справился метод </p><i>RandomForest</i><p> (случайный лес). Его коэффициенты детерминации выше остальных по обоим переменным: </p><img src="https://habrastorage.org/getpro/habr/post_images/e4f/8b7/cab/e4f8b7cab696a182562ce84e204086c8.png" title="LaTeX:R_{y1}^2 \approx 99\%,\ R_{y2}^2 \approx 90\%"/><p>
ля дальнейшего анализа давайте заново обучим нашу модель:

</p><pre><code class="python">model = models[1]
model.fit(Xtrn, Ytrn)
</code></pre><p>
При внимательном рассмотрении, может возникнуть вопрос, почему в предыдущий раз и делили зависимую выборку </p><i>Ytrn</i><p> на переменные(по столбцам), а теперь мы этого не делаем.</p><p>
Дело в том, что некоторые методы, такие как </p><i>RandomForestRegressor</i><p>, может работать с несколькими прогнозируемыми переменными, а другие (например </p><i>SVR</i><p>) могут работать только с одной переменной. Поэтому на при предыдущем обучении мы использовали разбиение по столбцам, чтобы избежать ошибки в процессе построения некоторых моделей.</p><p>
Выбрать модель это, конечно же, хорошо, но еще неплохо бы обладать информацией, как каждый фактор влиет на прогнозное значение. Для этого у модели есть свойство </p><i>feature_importances_</i><p>.</p><p>
С помощью него, можно посмотреть вес каждого фактора в итоговой моделей:

</p><pre><code class="python">model.feature_importances_
</code></pre>
<i>array([ 0.40717901, 0.11394948, 0.34984766, 0.00751686, 0.09158358,<br/>
 0.02992342])</i>
<p>
В нашем случае видно, что больше всего на нагрузку при обогреве и охлаждении влияют общая высота и площадь. Их общий вклад в прогнозной модели около 72%.</p><p>
Также необходимо отметить, что по вышеуказанной схеме можно посмотреть влияние каждого фактора отдельно на обогрев и отдельно на охлаждение, но т. к. эти факторы у нас очень тесно коррелируют между собой (</p><img src="https://habrastorage.org/getpro/habr/post_images/7d7/523/3aa/7d75233aa72e5427657c48ced3588fc0.png" title="LaTeX:r\ =\ 97\%"/><p>), мы сделали общий вывод по ним обоим который и был написан выше.

</p><h4>Заключение</h4><p>
В статье я постарался показать основные этапы при регрессионном анализе данных с помощью Python и аналитческих пакетов </p><b>pandas</b><p> и </p><b>scikit-learn</b><p>.</p><p>
Необходимо отметить, что набор данных специально выбирался таким образом чтобы быть максимально формализованым и первичная обработка входных данных была бы минимальна. На мой взгляд статья будет полезна тем, кто только начинает свой путь в анализе данных, а также тем кто имеет хорошую теоретическую базу, но выбирает инструментарий для работы.

      
      </p><p class="clear"/>
    </div>

    
  </div></body></html>