<html><body><div><article class="markdown-body entry-content" itemprop="text"><h2><a id="user-content-statistical-dependency-parsing-using-svm" class="anchor" href="#statistical-dependency-parsing-using-svm" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Statistical Dependency Parsing using SVM</h2>

<h3><a id="user-content-abstract" class="anchor" href="#abstract" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Abstract</h3>

<p>In this project, I studied and re-implemented the technique for dependency parsing proposed by Yamada, Hiroyasu, and Yuji Matsumoto in "Statistical dependency analysis with support vector machines.". In addition to recreating the results we also experimented with the biomedical data from the GENIA biomedical corpus(Yuka et al., 2005) and the Spanish universal dependency dataset(McDonald et al.,2013) to understand out of domain implications.</p>

<h4><a id="user-content-report-presentation-and-data" class="anchor" href="#report-presentation-and-data" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Report, Presentation and data</h4>

<p>PDF: <a href="http://bit.ly/nlp-cs6741">http://bit.ly/nlp-cs6741</a><br/>
Presentation+Data+Models: <a href="http://bit.ly/nlp-cs6741-full">http://bit.ly/nlp-cs6741-full</a><br/>
For any questions email at: rohitjain [dot] nsit [at] gmail [dot] com</p>

<h4><a id="user-content-instructions-to-run-the-code" class="anchor" href="#instructions-to-run-the-code" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Instructions to run the code:</h4>

<h5><a id="user-content-folder-structure" class="anchor" href="#folder-structure" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Folder Structure:</h5>

<ol>
<li>Converter - All files to pre-process data. Convert data sets to the parser formatting and also do the tagging.</li>
<li>Parser - All the files required to do the parsing</li>
<li>Data - Folder with all the pre-processed data, Download from the <a href="https://drive.google.com/folderview?id=0B27E_jXWuNWdRFJMZ3ZYWFFwYTQ&amp;usp=sharing">link</a></li>
<li>Models - SVM models that have already been trained, download from <a href="https://drive.google.com/folderview?id=0B27E_jXWuNWdNWk3V19RbHQ4QU0&amp;usp=sharing">link</a></li>
</ol>

<h5><a id="user-content-sections" class="anchor" href="#sections" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Sections:</h5>

<ol>
<li>Pre-requisites</li>
<li>Running the code</li>
<li>Pre-processing and data format</li>
<li>Data Structures</li>
</ol>

<h5><a id="user-content-section-0-pre-requisites" class="anchor" href="#section-0-pre-requisites" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Section 0: Pre-requisites</h5>

<p>The following packages need to be installed:
    Python numpy, scipy, scikit-learn.
    If you are running an ubuntu machine on aws, run the following commands.<br/><br/>
    <code>sudo apt-get update</code><br/><br/>
    <code>sudo apt-get install -y python-numpy python-scipy python-matplotlib ipython ipython-notebook python-pandas python-sympy python-nose</code><br/><br/>
    <code>sudo apt-get install -y python-pip</code><br/><br/>
    <code>sudo pip install -U scikit-learn</code></p>

<p>These packages are sufficient to run the parser with the pre-processed data in the data folder. If you wish to pre-process your own datastanford pos tagger and genia tagger need to be installed.
    Please refer to section 2</p>

<h5><a id="user-content-section-1-running-the-code" class="anchor" href="#section-1-running-the-code" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Section 1: Running the code</h5>

<p>Make sure you are in the root directory i.e. dp
Assuming the pre-processing has been done these are the instructions to run the code using the existing models. To read more about pre-processing look at section 2.<br/></p>

<p><code>python parser/__init__.py -c &lt;cache size&gt; -i &lt;input dataset&gt; -t &lt;tagger&gt;</code></p>

<p><code>cache size</code>: The cache size for SVM. significantly affects performance.</p>

<p><code>input dataset</code>: The input dataset, can take the following 3 values<br/>
    1. ptb - this will load the penn treebank dataset<br/>
    2. genia - this will load the penn treebank corpus<br/>
    3. spanish - this will load the spanish dependency corpus</p>

<p><code>tagger</code>: the tagger to be used for test. the test files have been pre-processed to save time so this is mainly to load the tagged test files.
    it can take the following values:
    1. stanford - loads the stanford tagged test files
    2. gdep - loads the files pre-processed with the <a href="https://github.com/saffsd/geniatagger">genia tagger</a>, <a href="https://pypi.python.org/pypi/geniatagger-python/0.1">python wrapper</a></p>

<p>Eg. If you want to run penn treebank with stanford tagger for test sentences the command would be: <code>python parser/__init__.py -c 5120 -i ptb -t stanford</code></p>

<p>Similarly for the genia treebank and geniatagger
<code>python parser/__init__.py -c 5120 -i genia -t gdep</code></p>

<p>The models are stored in a folder under the "models" directory within the folder dataset_tagger. eg. the model for each post tage for penn tree bank with stanford tagger can be found under ptb_stanford.</p>

<h6><a id="user-content-note-1" class="anchor" href="#note-1" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Note 1:</h6>

<p>The parser looks for individual pos tag models under the model directory and only trains again if they are not found. Usuall training from scratch would take upto 10 hours for PTB sized dataset.</p>

<h6><a id="user-content-note-2" class="anchor" href="#note-2" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Note 2:</h6>

<p>For spanish only stanford tagger can be used because there are no pretrained models for spanish with geniatagger</p>

<h6><a id="user-content-note-3" class="anchor" href="#note-3" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Note 3:</h6>

<p>The code assumes file names for the pre-processed file names. If you wish to change these then you need to change the variable names in the main function of parser/<strong>init</strong>.py file.
The variables on lines 170-184 are commented and can be changed as per the need.</p>

<h5><a id="user-content-section-2-pre-processing" class="anchor" href="#section-2-pre-processing" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Section 2: Pre-processing</h5>

<p>All the pre-processing scripts are inside the converter folder. The pre-processing is different for each dataset and the goal is to get the training and testing files in the following format:</p>

<h6><a id="user-content-train-file-format" class="anchor" href="#train-file-format" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Train file format:</h6>

<pre><code>&lt;Token&gt; &lt;Gold POS Tag&gt; &lt;Head node's position/dependency index&gt;
Eg. 
It  PRP 1
was VBD -1
supposed    VBN 1
to  TO  4
be  VB  2
a   DT  8
routine JJ  8
courtesy    NN  8
call    NN  4
.   .   1
</code></pre>

<h6><a id="user-content-test-file-format" class="anchor" href="#test-file-format" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Test file format:</h6>

<pre><code>&lt;Token&gt; &lt;Assigned Tag&gt;  &lt;Gold POS Tag&gt; &lt;Head node's position/dependency index&gt;
Eg.
You PRP PRP 1
know    VBP VBP -1
what    WP  WP  1
the DT  DT  4
law NN  NN  7
of  IN  IN  4
averages    NNS NNS 5
is  VBZ VBZ 2
,   ,   ,   1
do  VBP VBP 1
n't RB  RB  9
you PRP PRP 9
?   .   .   1
</code></pre>

<h5><a id="user-content-21-pre-processing-penn-treebank" class="anchor" href="#21-pre-processing-penn-treebank" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>2.1 Pre-processing penn treebank</h5>

<p>PTB by default doesn't provide dependency trees. Original authors of the paper provided a tool(folder ptbconv.old) to convert to dependency tools based on collins head rules. the script convert.sh can be used to do that.</p>

<p><code>wsj_tagger.py</code>:
    This is used to produce the pre-processed file for test data for ptb. It can use either stanford tagger or genia tagger based on the flags specified in the main function of the script. </p>



<p>For stanford tagger the environment variables CLASSPATH and STANFORD_MODELS are also required. sample values can be found in environment_variables file.</p>

<h5><a id="user-content-22-pre-processing-genia-corpus" class="anchor" href="#22-pre-processing-genia-corpus" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>2.2 Pre-Processing genia corpus:</h5>

<p>Training data:
    The code for this is available in <code>genia_converter.py</code>. It converts genia dependency tree to parser format using stanford dependency parser. A copy of stanford dependency parser is in the converter folder.</p>

<p>Testing data:
    The code for this is available in <code>genia_test_tagger.py</code>. It can use either geniatagger or stanford tagger using the flag values in main function. Similar to wsj as a pre-requisite both need to be installed and environment variables need to be initialised.</p>

<h5><a id="user-content-23-pre-processing-spanish-universal-dependency-corpus" class="anchor" href="#23-pre-processing-spanish-universal-dependency-corpus" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>2.3 Pre-Processing spanish universal dependency corpus:</h5>

<p>The code for this is available in <code>universal_dependency_converter.py</code>. The file contains methods for producing both train and test tags. This is easliy extensible to other languages, only caveat is that the model for stanford pos tagger needs to change.</p>

<p>Spanish POS tags produced by the tagger comply with EAGLES standard but are simplified further ( Question 6: <a href="http://nlp.stanford.edu/software/help/spanish-faq.shtml">http://nlp.stanford.edu/software/help/spanish-faq.shtml</a>). There is no straight forward way of mapping them to universal tagset. General mappings for universal tagset can be found <a href="https://github.com/slavpetrov/universal-pos-tags/blob/master/es-eagles.map">here</a>.</p>

<p>For the purpose of the experiment I ave done the mapping manually by inspecting both files and the mapping is available in spanish_mapping file.</p>

<h5><a id="user-content-section-3-data-structures" class="anchor" href="#section-3-data-structures" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Section 3: Data structures</h5>

<p>2 primary data structure have been used. <br/><br/>
1. Node: code is available in node.py. This is the node of the parse tree, can contain any number of left and right children.<br/>
2. Sentence: Data structure used to read and store the training and test sentences. Allows realing labeled/unlabeled sentences.</p>
</article>
  </div></body></html>