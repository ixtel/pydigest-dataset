<html><body><div><div class="text_cell_render border-box-sizing rendered_html">
<p>Next, we have to generate an error function so that we have a way to train our model. For logistic regression, we could use the negative log-likelihood.</p>
<p>In neural networks, the classic way of generating an error model is called <strong>binary cross-entropy</strong>, and since we're treating this regression as just a neural network without a hidden layer, let's go ahead and use it. If our prediction is $\hat y$ and the real value is $y$, then binary cross-entropy is defined as</p><p>
$$b(y, \hat y) = -y \log(\hat y) - (1 - y)\log(1 - \hat y)$$</p><p>Since $y$ is either zero or one, this is really applying a logarithmic penalty to $\hat y$ according to whatever $y$ should be.</p>
<p>When $y$ and $\hat y$ are vectors, then apply binary cross-entropy element-wise and take the mean of the components to get the total cost.</p>

</div>
</div></body></html>