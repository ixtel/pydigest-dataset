<html><body><div><div class="post-text" itemprop="text">
<p>I fixed it! Thanks for all the suggestions. I worked out numeric partials and found that my o and deltas were correct, but I was multiplying the wrong ones. That's why I now take numpy.outer(d[layer+1], o[layer]) instead of numpy.outer(d[layer], o[layer+1]).</p>

<p>I was also skipping the update on one layer. That's why I changed <code>for layer in range(self.hidden_layers)</code> to <code>for layer in range(self.hidden_layers+1)</code>. </p>

<p>I'll add that I caught a bug just before posting originally. My output layer delta was incorrect because my net (intentionally) doesn't activate the final outputs, but my delta was computed as though it did. </p>

<p>Debugged primarily with a one hidden layer, one hidden unit net, then moved to a 2 input, 3 hidden layers of 2 neurons each, 2 output model.</p>

<pre><code>from __future__ import division
import numpy
import scipy
import scipy.special
import matplotlib
#from pylab import *

#numpy.random.seed(23)

def nmap(f, x):
    return numpy.array(map(f, x))

class Net:
    def __init__(self, *sizes):
        self.hidden_layers = len(sizes)-2
        self.weights = [numpy.random.uniform(-1, 1, (sizes[i+1],sizes[i])) for i in range(self.hidden_layers+1)]

    @staticmethod
    def activate(x):
        return scipy.special.expit(x)
        #return 1/(1+numpy.exp(-x))

    @staticmethod
    def activate_(x):
        s = scipy.special.expit(x)
        return s*(1-s)

    def y(self, x):
        o = [numpy.array(x)] #o[i] is the (activated) output of hidden layer i, "hidden layer 0" is inputs and not activated
        for weight in self.weights[:-1]:
            o.append(Net.activate(weight.dot(o[-1])))
        o.append(self.weights[-1].dot(o[-1]))
#        for weight in self.weights:
#            o.append(Net.activate(weight.dot(o[-1])))
        return o

    def __call__(self, x):
        return self.y(x)[-1]

    def delta(self, x, t):
        x = numpy.array(x)
        t = numpy.array(t)
        o = self.y(x)
        #delta = [(o[-1]-t) * o[-1] * (1-o[-1])]
        delta = [o[-1]-t]
        for i, weight in enumerate(reversed(self.weights)):
            delta.append(weight.T.dot(delta[-1]) * o[-i-2] * (1-o[-i-2]))
        delta.reverse() #surely i need this
        return o, delta

    def train(self, inputs, outputs, epochs=1000, rate=.1):
        errors = []
        for epoch in range(epochs):
            for x, t in zip(inputs, outputs): #shuffle? subset?
                o, d = self.delta(x, t)
                for layer in range(self.hidden_layers+1):
                    grad = numpy.outer(d[layer+1], o[layer])
                    self.weights[layer] -=  rate * grad

        return errors

    def rmse(self, inputs, outputs):
        return ((outputs - nmap(self, inputs))**2).sum()**.5/len(inputs)



n = Net(1, 8, 1)
X = numpy.linspace(0, 2*3.1415, 10)
T = numpy.sin(X)
Y = map(n, X)
Y = numpy.array([y[0,0] for y in Y])
matplotlib.pyplot.plot(X, T, 'g')
matplotlib.pyplot.plot(X, Y, 'r')
print 'output successful'
print n.rmse(X, T)
errors = n.train(X, T)
print 'tried to train successfully'
print n.rmse(X, T)
Y = map(n, X)
Y = numpy.array([y[0,0] for y in Y])
matplotlib.pyplot.plot(x, Y, 'b')
matplotlib.pyplot.show()
</code></pre>
    </div>
    </div></body></html>