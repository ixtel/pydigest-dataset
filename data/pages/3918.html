<html><body><div><div class="section">
              <h1>Theano 0.8.0rc1</h1>

              


<p>Optimizing compiler for evaluating mathematical expressions on CPUs and GPUs.</p><p>








Theano is a Python library that allows you to define, optimize, and efficiently evaluate mathematical expressions involving multi-dimensional arrays. It is built on top of NumPy_. Theano features:</p><p> * **tight integration with NumPy:** a similar interface to NumPy's. numpy.ndarrays are also used internally in Theano-compiled functions.</p><p> * **transparent use of a GPU:** perform data-intensive computations up to 140x faster than on a CPU (support for float32 only).</p><p> * **efficient symbolic differentiation:** Theano can compute derivatives for functions of one or many inputs.</p><p> * **speed and stability optimizations:** avoid nasty bugs when computing expressions such as log(1 + exp(x)) for large values of x.</p><p> * **dynamic C code generation:** evaluate expressions faster.</p><p> * **extensive unit-testing and self-verification:** includes tools for detecting and diagnosing bugs and/or potential problems.</p><p>Theano has been powering large-scale computationally intensive scientific</p><p>research since 2007, but it is also approachable enough to be used in the</p><p>classroom (IFT6266 at the University of Montreal).</p><p>.. _NumPy: http://numpy.scipy.org/</p><p>.. _NEWS:</p><p>=============</p><p>Release Notes</p><p>=============</p><p>Theano 0.8 rc1 (29th of February, 2016)</p><p>=======================================</p><p>We recommend that everybody update to this version.</p><p>Highlights:</p><p>- Python 2 and 3 support with the same code base</p><p>- Faster optimization</p><p>- New GPU back-end</p><p>   - Float16 new back-end (need cuda 7.5)</p><p>   - Multi dtypes</p><p>   - Multi-GPU support in the same process</p><p>- Integration of CuDNN for better GPU performance</p><p>- Many Scan improvements (execution speed up, ...)</p><p>- optimizer=fast_compile moves computation to the GPU.</p><p>- Better convolution on CPU and GPU. (CorrMM, cudnn, 3d conv, more parameter)</p><p>- Interactive visualization of graphs with d3viz</p><p>- cnmem (better memory management on GPU)</p><p>- BreakpointOp</p><p>- Multi-GPU for data parallism via Platoon (https://github.com/mila-udem/platoon/)</p><p>A total of 135 people contributed to this release, see the list at the bottom.</p><p>Installation:</p><p>- Better blas detection</p><p>- Fixes for more recent software and OS versions</p><p>- Support Anaconda on Windows</p><p>Bug fixes:</p><p>- GpuJoin now supports negative axis</p><p>- Fix GpuCumsum for negative axis</p><p>Interface Deprecation (a warning is printed):</p><p>- Deprecate Param class, use In instead</p><p>Interface Changes:</p><p>- Rename DownsampleFactorMax to Pool.</p><p>- tensor.stack now uses the same interface as numpy.stack</p><p>- optimizer=fast_compile moves computation to the GPU</p><p>- Raise the user stack trace more frequently.</p><p>- Change dev version numbering to follow the PEP 440</p><p>New Interface (reuses existing functionality):</p><p>- theano.tensor.nnet.relu</p><p>- theano.tensor.nnet.elu</p><p>- BatchNormalization.</p><p>- MaxAndArgmax support axis=None</p><p>- Add theano.tensor.compress (equivalent of numpy.compress)</p><p>- theano.tensor.signal.downsamples.max_pool_2d_same_size</p><p>- COp</p><p>- __props__</p><p>New features</p><p>- tensor.unique</p><p>- map_variables</p><p>- erfcx</p><p>- mgrid, ogrid</p><p>- allclose</p><p>- BreakpointOp</p><p>- Make bincount work on GPU</p><p>- SolveOp on GPU</p><p>- Optional optimization remove_all_assert</p><p>- AllocEmpty</p><p>- LogSoftmax, for stability optimization when the crossentropy optimization does not apply.</p><p>- theano.tensor.repeat works on GPU</p><p>- BatchedDot on the GPU and faster on the CPU.</p><p>- Faster batched_tensordot and make it work on GPU.</p><p>- SoftmaxGrad grad</p><p>- 3d conv via CorrMM on the GPU</p><p>- CPU Max Pool support of padding and strides!=windows size</p><p>- theano.function() now accepts a dict for the outputs. When doing this, the function will return a dict. Helpful to keep track of which output is what.</p><p>- Warn for unknown or misspelled theano config variables</p><p>- theano.tensor.tile update (accept symbolic reps, work on GPU)</p><p>- scan how have a strict flag. If set to True, this make scan building faster and could make execution faster.</p><p>- theano.tensor.signal.conv2d(2d,2d) output 2d answer</p><p>Speed-ups:</p><p>- Faster SetSubtensor on the GPU.</p><p>- Support more reduction pattern on the GPU.</p><p>- More graph optimization</p><p>- Faster graph optimization</p><p>- GpuCrossentropySoftmaxArgmax1HotWithBias</p><p>Crash/no return fixes:</p><p>- Fix crash in the assert op grad</p><p>- Fix curand crash on Mac</p><p>- Multiple Fix scan crashes</p><p>- Finish to update all Op.grad() implementation to the new interface</p><p>Others:</p><p>- Support ARM processor.</p><p>- Better tests</p><p>- Code clean up.</p><p>- Doc updates</p><p>- doctest and sphinx test in travis</p><p>- More tests tagged as slow</p><p>- Better same_shape implementation</p><p>- More op with c code to lower overhead</p><p>- Custom pickler for SharedVariable theano.misc.pkl_utils.{dump,load}</p><p>- function_dump to help us reproduce user error during compilation</p><p>- assert_no_cpu_op</p><p>- pep8, flake8</p><p>- Better error messages</p><p>- On non-default modes, reduce the number of allocation when allow_gc=False</p><p>- Better lock</p><p>Committers for this dev version only:</p><p>- Frederic Bastien</p><p>- Arnaud Bergeron</p><p>- Pierre Luc Carrier</p><p>- Iban Harlouchet</p><p>- Pascal Lamblin</p><p>- Chienli Ma</p><p>- David Warde-Farley</p><p>- Amjad Almahairi</p><p>- Tim Cooijmans</p><p>- Christof Angermueller</p><p>- Nicolas Ballas</p><p>- Ziye Fan</p><p>- Caglar</p><p>- Sina Honari</p><p>- Roy Xue</p><p>- hantek</p><p>- Mohammad Pezeshki</p><p>- Melanie Ducoffe</p><p>- Alexandre de Brebisson</p><p>- Harm de Vries</p><p>- Alex Lamb</p><p>- Ramana.S</p><p>- Saizheng Zhang</p><p>- Francesco Visin</p><p>- Ying Zhang</p><p>- Bart van Merrienboer</p><p>- Cesar Laurent</p><p>- Jan Schlüter</p><p>- Xavier Bouthillier</p><p>- Iulian Vlad Serban</p><p>- Samira Shabanian</p><p>- Sigurd Spieckermann</p><p>- Dmitrii Serdiuk</p><p>- Kelvin Xu</p><p>- Li Yao</p><p>- Sebastien Jean</p><p>- Thomas Mesnard</p><p>- Seon-Wook Park</p><p>- Dustin Webb</p><p>- Mikhail Korobov</p><p>- orhanf</p><p>- Daniel Renshaw</p><p>- Julien Rebetez</p><p>- Sean Lee</p><p>- TimSalimans</p><p>- Andre Holzner</p><p>- Gijs van Tulder</p><p>- Guillaume Alain</p><p>- Julien Demouth</p><p>- Markus Beissinger</p><p>- Mehdi Mirza</p><p>- Moslem Kazemi</p><p>- Saxenauts</p><p>- skaae</p><p>- Anatoly Belikov</p><p>- Diogo Moitinho de Almeida</p><p>- Kashif Rasul</p><p>- Laurent Dinh</p><p>- Rémy Léone</p><p>- gw0 [http://gw.tnode.com/]</p><p>- mronian</p><p>- vesis84</p><p>- Benni</p><p>- JesseLivezey</p><p>- Marius Killinger</p><p>- Matt Graham</p><p>- Matthew Willson</p><p>- Piotr Frankowski</p><p>- Stefan Krastanov</p><p>- vdumoulin</p><p>- Luke Metz</p><p>- Anish Shah</p><p>- Balázs Hidasi</p><p>- Colin Raffel</p><p>- Cory Lorenz</p><p>- Doug</p><p>- Jesse Livezey</p><p>- John Salvatier</p><p>- John Zedlewski</p><p>- Jonathan Ho</p><p>- Kaixhin</p><p>- Liang-Chi Hsieh</p><p>- Lucas Beyer</p><p>- Marc-Alexandre Cote</p><p>- Martin Arjovsky</p><p>- Matthias Kümmerer</p><p>- Sirisha Rambhatla</p><p>- Vincent Michalski</p><p>- briancheung</p><p>- ivdorelian</p><p>- jan-matthis</p><p>- jojolalpin</p><p>- joncrall</p><p>- peterjsadowski</p><p>- scottsievert</p><p>- Étienne Simon</p><p>- A. Flaxman</p><p>- AlOa</p><p>- Albert Zeyer</p><p>- Andrea</p><p>- Andy Jiang</p><p>- Balázs</p><p>- Ben Poole</p><p>- Brian Cheung</p><p>- Christophe Van Gysel</p><p>- Claude Coulombe</p><p>- Clay McLeod</p><p>- Dario Garcia</p><p>- Jakob Lombacher</p><p>- Jakub Sygnowski</p><p>- Joao Felipe Santos</p><p>- John Arevalo</p><p>- Jonas Degrave</p><p>- Martin Thoma</p><p>- Mathieu Germain</p><p>- Matthew Koichi Grimes</p><p>- Michael Eickenberg</p><p>- Michael Opitz</p><p>- Paul Hollensen</p><p>- Prayag Verma</p><p>- Saatvik Shah</p><p>- Sergei Lebedev</p><p>- Vik Kamath</p><p>- Wei Ouyang</p><p>- Yi-Lin Juang</p><p>- Yurii Shevchuk</p><p>- Zach Dwiel</p><p>- dan</p><p>- eulerreich</p><p>- jotterbach</p><p>- rolf</p><p>- wuaalb

</p><a name="downloads"> </a>


<ul class="nodot">
  <li><strong>Downloads (All Versions):</strong></li>
  <li>
    <span>0</span> downloads in the last day
  </li>
  <li>
    <span>2435</span> downloads in the last week
  </li>
  <li>
    <span>21987</span> downloads in the last month
  </li>
</ul>



<ul class="nodot">
 <li>
  <strong>Author:</strong>
  <span>LISA laboratory, University of Montreal</span>
 </li>

 

 


 <li>
  <strong>Home Page:</strong>
  
  <a href="http://deeplearning.net/software/theano/">http://deeplearning.net/software/theano/</a>
 </li>


 

 


 <li>
  <strong>Keywords:</strong>
  <span>theano math numerical symbolic blas numpy gpu autodiff differentiation</span>
 </li>

 <li>
  <strong>License:</strong>
  
  
  <span>BSD</span>
  
 </li>

 <li>
  <strong>Platform:</strong>
  <span>Windows,Linux,Solaris,Mac OS-X,Unix</span>
 </li>


 
 
 

 <li>
  <strong>Categories</strong>
  
 </li>



 

 

 

 

 


 <li>
  <strong>Package Index Owner:</strong>
  <span>abergeron, lamblin, nouiz, JamesBergstra, dwf</span>
 </li>

 

 <li>
  <strong><a href="http://usefulinc.com/doap">DOAP</a> record:</strong>
  <a href="/pypi?:action=doap&amp;name=Theano&amp;version=0.8.0rc1">Theano-0.8.0rc1.xml</a>
 </li>

</ul>





            </div>


          </div></body></html>