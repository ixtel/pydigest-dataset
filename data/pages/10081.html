<html><body><div><div class="entry-content">
		<p><span>In the </span><a href="http://blog.cambridgecoding.com/2016/01/25/implementing-your-own-spam-filter/"><span>previous post</span></a><span>, you learned how to implement your own spam filter. Wouldn’t it be nice to use this spam filter on your incoming emails? Let’s try it out!</span></p>
<p><b><i>Applying your spam filter to new emails</i></b><span id="more-611"/></p>
<p><span>You can use the spam filter as a stand-alone module and call it from other programs – similar to how you use external libraries (like </span><a href="http://www.nltk.org"><span>NLTK</span></a><span>). Start a new Python program and add the import statements, including your spam filter:</span></p>
<pre class="brush: python; first-line: 2; title: ; notranslate" title="">
import os, random
import nltk
from nltk import word_tokenize, WordNetLemmatizer from nltk.corpus
import stopwords
import spam_filter as sf
</pre>
<p><span>Let’s add a function </span><b>run_online </b><span>to your program:</span></p>
<pre class="brush: python; first-line: 8; title: ; notranslate" title="">
def run_online(classifier, setting):
    while True:
    features = sf.get_features(raw_input('Your new email: '), setting)
    if (len(features) == 0):
        break
    print (classifier.classify(features))
</pre>
<p><span>You can call this function from the main method as </span><b>run_online(classifier, </b><b>“”</b><b>)</b><span>. It will ask for a line of text and will classify it as spam or ham once you click “Enter”, and stop if you don’t type in anything.</span></p>
<p><span>The main part of the program will look like that:</span></p>
<pre class="brush: python; first-line: 54; title: ; notranslate" title="">
    spam = sf.init_lists('enron1/spam/')
    ham = sf.init_lists('enron1/ham/')
    all_emails = [(email, 'spam') for email in spam]
    all_emails += [(email, 'ham') for email in ham]
    random.shuffle(all_emails)
    print ('Corpus size = ' + str(len(all_emails)) + ' emails')

    all_features = [(sf.get_features(email, ''), label) for (email, label) in all_emails]
    train_set, test_set, classifier = sf.train(all_features, 1.0)

    #classify your new email
    run_online(classifier, "")
</pre>
<p><span>Note that you can use the functions you implemented in </span><b>spam_filter.py</b><span> by adding the reference </span><b>sf </b><span>in front of the function calls. You can now use all of the emails in </span><span><i><span>enron1</span></i></span><span> folder to train the classifier, thus </span><span><b>1.0 </b></span><span>as an argument for </span><b>sf.train().</b></p>
<p>Try out your favourite spam text lines. For example, the ones from the previous post:<img class=" size-full wp-image-658 aligncenter" src="https://cambridgecoding.files.wordpress.com/2016/02/1_single_line.jpg?w=610" alt="1_single_line.jpg"/></p>
<p><span>What the spam filter tells you here is that it is confident you haven’t unexpectedly become a millionaire, your account is in fact safe, and you still have to come to the meeting next Monday.</span></p>
<p><span>So far so good! But what if you want to use your spam filter on a whole bunch of new emails? Let’s add a function </span><b>detect_spam</b><span>:</span></p>
<pre class="brush: python; first-line: 15; title: ; notranslate" title="">
def detect_spam(folder, classifier, setting):
    output = {}
    file_list = os.listdir(folder)
    for a_file in file_list:
        f = open(folder + a_file, 'r')
        features = sf.get_features(f.read(), setting)
        f.close()
        output[a_file] = classifier.classify(features)
    for item in output.keys():
        print (item + '\t' + output.get(item))
</pre>
<p><span>Now you can save some of your own emails in a folder </span><i><span><span>my_emails</span> </span></i><span>and use your spam filter on those by calling the function from the main part of the program:</span></p>
<pre class="brush: python; first-line: 75; title: ; notranslate" title="">
    detect_spam("my_emails/", classifier, "")
</pre>
<p><span>Alternatively, download another folder – for example, </span><span><i><span>enron2</span></i></span><span> – from the </span><a href="http://labs-repos.iit.demokritos.gr/skel/i-config/downloads/enron-spam/preprocessed/"><span>Enron email dataset</span></a><span> and run your spam filter on the emails there:</span></p>
<pre class="brush: python; first-line: 75; title: ; notranslate" title="">
    detect_spam("enron2/ham/", classifier, "")
</pre>
<p><span>This will output: </span></p>
<p><img class=" wp-image-678 aligncenter" src="https://cambridgecoding.files.wordpress.com/2016/02/2_enron1_on_ham.jpg?w=404&amp;h=212" alt="2_enron1_on_ham.jpg"/></p>
<p><span>This output shows that the spam filter that worked well on </span><span><i><span>enron1</span></i></span><span> emails</span> <span>(remember, the accuracy was about 95% on both training and test sets!) misclassifies many ham emails in <span><em>enron2</em></span> as spam…</span><span> What is going on here?</span></p>
<p><span><b><i><span>Take-home message 1</span>:</i></b></span></p>
<p><img class="aligncenter wp-image-690" src="https://cambridgecoding.files.wordpress.com/2016/02/message1.jpg?w=546&amp;h=134" alt="message1.jpg"/></p>
<p><span>The reason your spam filter performs differently on </span><span><i><span>enron2 </span></i></span><span>is that you’ve used the emails in one dataset (e.g., </span><span><i><span>enron1</span></i></span><span>) to train the classifier and it learned to distinguish between spam and ham in this dataset. Even though you’ve split the data into non-overlapping training and test sets, chances are your classifier still </span><i><span>overfits</span></i><span> – it learns some inherent properties of the dataset </span><span><i><span>enron1 </span></i></span><span>that do not generalise and carry over to any other datasets.</span></p>
<p><span>Let’s find out how the classifier trained on </span><span><i><span>enron1</span></i></span><span> emails performs on </span><span><i><span>enron2</span></i></span><span> emails. Add a function </span><b>print_stat</b><span>:</span></p>
<pre class="brush: python; first-line: 26; title: ; notranslate" title="">
def print_stat(folder, classifier, setting):
    total = 0
    spam = 0
    ham = 0
    file_list = os.listdir(folder)
    for a_file in file_list:
        total+=1
        f = open(folder + a_file, 'r')
        features = sf.get_features(f.read(), setting)
        f.close()
        if classifier.classify(features)=='spam':
            spam+=1
        else:
            ham+=1
    print('%.2f' % (100*float(spam)/float(total)) + '% spam emails')
    print('%.2f' % (100*float(ham)/float(total)) + '% ham emails')
</pre>
<p><span>Call it from the main method as:</span></p>
<pre class="brush: python; first-line: 77; title: ; notranslate" title="">
    print('\nHAM:')
    print_stat('enron2/ham/', classifier, "")
    print('SPAM:')
    print_stat('enron2/spam/', classifier, "")
</pre>
<p><span>This will output:</span></p>
<p><img class=" wp-image-716 aligncenter" src="https://cambridgecoding.files.wordpress.com/2016/02/3_enron1_to_enron2.jpg?w=356&amp;h=110" alt="3_enron1_to_enron2.jpg"/></p>
<p><span>This shows that the classifier trained on </span><span><i><span>enron1</span></i></span><span> emails is quite accurate in identifying spam emails in </span><i><span><span>enron2</span>, </span></i><span>but makes many mistakes when applied to ham emails. In a way, it shows that one man’s ham is another man’s spam. Let’s look more closely into the classifier’s decision making process.</span></p>
<p><span><b><i><span>Take-home message 2</span>:</i></b></span></p>
<p><img class="aligncenter wp-image-720" src="https://cambridgecoding.files.wordpress.com/2016/02/message2.jpg?w=546&amp;h=99" alt="message2.jpg"/></p>
<p><span>In the </span><a href="http://blog.cambridgecoding.com/2016/01/25/implementing-your-own-spam-filter/"><span>previous post</span></a><span>, you learned how to check what features the classifier finds most informative, or discriminative. Let’s check if the classifiers trained on different datasets agree on the set of informative features.</span></p>
<p><span>Here’s the list of the most informative features for </span><span><i><span>enron1</span></i></span><span> from the previous post:</span></p>
<p><img class=" wp-image-723 aligncenter" src="https://cambridgecoding.files.wordpress.com/2016/02/5_enron1_feats.jpg?w=427&amp;h=120" alt="5_enron1_feats.jpg"/></p>
<p><span>And now the one for </span><i><span><span>enron2</span>:</span></i></p>
<p><img class=" wp-image-725 aligncenter" src="https://cambridgecoding.files.wordpress.com/2016/02/6_enron2_feats.jpg?w=434&amp;h=138" alt="6_enron2_feats.jpg"/></p>
<p><span>You can see that in </span><span><i><span>enron2</span></i></span><span>, “</span><i><span>privacy</span></i><span>”, “</span><i><span>identity</span></i><span>” and “</span><i><span>2005</span></i><span>” are considered to be indicative of spam. If in </span><span><i><span>enron1 </span></i></span><span>these words occurred in ham emails, they would be misclassified as spam. This would explain the high number of ham emails from </span><span><i><span>enron1 </span></i></span><span>that are classified as spam by the spam filter trained on </span><i><span><span>enron2</span>.</span></i></p>
<p><a href="http://www.nltk.org/book/ch01.html"><span>NLTK</span></a><span> allows us to explore the word frequencies in texts and visualise them with FreqDist. Let’s add the following function to your code:</span></p>
<pre class="brush: python; first-line: 43; title: ; notranslate" title="">
def explore_feats(dataset):
    stoplist = stopwords.words('english')
    lemmatizer = WordNetLemmatizer()
    words = []
    for email in dataset:
        words += [lemmatizer.lemmatize(word.lower()) for word in
        word_tokenize(unicode(email, errors='ignore')) if not word.lower() in stoplist]
    fdist = nltk.FreqDist(words)
    fdist.plot(75, cumulative=True)
</pre>
<p><span>Applying this function to the spam emails in <em><span>enron1</span> </em>will produce the following graph:</span></p>
<p><img class=" wp-image-744 aligncenter" src="https://cambridgecoding.files.wordpress.com/2016/02/enron1_spam.jpg?w=546&amp;h=300" alt="enron1_spam.jpg"/></p>
<p>Apply this function to ham and spam emails in <em><span>enron1</span> </em>and <span><em>enron2 </em></span>and compare the set of <span>75</span> <span>most frequent words in the two datasets (you’ll find them on the x-axes of the graphs produced). Are these sets of words same or different?</span></p>
<p><span>For those of you who prefer word cloud-style visualisation instead, there’s a Python </span><a href="http://peekaboo-vision.blogspot.co.uk/2012/11/a-wordcloud-in-python.html"><span>library</span></a><span> that implements this. For the spam emails in <span><em>enron1</em></span>, it will produce:</span></p>
<p><img class="aligncenter wp-image-757" src="https://cambridgecoding.files.wordpress.com/2016/02/enron1_spam_wc1.jpg?w=546&amp;h=275" alt="enron1_spam_wc.jpg"/></p>
<p><span><b>Bonus exercise</b></span><span><span>:</span> The frequency distribution and the word cloud above use all of the words in the emails. Try extracting the top discriminative words for ham and spam emails and visualise their distribution only.</span></p>
<p><span><b><i>Take-home message 3: </i></b></span></p>
<p><img class="aligncenter wp-image-765" src="https://cambridgecoding.files.wordpress.com/2016/02/message3.png?w=546&amp;h=99" alt="message3.png"/></p>
<p><span>Now you know that there is content mismatch between the two sets of emails in </span><span><i><span>enron1 </span></i></span><span>and </span><span><i><span>enron2 </span></i></span><span>(or may as well be in your own mail inbox). The classifier trained on one of the sets may overfit and not generalise well to the emails in another. Let’s try to merge the data – add the following code to the main method:</span></p>
<pre class="brush: python; first-line: 0; title: ; notranslate" title="">
    spam = sf.init_lists('enron1/spam/')
    ham = sf.init_lists('enron1/ham/')
    all_emails = [(email, 'spam') for email in spam]
    all_emails += [(email, 'ham') for email in ham]

    spam2 = sf.init_lists('enron2/spam/')
    ham2 = sf.init_lists('enron2/ham/')
    all_emails += [(email, 'spam') for email in spam2]
    all_emails += [(email, 'ham') for email in ham2]

    random.shuffle(all_emails)
    print ('Corpus size = ' + str(len(all_emails)) + ' emails')

    all_features = [(sf.get_features(email, ''), label) for (email, label) in all_emails]
    train_set, test_set, classifier = sf.train(all_features, 0.8)

    sf.evaluate(train_set, test_set, classifier)
</pre>
<p><span>Now run your spam filter and you’ll see the improved results:</span></p>
<p><img class="alignnone size-full wp-image-774 aligncenter" src="https://cambridgecoding.files.wordpress.com/2016/02/7_comb_res.jpg?w=610" alt="7_comb_res.jpg"/></p>
<p> </p>
<p><span><b><i>Take-home message 4:</i></b></span></p>
<p><img class="aligncenter wp-image-781" src="https://cambridgecoding.files.wordpress.com/2016/02/message4.jpg?w=546&amp;h=111" alt="message4.jpg"/></p>
<p><span>The Naive Bayes algorithm assumes that each feature – word in an email – is independent of all the other words used in the same email. It’s easy to see that this is not quite true of language data since in reality words in a sentence are usually interdependent: e.g., if there is a word “click” there is a high chance the next one is “here”. It is always a good idea to try out different classification algorithms and find out which one is best for the problem you try to solve. </span></p>
<p><span>NLTK comes with a </span><a href="http://www.nltk.org/book/ch06.html"><span>number of different machine learning algorithms</span></a><span>, including, for example, a Decision Tree classifier overviewed in the </span><a href="http://blog.cambridgecoding.com/2016/01/03/getting-started-with-regression-and-decision-trees/"><span>previous posts</span></a><span>.</span></p>
<p><span><b>Bonus exercise</b></span><span>: Try applying a different classifier to the task and compare the results. </span></p>
<p> </p>
<p><strong>You can post your solutions for the bonus exercises and your results in the comments.</strong></p>
<p><b>Want to learn more?  Check out our two-day Data Science Bootcamp:</b></p>
<p><a href="https://cambridgecoding.com/datascience-bootcamp">https://cambridgecoding.com/datascience-bootcamp</a></p>
<p> </p>
<hr/>
<p> </p>
<p>The <strong>full script</strong>:</p>
<div>
<pre class="brush: python; title: ; notranslate" title="">
from __future__ import print_function, division
import os, random
import nltk
from nltk import word_tokenize, WordNetLemmatizer
from nltk.corpus import stopwords
import spam_filter as sf

def run_online(classifier, setting):
    while True:
    features = sf.get_features(raw_input('Your new email: '), setting)
    if (len(features) == 0):
        break
    print (classifier.classify(features))

def detect_spam(folder, classifier, setting):
    output = {}
    file_list = os.listdir(folder)
    for a_file in file_list:
        f = open(folder + a_file, 'r')
        features = sf.get_features(f.read(), setting)
        f.close()
        output[a_file] = classifier.classify(features)
    for item in output.keys():
        print (item + '\t' + output.get(item))

def print_stat(folder, classifier, setting):
    total = 0
    spam = 0
    ham = 0
    file_list = os.listdir(folder)
    for a_file in file_list:
        total+=1
        f = open(folder + a_file, 'r')
        features = sf.get_features(f.read(), setting)
        f.close()
        if classifier.classify(features) == 'spam':
            spam+=1
        else:
            ham+=1
    print('%.2f' % (100*float(spam)/float(total)) + '% spam emails')
    print('%.2f' % (100*float(ham)/float(total)) + '% ham emails')

def explore_feats(dataset):
    stoplist = stopwords.words('english')
    lemmatizer = WordNetLemmatizer()
    words = []
    for email in dataset:
        words += [lemmatizer.lemmatize(word.lower()) for word in
        word_tokenize(unicode(email, errors='ignore')) if not word.lower() in stoplist]
    fdist = nltk.FreqDist(words)
    fdist.plot(75, cumulative=True)

if __name__ == "__main__":
    spam = sf.init_lists('enron1/spam/')
    ham = sf.init_lists('enron1/ham/')
    all_emails = [(email, 'spam') for email in spam]
    all_emails += [(email, 'ham') for email in ham]

    spam2 = sf.init_lists('enron2/spam/')
    ham2 = sf.init_lists('enron2/ham/')
    all_emails += [(email, 'spam') for email in spam2]
    all_emails += [(email, 'ham') for email in ham2]

    random.shuffle(all_emails)
    print ('Corpus size = ' + str(len(all_emails)) + ' emails')

    all_features = [(sf.get_features(email, ''), label) for (email, label) in all_emails]
    train_set, test_set, classifier = sf.train(all_features, 0.8)

    sf.evaluate(train_set, test_set, classifier)

    #classify your new email
    run_online(classifier, "")

    detect_spam('enron2/ham/', classifier, "")

    print('\nHAM:')
    print_stat('enron2/ham/', classifier, "")
    print('SPAM:')
    print_stat('enron2/spam/', classifier, "")
    explore_feats(spam)

</pre>
<hr/>
<p><strong>ABOUT THE AUTHOR</strong></p>
<p><img class="alignleft wp-image-562 size-thumbnail" src="https://cambridgecoding.files.wordpress.com/2016/01/ekaterina.jpg?w=150&amp;h=150" alt=""/>EKATERINA KOCHMAR</p>
<p>Ekaterina is a research associate at the Computer Laboratory of the University of Cambridge. Her research focuses on Natural Language Processing (NLP) applications and Machine Learning methods. She holds a PhD in Natural Language and Information Processing and an MPhil in Advanced Computer Science from the University of Cambridge.</p>
</div>
<div id="jp-post-flair" class="sharedaddy sd-like-enabled sd-sharing-enabled"><div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" id="like-post-wrapper-104127362-611-56d5a78ce380a" data-src="//widgets.wp.com/likes/#blog_id=104127362&amp;post_id=611&amp;origin=cambridgecoding.wordpress.com&amp;obj_id=104127362-611-56d5a78ce380a" data-name="like-post-frame-104127362-611-56d5a78ce380a"><h3 class="sd-title">Like this:</h3><p class="likes-widget-placeholder post-likes-widget-placeholder"><span class="button"><span>Like</span></span> <span class="loading">Loading...</span></p><span class="sd-text-color"/><a class="sd-link-color"/></div>
<p id="jp-relatedposts" class="jp-relatedposts">
	<h3 class="jp-relatedposts-headline"><em>Related</em></h3>
</p></div>			</div>

	</div></body></html>