<html><body><div><div class="post-content">
<p>A few months ago, I got a chance to dive into some Python code that was performing slower than expected.</p>
<p><img src="//blog.hartleybrody.com/wp-content/uploads/2014/12/lightning-fast-serialization-python-150x150.png" alt="lightning-fast-serialization-python" class="alignright size-thumbnail wp-image-2407"/>The code in question was taking tiny bits of data off of a queue, translating some values from strings to primary keys, and then saving the data back to another queue for another worker to process.</p>
<p>The translation step should have been fast. We were loading the data into memory from a MySQL database during initialization, and had organized the data structure so that the id -&gt; string lookups were constant time.</p>
<h3>Finding the Problem</h3>
<p>In order to figure out where the bottleneck(s) were, I used Python’s <a href="http://stackoverflow.com/a/582337/625840" target="_blank">builtin CProfile package</a>, and combed through the results using the <a href="http://ymichael.com/2014/03/08/profiling-python-with-cprofile.html" target="_blank">awesome CProfileV package</a>, written by a former Quora intern.</p>
<p>After letting the script run for awhile, the bottleneck jumped out right away — the workers were spending about 40% of their time <a href="http://en.wikipedia.org/wiki/Serialization">serializing and deserializing data</a>.</p>
<p>In order to keep messages on the queue for other workers to pick up, we were translating the Python dicts into JSON objects using the standard library’s <code>json</code> package.</p>
<p>Our worker was reading the text data from the queue, deserializing it into a Python dict, changing a few values and then serializing it back into text data to save onto a new queue.</p>
<p>The translation steps were taking up about 40% of the total runtime.</p>
<p>So I set out to see if there was a faster way to serialize a Python dict.</p>
<p><span id="more-2406"/><br/>
</p><h3>Our Concerns</h3>
<p>When you’re optimizing code, it’s helpful to think about what sort of gains you’re looking for.</p>
<ul>
<li>Gaining a few percentage points faster isn’t usually too challenging</li>
<li>Gaining several times faster (ie 200-800%) requires more strategic thinking</li>
<li>Gaining orders of magnitude in speed often requires rearchitecture or starting over</li>
</ul>
<p>Since we were already using Python’s builtin <code>json</code> module, we knew it’d be hard to eek out an order of magnitude improvement. But a few percentage points wasn’t going to cut it. It had to be a meaningful speedup in order to take a big chunk out of that 40% of time spent doing serialization/deserialization.</p>
<p>Each message of data was small — 5 keys with small values tipped the scales at a few dozen bytes each — so we weren’t worried about saturating the network card. Bandwidth and latency also weren’t a huge factor since the queue and all the workers were in the same availability zone on EC2.</p>
<p>I should note that all of the workers that’d be touching this data were in-house, so interoperability with common data serialization standards wasn’t a huge concern.</p>
<p>If the fastest way to encode data was to string it together with pipes <code>|</code> and backslashes <code/>, that was fine. We could update all of the workers to accommodate it.</p>
<p>I tried searching for as many Python data serialization libraries as I could find — as well as coming up with my own serialization schemes.</p>
<h3>The Code</h3>
<p>I learn a bunch about the performance of different string building functions while building my own <code>home_brew</code>‘d serialization process. If you have any more ideas, let me know and I’ll be sure to add them!</p>
<p>Note that some packages require a file handle in order to write the serialized data, while others just dumped it to a string in-memory.</p>
<p>The overhead of opening and closing the file was undetectable on the order of time I was examining, but I commented the file-handling code out for the packages that didn’t need it, to simulate the actual cost of using each package in production.</p>
<p/>
<h3>Results &amp; Observations</h3>
<p>To get the times for each of the different serialization functions, I ran the script with the <a href="http://en.wikipedia.org/wiki/Time_(Unix)" target="_blank">unix <code>time</code> command</a> and summed <a href="http://stackoverflow.com/questions/556405/what-do-real-user-and-sys-mean-in-the-output-of-time1" target="_blank">the <code>user</code> and <code>system</code> time</a>.</p>
<p>I ran the script 10 times for each package and made a mental average. That’s the number you see listed in the comments next to each function.</p>
<p><img src="//blog.hartleybrody.com/wp-content/uploads/2014/12/avg-time-json-serilization-packages.png" alt="Average time per Python package" class="aligncenter size-full wp-image-2408"/></p>
<p><img src="//blog.hartleybrody.com/wp-content/uploads/2014/12/pyth-data-serialization-rate-speed.png" alt="Millions of messages serialized per second" class="aligncenter size-full wp-image-2409"/></p>
<p>If speed is your primary concern, I’d recommend checking out <a href="https://github.com/esnme/ultrajson" target="_blank">“Ultra JSON” aka <code>ujson</code></a> from <a href="http://pushingtheweb.com/2011/03/ultra-fast-json-encoding-decoding-python/" target="_blank">these fine folks</a>.</p>
<p>We switched to using ujson and saw a roughly 1/3rd overall increase in our pipeline processing speed, which was inline with our expectations from the test results.</p>
<p>Any packages I missed? Different ideas for home brewed serialization? Shoot me a <a href="https://twitter.com/hartleybrody">note on Twitter</a>.</p>
 
<p id="fb-root"/><comments href="https://blog.hartleybrody.com/python-serialize/" num_posts="5"/>
</div>
</div></body></html>