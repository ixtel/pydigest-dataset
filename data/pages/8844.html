<html><body><div><div id="readme" class="readme boxed-group clearfix announce instapaper_body txt">
    <h3>
      <svg aria-hidden="true" class="octicon octicon-book" role="img" version="1.1" viewbox="0 0 16 16"><path d="M2 5h4v1H2v-1z m0 3h4v-1H2v1z m0 2h4v-1H2v1z m11-5H9v1h4v-1z m0 2H9v1h4v-1z m0 2H9v1h4v-1z m2-6v9c0 0.55-0.45 1-1 1H8.5l-1 1-1-1H1c-0.55 0-1-0.45-1-1V3c0-0.55 0.45-1 1-1h5.5l1 1 1-1h5.5c0.55 0 1 0.45 1 1z m-8 0.5l-0.5-0.5H1v9h6V3.5z m7-0.5H8.5l-0.5 0.5v8.5h6V3z"/></svg>
      README.txt
    </h3>

      <div class="plain"><pre>You will need to install python and scipy to run this software.

On Debian or Ubuntu Linux, you can use this command to get both:

sudo apt-get install python-scipy

On other operating systems, try the Canopy Express Free version:

<a href="https://store.enthought.com/">https://store.enthought.com/</a>

If you are using Python 3, Python 3's 2to3 tool successfully converts
all of the existing code, and all 100+ unit tests run successfully.

See the Examples directory to get started.  All of the examples should
run by typing "python examplename.py" at a command prompt.  If your
copy of pyeq2 does not include the Examples directory, you can find
the examples at <a href="https://github.com/zunzun/pyeq2/tree/master/Examples">https://github.com/zunzun/pyeq2/tree/master/Examples</a>


Prior to the invention of electronic calculation, only manual methods
were available, of course - meaning that creating mathematical models
from experimental data was done by hand.  Even Napier's invention of
logarithms did not help much in reducing the tediousness of this task.
Linear regression techniques worked, but how to then compare models?
And so the F-statistic was created for the purpose of model selection,
since graphing models and their confidence intervals was practically
out of the question.  Forward and backward regression techniques used
linear methods, requiring less calculation than nonlinear methods, but
limited the possible mathematical models to linear combinations
of functions.

With the advent of computerized calculations, nonlinear methods which
were impractical in the past could be automated and made practical.
However, the nonlinear fitting methods all required starting points
for their solvers - meaning in practice you had to have a good idea of
the final equation parameters to begin with!

If however a genetic or monte carlo algorithm searched error space for
initial parameters prior to running the nonlinear solvers, this problem
could be strongly mitigated.  This meant that instead of hit-or-miss
forward and backward regression, large numbers of known linear *and*
nonlinear equations could be fitted to an experimental data set, and
then ranked by a fit statistic such as AIC or SSQ errors.

Note that for an initial guesstimate of parameter values, not all data
need be used.  A reduced size data set with min, max, and (hopefully)
evenly spaced additional data points in between are used.  The total
number of data points required is the number of equation parameters
plus a few extra points.

Reducing the data set size used by the code's genetic algorithm greatly
reduces total processing time.  I tested many different methods before
choosing the one in the code, a genetic algorithm named
"Differential Evolution".


I hope you find this code useful, and to that end I have sprinkled
explanatory comments throughout the code.  If you have any questions
or comments, please e-mail me directly at zunzun@zunzun.com.

James R. Phillips
2548 Vera Cruz Drive
Birmingham, AL 35235 USA

email: zunzun@zunzun.com
</pre></div>
  </div>


  </div></body></html>