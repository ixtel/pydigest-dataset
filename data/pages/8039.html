<html><body><div><div class="sqs-block-content"><p>Since we have 10 workers available in the thread pool, all 9 of the level C invocations will begin "computing" (i.e., sleeping) immediately, while the 10th slot remains unused since there's nothing for it to do. These will all complete after 1 second, at which point all of the B computations will initiate. Likewise, after 2 seconds, they will all complete and be passed along back up to the A layer (which is defined to take up 3 seconds of CPU time). Output (note the timestamps):</p><pre>[11416]2015-11-23 00:47:14,407: Running cpu-bound op on (0.7142857142857143,) for 1 seconds
[5676 ]2015-11-23 00:47:14,408: Running cpu-bound op on (1.25,) for 1 seconds
[13120]2015-11-23 00:47:14,408: Running cpu-bound op on (2.5,) for 1 seconds
[13624]2015-11-23 00:47:14,408: Running cpu-bound op on (1.0714285714285714,) for 1 seconds
[8740 ]2015-11-23 00:47:14,408: Running cpu-bound op on (1.875,) for 1 seconds
[14820]2015-11-23 00:47:14,408: Running cpu-bound op on (3.75,) for 1 seconds
[1820 ]2015-11-23 00:47:14,409: Running cpu-bound op on (0.625,) for 1 seconds
[11860]2015-11-23 00:47:14,409: Running cpu-bound op on (1.25,) for 1 seconds
[13124]2015-11-23 00:47:14,409: Running cpu-bound op on (0.35714285714285715,) for 1 seconds
[11416]2015-11-23 00:47:15,409: Running cpu-bound op on (2.5, 1.25, 0.7142857142857143) for 2 seconds
[13120]2015-11-23 00:47:15,409: Running cpu-bound op on (3.75, 1.875, 1.0714285714285714) for 2 seconds
[5676 ]2015-11-23 00:47:15,410: Running cpu-bound op on (1.25, 0.625, 0.35714285714285715) for 2 seconds
[5676 ]2015-11-23 00:47:17,411: Running cpu-bound op on (2.232142857142857, 4.464285714285714, 6.696428571428571) for 3 seconds
[13828]2015-11-23 00:47:20,412: Completed (13.392857142857142) in 6.005390644073486 seconds
</pre><p>As expected, total execution time is 6 seconds (1 second for all of level C, 2 seconds for level B, and 3 seconds for level A).</p><h1>Well that's neat</h1><p>The elegant thing about this solution is that all we really needed to know about the problem we want to solve is which parts of the pipeline are the computationally expensive bits we want to offload to a thread pool. Otherwise we can handle just about any computational pipeline with the same pattern <strong>—</strong> there might be 10 or 20 or 100 layers, or layer A might invoke layer B a hundred times per call, or we might have a case where A only calls B once and B only calls C once. In any case, all of our worker threads are always only being used for actual computations while active (they're never blocking on another thread), we're always using them as much as possible as defined by the pipeline, we're never instantiating more threads than needed, and the overall flow of the pipeline is always being handled by the main (event loop) thread.</p><p>Furthermore, we didn't really have to worry about how the main loop figures out which part of the pipeline is ready to go based off of which tasks have completed. For simplicity in this example we made each layer take up a pre-defined amount of time, which leads to a pretty predictable ordering of events, but in reality there might be much greater variation in execution time for each processing event <strong>—</strong> maybe C6, C1, and C7 finish first, in which case we do not yet have enough data to begin processing anything at layer B. But that's fine <strong>—</strong> the results from those computations will wait while the other threads continue to chug away, and once enough data has been computed to begin a layer B computation, it will get kicked off immediately (assuming the main event loop isn't busy doing anything else).</p><p>In fact perhaps the biggest downside to this architecture is that it allows you to shoot yourself in the foot by trying to maintain too many active portions of the pipeline. Say instead of passing single floats around, we're processing a large N dimensional numpy dataset. We could exhaust our memory resources by jumping around too much in the pipeline while maintaining state for all the different bits we've started processing on, and might have to instead restrict execution at some level. More on that later...</p><h1>Supplemental Material</h1><p><a href="http://dabeaz.com/coroutines/Coroutines.pdf">David Beazley's slides on coroutine</a><br/><a href="http://dabeaz.com/coroutines/">And his examples</a><br/><a href="https://www.python.org/dev/peps/pep-3156/">PEP-3156 <strong>—</strong> Asynchronous IO Support Rebooted: the "asyncio" Module</a><br/><a href="https://www.python.org/dev/peps/pep-0492/">PEP-0492 <strong>—</strong> Coroutines with async and await syntax</a><br/><a href="https://www.youtube.com/watch?v=aurOB4qYuFM">Guido on Tulip/asyncio</a></p></div></div></body></html>