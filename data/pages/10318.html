<html><body><div><div class="blog-post-article">
      <p><a href="https://spark-summit.org/east-2016/" target="_blank">Spark Summit East</a> is just around the corner. If you haven’t registered yet, you can get tickets <a href="http://www.prevalentdesignevents.com/sparksummit2016/east/registration.aspx?source=DBblog">here</a> and here’s a promo code for 20% off: <em>Databricks20</em>.</p>
<hr/>
<p>Data scientists often spend hours or days tuning models to get the highest accuracy. This tuning typically involves running a large number of independent Machine Learning (ML) tasks coded in Python or R. Following some work presented at Spark Summit Europe 2015, we are excited to release <a href="http://spark-packages.org/package/databricks/spark-sklearn" target="_blank">Scikit-learn integration package for Spark</a> that dramatically simplifies the life of data scientists using Python. This package automatically distributes the most repetitive tasks of model tuning on a Spark cluster, without impacting the workflow of data scientists:</p>
<ul>
<li>When used on a single machine, Spark can be used as a substitute to the default multithreading framework used by <a href="http://scikit-learn.org/">scikit-learn</a> (<a href="https://pythonhosted.org/joblib/">Joblib</a>).</li>
<li>If a need comes to spread the work across multiple machines, no change is required in the code between the single-machine case and the cluster case.</li>
</ul>
<p> </p>
<h2>Scale data science effortlessly</h2>
<p>Python is one of the most popular programming languages for data exploration and data science, and this is in no small part due to high quality libraries such as <a href="http://pandas.pydata.org/">Pandas</a> for data exploration or <a href="http://scikit-learn.org/">scikit-learn</a> for machine learning. Scikit-learn provides fast and robust implementations of standard ML algorithms such as clustering, classification, and regression.</p>
<p>Scikit-learn’s strength has typically been in the realm of computing on a single node, though. For some common scenarios, such as parameter tuning, a large number of small tasks can be run in parallel. These scenarios are perfect use cases for Spark.</p>
<p>We explored how to integrate Spark with scikit-learn, and the result is the Scikit-learn integration package for Spark. It combines the strengths of Spark and scikit-learn <em>with no changes to users’ code</em>. It re-implements some components of scikit-learn that benefit the most from distributed computing. Users will find a Spark-based cross-validator class that is fully compatible with scikit-learn’s cross-validation tools. By swapping out a single class import, users can distribute cross-validation for their existing scikit-learn workflows.</p>
<p> </p>
<h2>Distribute tuning of Random Forests</h2>
<p>Consider a classical example of identifying digits in images. Here are a few examples of images taken from the popular digits dataset, with their labels:</p>
<p><img class="aligncenter size-full wp-image-6313" src="https://databricks.com/wp-content/uploads/2016/02/image-taken-from-the-popular-digits-dataset.png" alt="A couple examples of images taken from the popular digits dataset."/></p>
<p>We are going to train a <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">random forest classifier</a> to recognize the digits. This classifier has a number of parameters to adjust, and there is no easy way to know which parameters work best, other than trying out many different combinations. Scikit-learn provides GridSearchCV, a search algorithm that explores many parameter settings automatically. <a href="http://scikit-learn.org/stable/modules/grid_search.html">GridSearchCV</a> uses selection by cross-validation, illustrated below. Each parameter setting produces one model, and the best-performing model is selected.</p>
<p><img class="aligncenter size-full wp-image-6332" src="https://databricks.com/wp-content/uploads/2016/02/scikit-learn-without-spark-training-diagram.png" alt="Diagram outlining how we are going to train the random forest classifier to recognize the digits."/></p>
<p>The <a href="http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#example-classification-plot-digits-classification-py">original code</a>, using only scikit-learn, is as follows:</p>
<pre class="brush: python; title: ; notranslate" title="">
from sklearn import grid_search, datasets
from sklearn.ensemble import RandomForestClassifier
from sklearn.grid_search import GridSearchCV
digits = datasets.load_digits()
X, y = digits.data, digits.target
param_grid = {"max_depth": [3, None],
              "max_features": [1, 3, 10],
              "min_samples_split": [1, 3, 10],
              "min_samples_leaf": [1, 3, 10],
              "bootstrap": [True, False],
              "criterion": ["gini", "entropy"],
              "n_estimators": [10, 20, 40, 80]}
gs = grid_search.GridSearchCV(RandomForestClassifier(), param_grid=param_grid)
gs.fit(X, y)
</pre>
<p>The dataset is small (in the hundreds of kilobytes), but exploring all the combinations takes about 5 minutes on a single core. The scikit-learn package for Spark provides an alternative implementation of the cross-validation algorithm that distributes the workload on a Spark cluster. Each node runs the training algorithm using a local copy of the scikit-learn library, and reports the best model back to the master:</p>
<p><img class="aligncenter size-full wp-image-6331" src="https://databricks.com/wp-content/uploads/2016/02/scikit-learn-with-spark-training-diagram.png" alt="Diagram showing Spark perform a distributed cross validation using spark-sklearn."/></p>
<p>The code is the same as before, except for a one-line change:</p>
<pre class="brush: python; title: ; notranslate" title="">
from sklearn import grid_search, datasets
from sklearn.ensemble import RandomForestClassifier
# Use spark_sklearn’s grid search instead:
from spark_sklearn import GridSearchCV
digits = datasets.load_digits()
X, y = digits.data, digits.target
param_grid = {"max_depth": [3, None],
              "max_features": [1, 3, 10],
              "min_samples_split": [1, 3, 10],
              "min_samples_leaf": [1, 3, 10],
              "bootstrap": [True, False],
              "criterion": ["gini", "entropy"],
              "n_estimators": [10, 20, 40, 80]}
gs = grid_search.GridSearchCV(RandomForestClassifier(), param_grid=param_grid)
gs.fit(X, y)
</pre>
<p>This example runs under 30 seconds on a 4-node cluster (which has 16 CPUs). For larger datasets and more parameter settings, the difference is even more dramatic.</p>
<p><img class="aligncenter size-full wp-image-6316" src="https://databricks.com/wp-content/uploads/2016/02/scikit-learn-results.png" alt="Results from processing variously-sized datasets using spark-sklearn."/></p>
<p> </p>
<h2>Get started</h2>
<p>If you would like to try out this package yourself, it is available as a <a href="http://spark-packages.org/package/databricks/spark-sklearn">Spark package</a> and as a <a href="https://pypi.python.org/pypi/spark-sklearn">PyPI library</a>. To get started, <a href="http://go.databricks.com/hubfs/notebooks/Samples/Miscellaneous/blog_post_cv.html" target="_blank">check out this example notebook on Databricks</a>.</p>
<p>In addition to distributing ML tasks in Python across a cluster, Scikit-learn integration package for Spark provides additional tools to export data from Spark to python and vice-versa. You can find methods to convert Spark DataFrames to Pandas dataframes and numpy arrays. More details can be found in this <a href="http://www.slideshare.net/databricks/spark-summit-europe-2015-combining-the-strengths-of-mllib-scikitlearn-and-r">Spark Summit Europe presentation</a> and in the <a href="http://pythonhosted.org/spark-sklearn/">API documentation</a>.</p>
<p>We welcome feedback and contributions to our open-source <a href="https://github.com/databricks/spark-sklearn">implementation on Github</a> (Apache 2.0 license).</p>
    </div>
  </div></body></html>