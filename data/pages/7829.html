<html><body><div><div class="post-text" itemprop="text">
<h1>Sample Code</h1>

<p>I rewrote your sample code a little bit to look into this issue. Here's where I landed, I'll use it in my answer below:</p>

<p><code>so.py</code>:</p>

<pre><code>from multiprocessing import sharedctypes as sct
import ctypes as ct
import numpy as np

n = 100000
l = np.random.randint(0, 10, size=n)


def sct_init():
    sh = sct.RawArray(ct.c_int, l)
    return sh

def sct_subscript():
    sh = sct.RawArray(ct.c_int, n)
    sh[:] = l
    return sh

def ct_init():
    sh = (ct.c_int * n)(*l)
    return sh

def ct_subscript():
    sh = (ct.c_int * n)(n)
    sh[:] = l
    return sh
</code></pre>

<p>Note that I added two test cases that do not use shared memory (and use regular a <code>ctypes</code> array instead).</p>

<p><code>timer.py</code>:</p>

<pre><code>import traceback
from timeit import timeit

for t in ["sct_init", "sct_subscript", "ct_init", "ct_subscript"]:
    print(t)
    try:
        print(timeit("{0}()".format(t), setup="from so import {0}".format(t), number=100))
    except Exception as e:
        print("Failed:", e)
        traceback.print_exc()
    print

print()

print ("Test",)
from so import *
sh1 = sct_init()
sh2 = sct_subscript()

for i in range(n):
    assert sh1[i] == sh2[i]
print("OK")
</code></pre>

<h1>Test results</h1>

<p>The results from running the above code using Python 3.6a0 (specifically <a href="https://github.com/python/cpython/commit/3c2fbdb" rel="nofollow"><code>3c2fbdb</code></a>) are:</p>

<pre><code>sct_init
2.844902500975877
sct_subscript
0.9383537038229406
ct_init
2.7903486443683505
ct_subscript
0.978101353161037

Test
OK
</code></pre>

<p>What's interesting is that <em>if you change <code>n</code></em>, the results scale linearly. For example, using <code>n = 100000</code> (10 times bigger), you get something that's pretty much 10 times slower:</p>

<pre><code>sct_init
30.57974253082648
sct_subscript
9.48625904135406
ct_init
30.509132395964116
ct_subscript
9.465419146697968

Test
OK
</code></pre>

<h1>Speed difference</h1>

<p>In the end, the speed difference lies in the hot loop that is called to initialize the array by copying every single value over from the Numpy array (<code>l</code>) to the new array (<code>sh</code>). This makes sense, because as we noted speed scales linearly with array size.</p>

<p>When you pass the Numpy array as a constructor argument, the function that does this is <a href="https://github.com/python/cpython/blob/6fd916862e1a93b1578d8eabdefc3979a4d4af62/Modules/_ctypes/_ctypes.c#L4213-L4232" rel="nofollow"><code>Array_init</code></a>. However, if you assign using <code>sh[:] = l</code>, then it's <a href="https://github.com/python/cpython/blob/6fd916862e1a93b1578d8eabdefc3979a4d4af62/Modules/_ctypes/_ctypes.c#L4398-L4453" rel="nofollow"><code>Array_ass_subscript</code> that does the job</a>.</p>

<p>Again, what matters here are the hot loops. Let's look at them.</p>

<p><code>Array_init</code> hot loop (slower):</p>

<pre><code>for (i = 0; i &lt; n; ++i) {
    PyObject *v;
    v = PyTuple_GET_ITEM(args, i);
    if (-1 == PySequence_SetItem((PyObject *)self, i, v))
        return -1;
}
</code></pre>

<p><code>Array_ass_subscript</code> hot loop (faster): </p>

<pre><code>for (cur = start, i = 0; i &lt; otherlen; cur += step, i++) {
    PyObject *item = PySequence_GetItem(value, i);
    int result;
    if (item == NULL)
        return -1;
    result = Array_ass_item(myself, cur, item);
    Py_DECREF(item);
    if (result == -1)
        return -1;
}
</code></pre>

<p>As it turns out, the majority of the speed difference lies in using <code>PySequence_SetItem</code> vs. <code>Array_ass_item</code>. </p>

<p>Indeed, if you change the code for <code>Array_init</code> to use <code>Array_ass_item</code> instead of <code>PySequence_SetItem</code> (<code>if (-1 == Array_ass_item((PyObject *)self, i, v))</code>), and recompile Python, the new results become:</p>

<pre><code>sct_init
11.504781467840075
sct_subscript
9.381130554247648
ct_init
11.625461496878415
ct_subscript
9.265848568174988

Test
OK
</code></pre>

<p>Still a bit slower, but not by much.</p>

<p>In other words, most of the overhead is caused by a slower hot loop, and mostly caused by <a href="https://github.com/python/cpython/blob/1364858e6ec7abfe04d92b7796ae8431eda87a7a/Objects/abstract.c#L1584-L1609" rel="nofollow">the code that <code>PySequence_SetItem</code> wraps around <code>Array_ass_item</code></a>.</p>

<p>This code might appear like little overhead at first read, but it really isn't. </p>

<p><code>PySequence_SetItem</code> actually calls into the entire Python machinery to resolve the <code>__setitem__</code> method and call it. </p>

<p>This <em>eventually</em> resolves in a call to <code>Array_ass_item</code>, but only after a large number of levels of indirection (which a direct call to <code>Array_ass_item</code> would bypass entirely!)</p>

<p>Going through the rabbit hole, the call sequence looks a bit like this:</p>

<ul>
<li><code>s-&gt;ob_type-&gt;tp_as_sequence-&gt;sq_ass_item</code> points to <a href="https://github.com/python/cpython/blob/cca9b8e3ff022d48eeb76d8567f297bc399fec3a/Objects/typeobject.c#L5790-L5803" rel="nofollow"><code>slot_sq_ass_item</code></a>.</li>
<li><code>slot_sq_ass_item</code> calls into <a href="https://github.com/python/cpython/blob/cca9b8e3ff022d48eeb76d8567f297bc399fec3a/Objects/typeobject.c#L1439-L1471" rel="nofollow"><code>call_method</code></a>.</li>
<li><code>call_method</code> calls into <a href="https://github.com/python/cpython/blob/1364858e6ec7abfe04d92b7796ae8431eda87a7a/Objects/abstract.c#L2149-L2175" rel="nofollow"><code>PyObject_Call</code></a></li>
<li>And on and on until we eventually get to <code>Array_ass_item</code>..!</li>
</ul>

<p>In other words, we have C code in <code>Array_init</code> that's calling Python code (<code>__setitem__</code>) in a hot loop. That's slow.</p>

<h2>Why ?</h2>

<p>Now, why does Python use <code>PySequence_SetItem</code> in <code>Array_init</code> and not <code>Array_ass_item</code> in <code>Array_init</code>? </p>

<p>That's because if it did, it would be bypassing the hooks that are exposed to the developer in Python-land.</p>

<p>Indeed, you <em>can</em> intercept calls to <code>sh[:] = ...</code> by subclassing the array and overriding <code>__setitem__</code> (<code>__setslice__</code> in Python 2). It will be called once, with a <code>slice</code> argument for the index.</p>

<p>Likewise, defining your own <code>__setitem__</code> also overrides the logic in the constructor. It will be called N times, with an integer argument for the index.</p>

<p>This means that if <code>Array_init</code> directly called into <code>Array_ass_item</code>, then you would lose something: <code>__setitem__</code> would no longer be called in the constructor, and you wouldn't be able to override the behavior anymore.</p>

<p>Now can we try to retain the faster speed all the while still exposing the same Python hooks? </p>

<p>Well, perhaps, using this code in <code>Array_init</code> instead of the existing hot loop:</p>

<pre><code> return PySequence_SetSlice((PyObject*)self, 0, PyTuple_GET_SIZE(args), args);
</code></pre>

<p>Using this will call into <code>__setitem__</code> <strong>once</strong> with a slice argument (on Python 2, it would call into <code>__setslice__</code>). We still go through the Python hooks, but we only do it once instead of N times.</p>

<p>Using this code, the performance becomes:</p>

<pre><code>sct_init
12.24651838419959
sct_subscript
10.984305887017399
ct_init
12.138383641839027
ct_subscript
11.79078131634742

Test
OK
</code></pre>

<h2>Other overhead</h2>

<p>I think the rest of the overhead may be due to the tuple instantiation that takes place <a href="https://github.com/python/cpython/blob/6fd916862e1a93b1578d8eabdefc3979a4d4af62/Lib/multiprocessing/sharedctypes.py#L66" rel="nofollow">when calling <code>__init__</code> on the array object</a> (note the <code>*</code>, and the fact that <code>Array_init</code> expects a tuple for <code>args</code>) â€” this presumably scales with <code>n</code> as well.</p>

<p>Indeed, if you replace <code>sh[:] = l</code> with <code>sh[:] = tuple(l)</code> in the test case, then the performance results become <em>almost</em> identical. With <code>n = 100000</code>:</p>

<pre><code>sct_init
11.538272527977824
sct_subscript
10.985187001060694
ct_init
11.485244687646627
ct_subscript
10.843198659364134

Test
OK
</code></pre>

<p>There's probably still something smaller going on, but ultimately we're comparing two substantially different hot loops. There's simply little reason to expect them to have identical performance.</p>

<p>I think it might be interesting to try calling <code>Array_ass_subscript</code> from <code>Array_init</code> for the hot loop and see the results, though!</p>

<h1>Baseline speed</h1>

<p>Now, to your second question, regarding allocating shared memory.</p>

<p>Note that there isn't really a cost to allocating <em>shared</em> memory. As noted in the results above, there isn't a substantial difference between using shared memory or not.</p>

<p>Looking at the Numpy code (<code>np.arange</code> is <a href="https://github.com/numpy/numpy/blob/eeba2cbfa4c56447e36aad6d97e323ecfbdade56/numpy/core/src/multiarray/multiarraymodule.c#L2912-L2930" rel="nofollow">implemented here</a>), we can finally understand why it's so much faster than <code>sct.RawArray</code>: <strong><code>np.arange</code> doesn't appear to make calls to Python "user-land"</strong> (i.e. no call to <code>PySequence_GetItem</code> or <code>PySequence_SetItem</code>).</p>

<p>That doesn't necessarily explain <em>all</em> the difference, but you'd probably want to start investigating there.</p>
    </div>
    </div></body></html>