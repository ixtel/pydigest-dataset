<html><body><div><section class="post-content">
            <p>Web scraping is one of those subjects that often appears in python discussions. There are many ways to do this, and there doesn't seem to be one best way. There are fully fledged frameworks like <a href="http://scrapy.org">scrapy</a> and more lightweight libraries like <a href="http://wwwsearch.sourceforge.net/mechanize/">mechanize</a>. Do-it-yourself solutions are also popular: one can go a long way by using <a href="http://python-requests.org/">requests</a> and <a href="http://www.crummy.com/software/BeautifulSoup/">beautifulsoup</a> or <a href="http://pythonhosted.org/pyquery/">pyquery</a>.</p>

<p>The reason for this diversity is that "scraping" actually covers multiple problems: you don't need to same tool to extract data from hundreds of pages and to automate some web workflow (like filling a few forms and getting some data back). I like the do-it-yourself approach because it's flexible, but it's not well-suited for massive data extraction, because requests does requests synchronously, and many requests means you have to wait a long time.</p>

<p>In this blog post, I'll present you an alternative to requests based on the new asyncio library : <a href="https://github.com/KeepSafe/aiohttp">aiohttp</a>. I use it to write small scraper that are really fast, and I'll show you how.</p>

<h2 id="basicsofasyncio">Basics of asyncio</h2>

<p><a href="http://docs.python.org/3.4/library/asyncio.html">asyncio</a> is the asynchronous IO library that was introduced in python 3.4. You can also get it from pypi on python 3.3. It's quite complex and I won't go too much into details. Instead, I'll explain what you need to know to write asynchronous code with it. If you want to know more about, I invite you to read its documentation.</p>

<p>To make it simple, there are two things you need to know about : coroutines and event loops. Coroutines are like functions, but they can be suspended or resumed at certain points in the code. This is used to pause a coroutine while it waits for an IO (an HTTP request, for example) and execute another one in the meantime. We use the <code>yield from</code> keyword to state that we want the return value of a coroutine. An event loop is used to orchestrate the execution of the coroutines.</p>

<p>There is much more to asyncio, but that's all we need to know for now. It might be a little unclear from know, so let's look at some code.</p>

<h2 id="aiohttp">aiohttp</h2>

<p><a href="https://github.com/KeepSafe/aiohttp">aiohttp</a> is a library designed to work with asyncio, with an API that looks like requests'. It's not very well documented for now, but there are some very useful <a href="https://github.com/KeepSafe/aiohttp/tree/master/examples">examples</a>. We'll first show its basic usage.</p>

<p>First, we'll define a coroutine to get a page and print it. We use <code>asyncio.coroutine</code> to decorate a function as a coroutine. <code>aiohttp.request</code> is a coroutine, and so is the <code>read</code> method, so we'll need to use <code>yield from</code> to call them. Apart from that, the code looks pretty straightforward:</p>

<pre><code class="language-python">@asyncio.coroutine
def print_page(url):  
    response = yield from aiohttp.request('GET', url)
    body = yield from response.read_and_close(decode=True)
    print(body)
</code></pre>

<p>As we have seen, we can call a coroutine from another coroutine with <code>yield from</code>. To call a coroutine from synchronous code, we'll need an event loop. We can get the standard one with <code>asyncio.get_event_loop()</code> and run the coroutine on it using its <code>run_until_complete()</code> method. So, all we have to do to run the previous coroutine is:</p>

<pre><code class="language-python">loop = asyncio.get_event_loop()  
loop.run_until_complete(print_page('http://example.com'))  
</code></pre>

<p>A useful function is <code>asyncio.wait</code>, which takes a list a coroutines and returns a single coroutine that wrap them all, so we can write:</p>

<pre><code class="language-python">loop.run_until_complete(asyncio.wait([print_page('http://example.com/foo'),  
                                      print_page('http://example.com/bar')]))
</code></pre>

<p>Another one is <code>asyncio.as_completed</code>, that takes a list of coroutines and returns an iterator that yields the coroutines in the order in which they are completed, so that when you iterate on it, you get each result as soon as it's available.</p>

<h2 id="scraping">Scraping</h2>

<p>Now that we know how to do asynchronous HTTP requests, we can write a scraper. The only other part we need is something to read the html. I use <a href="http://www.crummy.com/software/BeautifulSoup/">beautifulsoup</a> for that, be others like <a href="http://pythonhosted.org/pyquery/">pyquery</a> or <a href="http://lxml.de/">lxml</a>.</p>

<p>For this example, we'll write a small scraper to get the torrent links for various linux distributions from the pirate bay.</p>

<p>First of all, a helper coroutine to perform GET requests:</p>

<pre><code class="language-python">@asyncio.coroutine
def get(*args, **kwargs):  
    response = yield from aiohttp.request('GET', *args, **kwargs)
    return (yield from response.read_and_close(decode=True))
</code></pre>

<p>The parsing part. This post is not about beautifulsoup, so I'll keep it dumb and simple: we get the first magnet list of the page:</p>

<pre><code class="language-python">def first_magnet(page):  
    soup = bs4.BeautifulSoup(page)
    a = soup.find('a', title='Download this torrent using magnet')
    return a['href']
</code></pre>

<p>The coroutine. With this url, results are sorted by number of seeders, so the first result is actually the most seeded:</p>

<pre><code class="language-python">@asyncio.coroutine
def print_magnet(query):  
    url = 'http://thepiratebay.se/search/{}/0/7/0'.format(query)
    page = yield from get(url, compress=True)
    magnet = first_magnet(page)
    print('{}: {}'.format(query, magnet))
</code></pre>

<p>Finally, the code to call all of this:</p>

<pre><code class="language-python">distros = ['archlinux', 'ubuntu', 'debian']  
loop = asyncio.get_event_loop()  
f = asyncio.wait([print_magnet(d) for d in distros])  
loop.run_until_complete(f)  
</code></pre>

<h2 id="conclusion">Conclusion</h2>

<p>And there you go, you have a small scraper that works asynchronously. That means the various pages are being downloaded at the same time, so this example is 3 times faster than the same code with requests. You should now be able to write your own scrapers in the same way.</p>

<p>You can find the resulting code, including the bonus tracks, in this <a href="https://gist.github.com/madjar/9312452">gist</a>.</p>

<p>Once you are comfortable with all this, I recommend you take a look at <a href="http://docs.python.org/3.4/library/asyncio.html">asyncio</a>'s documentation and aiohttp <a href="https://github.com/KeepSafe/aiohttp/tree/master/examples">examples</a>, which will show you all the potential asyncio has.</p>

<p>One limitation of this approach (in fact, any hand-made approach) is that there doesn't seem to be a standalone library to handle forms. Mechanize and scrapy have nice helpers to easily submit forms, but if you don't use them, you'll have to do it yourself. This is something that bugs be, so I might write such a library at some point (but don't count on it for now).</p>

<h2 id="bonustrackdonthammertheserver">Bonus track: don't hammer the server</h2>

<p>Doing 3 requests at the same time is cool, doing 5000, however, is not so nice. If you try to do too many requests at the same time, connections might start to get closed, or you might even get banned from the website.</p>

<p>To avoid this, you can use a <a href="http://docs.python.org/3.4/library/asyncio-sync.html#semaphores">semaphore</a>. It is a synchronization tool that can be used to limit the number of coroutines that do something at some point. We'll just create the semaphore before creating the loop, passing as an argument the number of simultaneous requests we want to allow:</p>

<pre><code class="language-python">sem = asyncio.Semaphore(5)  
</code></pre>

<p>Then, we just replace:</p>

<pre><code class="language-python">page = yield from get(url, compress=True)  
</code></pre>

<p>by the same thing, but protected by a semaphore:</p>

<pre><code class="language-python">with (yield from sem):  
    page = yield from get(url, compress=True)
</code></pre>

<p>This will ensure that at most 5 requests can be done at the same time.</p>

<h2 id="bonustrackprogressbar">Bonus track: progress bar</h2>

<p>This one is just for free: <a href="https://github.com/noamraph/tqdm">tqdm</a> is a nice library to make progress bars. This coroutine works just like <code>asyncio.wait</code>, but displays a progress bar indicating the completion of the coroutines passed to it:</p>

<pre><code class="language-python">@asyncio.coroutine
def wait_with_progress(coros):  
    for f in tqdm.tqdm(asyncio.as_completed(coros), total=len(coros)):
        yield from f
</code></pre>
        </section>

        </div></body></html>