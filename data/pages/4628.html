<html><body><div><div class="entry-content">
		<p><a href="http://en.wikipedia.org/wiki/Sentiment_analysis">Sentiment Analysis</a> is one of the interesting applications of text analytics. Although the term is often associated with <a href="http://marcobonzanini.com/2015/01/19/sentiment-analysis-with-python-and-scikit-learn/">sentiment classification of documents</a>, broadly speaking it refers to the use of text analytics approaches applied to the set of problems related to identifying and extracting subjective material in text sources.</p>
<p>This article continues the series on mining Twitter data with Python, describing a simple approach for Sentiment Analysis and applying it to the rubgy data set (see <a href="http://marcobonzanini.com/2015/03/23/mining-twitter-data-with-python-part-4-rugby-and-term-co-occurrences/">Part 4</a>).</p>
<p>Tutorial Table of Contents:</p>

<h2>A Simple Approach for Sentiment Analysis</h2>
<p>The technique we’re discussing in this post has been elaborated from the traditional approach proposed by Peter Turney in his paper <a href="http://arxiv.org/abs/cs/0212032">Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews</a>. A lot of work has been done in Sentiment Analysis since then, but the approach has still an interesting educational value. In particular, it is intuitive, simple to understand and to test, and most of all <em>unsupervised</em>, so it doesn’t require any labelled data for training. </p>
<p>Firstly, we define the <em>Semantic Orientation</em> (SO) of a word as the difference between its associations with positive and negative words. In practice, we want to calculate “how close” a word is with terms like <em>good</em> and <em>bad</em>. The chosen measure of “closeness” is <a href="http://en.wikipedia.org/wiki/Pointwise_mutual_information">Pointwise Mutual Information</a> (PMI), calculated as follows (t1 and t2 are terms):</p>
<p><img src="http://s0.wp.com/latex.php?latex=%5Cmbox%7BPMI%7D%28t_1%2C+t_2%29+%3D+%5Clog%5CBigl%28%5Cfrac%7BP%28t_1+%5Cwedge+t_2%29%7D%7BP%28t_1%29+%5Ccdot+P%28t_2%29%7D%5CBigr%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mbox{PMI}(t_1, t_2) = \log\Bigl(\frac{P(t_1 \wedge t_2)}{P(t_1) \cdot P(t_2)}\Bigr)" title="\mbox{PMI}(t_1, t_2) = \log\Bigl(\frac{P(t_1 \wedge t_2)}{P(t_1) \cdot P(t_2)}\Bigr)" class="latex"/></p>
<p>In Turney’s paper, the SO of a word was calculated against <em>excellent</em> and <em>poor</em>, but of course we can extend the vocabulary of positive and negative terms. Using <img src="http://s0.wp.com/latex.php?latex=V%5E%7B%2B%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="V^{+}" title="V^{+}" class="latex"/> and a vocabulary of positive terms and <img src="http://s0.wp.com/latex.php?latex=V%5E%7B-%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="V^{-}" title="V^{-}" class="latex"/> for the negative ones, the Semantic Orientation of a term t is hence defined as:</p>
<p><img src="http://s0.wp.com/latex.php?latex=%5Cmbox%7BSO%7D%28t%29+%3D+%5Csum_%7Bt%27+%5Cin+V%5E%7B%2B%7D%7D%5Cmbox%7BPMI%7D%28t%2C+t%27%29+-+%5Csum_%7Bt%27+%5Cin+V%5E%7B-%7D%7D%5Cmbox%7BPMI%7D%28t%2C+t%27%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\mbox{SO}(t) = \sum_{t' \in V^{+}}\mbox{PMI}(t, t') - \sum_{t' \in V^{-}}\mbox{PMI}(t, t')" title="\mbox{SO}(t) = \sum_{t' \in V^{+}}\mbox{PMI}(t, t') - \sum_{t' \in V^{-}}\mbox{PMI}(t, t')" class="latex"/></p>
<p>We can build our own list of positive and negative terms, or we can use one of the many resources available on-line, for example the <a href="http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon">opinion lexicon</a> by Bing Liu.</p>
<h2>Computing Term Probabilities</h2>
<p>In order to compute <img src="http://s0.wp.com/latex.php?latex=P%28t%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P(t)" title="P(t)" class="latex"/> (the probability of observing the term <em>t</em>) and <img src="http://s0.wp.com/latex.php?latex=P%28t_1+%5Cwedge+t_2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P(t_1 \wedge t_2)" title="P(t_1 \wedge t_2)" class="latex"/> (the probability of observing the terms <em>t1</em> and <em>t2</em> occurring together) we can re-use some previous code to calculate <a href="http://marcobonzanini.com/2015/03/17/mining-twitter-data-with-python-part-3-term-frequencies/">term frequencies</a> and <a href="http://marcobonzanini.com/2015/03/23/mining-twitter-data-with-python-part-4-rugby-and-term-co-occurrences/">term co-occurrences</a>. Given the set of documents (tweets) <em>D</em>, we define the Document Frequency (DF) of a term as the number of documents where the term occurs. The same definition can be applied to co-occurrent terms. Hence, we can define our probabilities as:</p>
<p><img src="http://s0.wp.com/latex.php?latex=P%28t%29+%3D+%5Cfrac%7B%5Cmbox%7BDF%7D%28t%29%7D%7B%7CD%7C%7D%5C%5C++P%28t_1+%5Cwedge+t_2%29+%3D+%5Cfrac%7B%5Cmbox%7BDF%7D%28t_1+%5Cwedge+t_2%29%7D%7B%7CD%7C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="P(t) = \frac{\mbox{DF}(t)}{|D|}\\  P(t_1 \wedge t_2) = \frac{\mbox{DF}(t_1 \wedge t_2)}{|D|}" title="P(t) = \frac{\mbox{DF}(t)}{|D|}\\  P(t_1 \wedge t_2) = \frac{\mbox{DF}(t_1 \wedge t_2)}{|D|}" class="latex"/></p>
<p>In the previous articles, the document frequency for single terms was stored in the dictionaries <tt>count_single</tt> and <tt>count_stop_single</tt> (the latter doesn’t store stop-words), while the document frequency for the co-occurrencies was stored in the co-occurrence matrix <tt>com</tt></p>
<p>This is how we can compute the probabilities:</p>
<pre class="brush: python; title: ; notranslate" title=""># n_docs is the total n. of tweets
p_t = {}
p_t_com = defaultdict(lambda : defaultdict(int))

for term, n in count_stop_single.items():
    p_t[term] = n / n_docs
    for t2 in com[term]:
        p_t_com[term][t2] = com[term][t2] / n_docs
</pre>
<h2>Computing the Semantic Orientation</h2>
<p>Given two vocabularies for positive and negative terms:</p>
<pre class="brush: python; title: ; notranslate" title="">
positive_vocab = [
    'good', 'nice', 'great', 'awesome', 'outstanding',
    'fantastic', 'terrific', ':)', ':-)', 'like', 'love',
    # shall we also include game-specific terms?
    # 'triumph', 'triumphal', 'triumphant', 'victory', etc.
]
negative_vocab = [
    'bad', 'terrible', 'crap', 'useless', 'hate', ':(', ':-(',
    # 'defeat', etc.
]
</pre>
<p>We can compute the PMI of each pair of terms, and then compute the<br/>
Semantic Orientation as described above:</p>
<pre class="brush: python; title: ; notranslate" title="">pmi = defaultdict(lambda : defaultdict(int))
for t1 in p_t:
    for t2 in com[t1]:
        denom = p_t[t1] * p_t[t2]
        pmi[t1][t2] = math.log2(p_t_com[t1][t2] / denom)

semantic_orientation = {}
for term, n in p_t.items():
    positive_assoc = sum(pmi[term][tx] for tx in positive_vocab)
    negative_assoc = sum(pmi[term][tx] for tx in negative_vocab)
    semantic_orientation[term] = positive_assoc - negative_assoc
</pre>
<p>The Semantic Orientation of a term will have a positive (negative) value if the term is often associated with terms in the positive (negative) vocabulary. The value will be zero for neutral terms, e.g. the PMI’s for positive and negative balance out, or more likely a term is never observed together with other terms in the positive/negative vocabularies.</p>
<p>We can print out the semantic orientation for some terms:</p>
<pre class="brush: python; title: ; notranslate" title="">
semantic_sorted = sorted(semantic_orientation.items(), 
                         key=operator.itemgetter(1), 
                         reverse=True)
top_pos = semantic_sorted[:10]
top_neg = semantic_sorted[-10:]

print(top_pos)
print(top_neg)
print("ITA v WAL: %f" % semantic_orientation['#itavwal'])
print("SCO v IRE: %f" % semantic_orientation['#scovire'])
print("ENG v FRA: %f" % semantic_orientation['#engvfra'])
print("#ITA: %f" % semantic_orientation['#ita'])
print("#FRA: %f" % semantic_orientation['#fra'])
print("#SCO: %f" % semantic_orientation['#sco'])
print("#ENG: %f" % semantic_orientation['#eng'])
print("#WAL: %f" % semantic_orientation['#wal'])
print("#IRE: %f" % semantic_orientation['#ire'])
</pre>
<p>Different vocabularies will produce different scores. Using the <a href="http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon">opinion lexicon from Bing Liu</a>, this is what we can observed on the <a href="http://marcobonzanini.com/2015/03/23/mining-twitter-data-with-python-part-4-rugby-and-term-co-occurrences/">Rugby data-set</a>:</p>
<pre># the top positive terms
[('fantastic', 91.39950482011552), ('@dai_bach', 90.48767241244532), ('hoping', 80.50247748725415), ('#it', 71.28333427277785), ('days', 67.4394844955977), ('@nigelrefowens', 64.86112716005566), ('afternoon', 64.05064208341855), ('breathtaking', 62.86591435212975), ('#wal', 60.07283361352875), ('annual', 58.95378954406133)]
# the top negative terms
[('#england', -74.83306534609066), ('6', -76.0687215594536), ('#itavwal', -78.4558633116863), ('@rbs_6_nations', -80.89363516601993), ("can't", -81.75379628180468), ('like', -83.9319149443813), ('10', -85.93073078165587), ('italy', -86.94465165178258), ('#engvfra', -113.26188957010228), ('ball', -161.82146824640125)]
# Matches
ITA v WAL: -78.455863
SCO v IRE: -73.487661
ENG v FRA: -113.261890
# Individual team
#ITA: 53.033824
#FRA: 14.099372
#SCO: 4.426723
#ENG: -0.462845
#WAL: 60.072834
#IRE: 19.231722</pre>
<h2>Some Limitations</h2>
<p>The PMI-based approach has been introduced as simple and intuitive, but of course it has some limitations. The semantic scores are calculated on terms, meaning that there is no notion of “entity” or “concept” or “event”. For example, it would be nice to aggregate and normalise all the references to the team names, e.g. <em>#ita</em>, <em>Italy</em> and <em>Italia</em> should all contribute to the semantic orientation of the same entity. Moreover, do the opinions on the individual teams also contribute to the overall opinion on a match?</p>
<p>Some aspects of natural language are also not captured by this approach, more notably modifiers and negation: how do we deal with phrases like <em>not bad</em> (this is the opposite of just <em>bad</em>) or <em>very good</em> (this is stronger than just <em>good</em>)? </p>
<h2>Summary</h2>
<p>This article has continued the tutorial on mining Twitter data with Python introducing a simple approach for Sentiment Analysis, based on the computation of a semantic orientation score which tells us whether a term is more closely related to a positive or negative vocabulary. The intuition behind this approach is fairly simple, and it can be implemented using Pointwise Mutual Information as a measure of association. The approach has of course some limitations, but it’s a good starting point to get familiar with Sentiment Analysis.</p>
<p><a href="http://www.twitter.com/marcobonzanini">@MarcoBonzanini</a></p>

<div id="jp-post-flair" class="sharedaddy sd-like-enabled sd-sharing-enabled"><div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" id="like-post-wrapper-82018920-127-56d5b451c77a2" data-src="//widgets.wp.com/likes/#blog_id=82018920&amp;post_id=127&amp;origin=marcobonzanini.wordpress.com&amp;obj_id=82018920-127-56d5b451c77a2" data-name="like-post-frame-82018920-127-56d5b451c77a2"><h3 class="sd-title">Like this:</h3><p class="likes-widget-placeholder post-likes-widget-placeholder"><span class="button"><span>Like</span></span> <span class="loading">Loading...</span></p><span class="sd-text-color"/><a class="sd-link-color"/></div>
<p id="jp-relatedposts" class="jp-relatedposts">
	<h3 class="jp-relatedposts-headline"><em>Related</em></h3>
</p></div>	</div>

	
</div></body></html>