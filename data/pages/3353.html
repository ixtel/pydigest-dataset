<html><body><div><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-autocomplete-or-how-i-learned-to-stop-spelling-and-love-our-ai-overlords" class="anchor" href="#autocomplete-or-how-i-learned-to-stop-spelling-and-love-our-ai-overlords" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a><em>Autocomplete</em> or: How I learned to stop spelling and love our AI overlords</h1>

<p>Autocomplete is an adult and kid friendly exercise in creating your own AI program. </p>

<p>For those short on time, the <a href="#explain-like-im-5">ELI5</a> section is devoid of nomenclature but lengthy; the <a href="#tldr">tl;dr</a> section describes the implementation using the appropriate terms - basic principles of conditional probability, generalized <a href="http://en.wikipedia.org/wiki/Markov_chain">Markov chain</a> - but is short, concise, and includes references for further reading. </p>

<h2><a id="user-content-skip-to" class="anchor" href="#skip-to" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Skip to:</h2>



<hr/>

<h2><a id="user-content-how-to-install" class="anchor" href="#how-to-install" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>How to install:</h2>

<pre><code>pip install autocomplete
</code></pre>

<h2><a id="user-content-how-to-use" class="anchor" href="#how-to-use" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>How to use:</h2>

<div class="highlight highlight-source-python"><pre><span class="pl-k">import</span> autocomplete

<span class="pl-c"># load pickled python Counter objects representing our predictive models</span>
<span class="pl-c"># I use Peter Norvigs big.txt (http://norvig.com/big.txt) to create the predictive models</span>
autocomplete.load()

<span class="pl-c"># imagine writing "the b"</span>
autocomplete.predict(<span class="pl-s"><span class="pl-pds">'</span>the<span class="pl-pds">'</span></span>,<span class="pl-s"><span class="pl-pds">'</span>b<span class="pl-pds">'</span></span>)

[(<span class="pl-s"><span class="pl-pds">'</span>blood<span class="pl-pds">'</span></span>, <span class="pl-c1">204</span>),
 (<span class="pl-s"><span class="pl-pds">'</span>battle<span class="pl-pds">'</span></span>, <span class="pl-c1">185</span>),
 (<span class="pl-s"><span class="pl-pds">'</span>bone<span class="pl-pds">'</span></span>, <span class="pl-c1">175</span>),
 (<span class="pl-s"><span class="pl-pds">'</span>best<span class="pl-pds">'</span></span>, <span class="pl-c1">149</span>),
 (<span class="pl-s"><span class="pl-pds">'</span>body<span class="pl-pds">'</span></span>, <span class="pl-c1">149</span>),
 <span class="pl-c1">...</span>]

<span class="pl-c"># now you type an "o"</span>

autocomplete.predict(<span class="pl-s"><span class="pl-pds">'</span>the<span class="pl-pds">'</span></span>,<span class="pl-s"><span class="pl-pds">'</span>bo<span class="pl-pds">'</span></span>)

[(<span class="pl-s"><span class="pl-pds">'</span>bone<span class="pl-pds">'</span></span>, <span class="pl-c1">175</span>),
 (<span class="pl-s"><span class="pl-pds">'</span>body<span class="pl-pds">'</span></span>, <span class="pl-c1">149</span>),
 (<span class="pl-s"><span class="pl-pds">'</span>bones<span class="pl-pds">'</span></span>, <span class="pl-c1">122</span>),
 (<span class="pl-s"><span class="pl-pds">'</span>boy<span class="pl-pds">'</span></span>, <span class="pl-c1">46</span>),
 (<span class="pl-s"><span class="pl-pds">'</span>bottom<span class="pl-pds">'</span></span>, <span class="pl-c1">32</span>),
 (<span class="pl-s"><span class="pl-pds">'</span>box<span class="pl-pds">'</span></span>, <span class="pl-c1">24</span>),
 <span class="pl-c1">...</span>]</pre></div>

<h3><a id="user-content-spell-correction" class="anchor" href="#spell-correction" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Spell Correction</h3>

<p>Now say you are in the process of typing "body" (with a preceding "the")</p>

<div class="highlight highlight-source-python"><pre>autocomplete.predict(<span class="pl-s"><span class="pl-pds">'</span>the<span class="pl-pds">'</span></span>,<span class="pl-s"><span class="pl-pds">'</span>bo<span class="pl-pds">'</span></span>)

[(<span class="pl-s"><span class="pl-pds">'</span>bone<span class="pl-pds">'</span></span>, <span class="pl-c1">175</span>),
 (<span class="pl-s"><span class="pl-pds">'</span>body<span class="pl-pds">'</span></span>, <span class="pl-c1">149</span>),
 (<span class="pl-s"><span class="pl-pds">'</span>bones<span class="pl-pds">'</span></span>, <span class="pl-c1">122</span>),
 (<span class="pl-s"><span class="pl-pds">'</span>boy<span class="pl-pds">'</span></span>, <span class="pl-c1">46</span>),
 (<span class="pl-s"><span class="pl-pds">'</span>bottom<span class="pl-pds">'</span></span>, <span class="pl-c1">32</span>),
 (<span class="pl-s"><span class="pl-pds">'</span>box<span class="pl-pds">'</span></span>, <span class="pl-c1">24</span>),
 <span class="pl-c1">...</span>]
</pre></div>

<p>And then you make the fatal error of typing an "f" instead of a "d"</p>

<div class="highlight highlight-source-python"><pre>autocomplete.predict(<span class="pl-s"><span class="pl-pds">'</span>the<span class="pl-pds">'</span></span>,<span class="pl-s"><span class="pl-pds">'</span>bof<span class="pl-pds">'</span></span>)

[(<span class="pl-s"><span class="pl-pds">'</span>body<span class="pl-pds">'</span></span>, <span class="pl-c1">149</span>),
 (<span class="pl-s"><span class="pl-pds">'</span>bottom<span class="pl-pds">'</span></span>, <span class="pl-c1">32</span>),
 (<span class="pl-s"><span class="pl-pds">'</span>borzois<span class="pl-pds">'</span></span>, <span class="pl-c1">16</span>),
 (<span class="pl-s"><span class="pl-pds">'</span>bottle<span class="pl-pds">'</span></span>, <span class="pl-c1">13</span>),
 (<span class="pl-s"><span class="pl-pds">'</span>bodies<span class="pl-pds">'</span></span>, <span class="pl-c1">13</span>),
 (<span class="pl-s"><span class="pl-pds">'</span>border<span class="pl-pds">'</span></span>, <span class="pl-c1">12</span>)
 <span class="pl-c1">...</span>]
</pre></div>

<p>Relax! Autocomplete has you covered. Using a simple <a href="http://en.wikipedia.org/wiki/Fat-finger_error">"fat-finger"</a> error model,
you can rest assured that you won't be making <a href="http://www.bbc.com/news/business-29454265">six-hundred billion dollar mistakes</a> at your Japanese investment firm.</p>

<p>If you have your own language model in the form described in <a href="#explain-like-im-5">ELI5</a>, then use the <em>models</em> submodule to call the training method:</p>

<div class="highlight highlight-source-python"><pre><span class="pl-k">from</span> autocomplete <span class="pl-k">import</span> models

models.train_models(<span class="pl-s"><span class="pl-pds">'</span>some giant string of text<span class="pl-pds">'</span></span>)
</pre></div>

<p>Want to run it as a server (bottlepy required)?</p>

<div class="highlight highlight-source-python"><pre><span class="pl-k">import</span> autocomplete

autocomplete.run_server()

<span class="pl-c">#output</span>
Bottle v0.12.8 server starting up (using WSGIRefServer())<span class="pl-c1">...</span>
Listening on http:<span class="pl-k">//</span>localhost:<span class="pl-c1">8080</span><span class="pl-k">/</span>
Hit Ctrl<span class="pl-k">-</span><span class="pl-c1">C</span> to <span class="pl-c1">quit</span>.
</pre></div>

<p>Now head over to http://localhost:8080/the/bo</p>

<pre><code>http://localhost:8080/the/bo
#output
{"body": 149, "box": 24, "bottom": 32, "boy": 46, "borzois": 16, "bodies": 13, "bottle": 13, "bones": 122, "book": 14, "bone": 175}

http://localhost:8080/the/bos
#output
{"boscombe": 11, "boston": 7, "boss": 1, "bosom": 5, "bosses": 4}
</code></pre>

<h3><a id="user-content-obligatory-tests" class="anchor" href="#obligatory-tests" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Obligatory tests</h3>

<pre><code>python setup.py test
</code></pre>

<hr/>



<p>The following code excerpt is my interpretation of a series of lessons/concepts expressed in a number of different books.</p>

<p>The unifying concept can be said to be <a href="http://en.wikipedia.org/wiki/Conditional_probability">conditional probability</a>:</p>

<pre><code>P(A , B) = P(B | A) * P(A)
</code></pre>

<p>Which can read as saying:</p>

<pre><code>The probability of A and B occuring is equal to the probability of B occuring, given that A has occured
</code></pre>

<p>More on this below.</p>

<div class="highlight highlight-source-python"><pre>    <span class="pl-c"># "preperation" step</span>
    <span class="pl-c"># for every word in corpus, normalize ('The' -&gt; 'the'), insert to list</span>
    <span class="pl-c1">WORDS</span> <span class="pl-k">=</span> helpers.re_split(corpus)

    <span class="pl-c"># first model -&gt; P(word)</span>
    <span class="pl-c"># Counter constructor will take a list of elements and create a frequency distribution (histogram)</span>
    <span class="pl-c1">WORDS_MODEL</span> <span class="pl-k">=</span> collections.Counter(<span class="pl-c1">WORDS</span>)

    <span class="pl-c"># another preperation step</span>
    <span class="pl-c"># [a,b,c,d] -&gt; [[a,b], [b,c], [c,d]]</span>
    <span class="pl-c1">WORD_TUPLES</span> <span class="pl-k">=</span> <span class="pl-c1">list</span>(helpers.chunks(<span class="pl-c1">WORDS</span>, <span class="pl-c1">2</span>))

    <span class="pl-c"># second model -&gt; P(next word | prev. word)</span>
    <span class="pl-c"># I interpret "..| prev. word)" as saying "dictionary key</span>
    <span class="pl-c"># leading to seperate and smaller (than WORDS_MODEL) freq. dist.</span>
    <span class="pl-c1">WORD_TUPLES_MODEL</span> <span class="pl-k">=</span> {first:collections.Counter() <span class="pl-k">for</span> first, second <span class="pl-k">in</span> <span class="pl-c1">WORD_TUPLES</span>}

    <span class="pl-k">for</span> prev_word, next_word <span class="pl-k">in</span> <span class="pl-c1">WORD_TUPLES</span>:
        <span class="pl-c"># this is called the "conditioning" step where we assert</span>
        <span class="pl-c"># that the probability space of all possible "next_word"'s</span>
        <span class="pl-c"># is "conditioned" under the event that "prev_word" has occurred</span>
        <span class="pl-c1">WORD_TUPLES_MODEL</span>[prev_word].update([next_word])
</pre></div>

<p>Textbooks, and locations therein, where the concept-in-practice has been expressed:</p>

<p>I. <a href="http://ics.upjs.sk/%7Epero/web/documents/pillar/Manning_Schuetze_StatisticalNLP.pdf">Intro to Statistical Natural Language Processing</a> - Manning, Schütze, 1999</p>

<pre><code>a. frequency distribution showing the most common words and frequencies in *Tom Sawyer*, pg. 21

b. conditional probability definition expressed in page 42 - section 2.1.2

c. the intuition for *frequency* distributions found in pg. 153 (provided in the context of finding [*Collocations*](http://en.wikipedia.org/wiki/Collocation))
</code></pre>

<p>II. <a href="http://mitpress.mit.edu/books/probabilistic-graphical-models">Probabilistic Graphical Models</a> - Kohler, Friedman, 2009</p>

<pre><code>a. conditional probability definition found on pg. 18 (hilariously and coincidentally found in section 2.1.2.1)
</code></pre>

<p>III. <a href="http://aima.cs.berkeley.edu">Artificial Intelligence - A Modern Approach</a> - Russell, Norvig, 3rd. ed. 2010</p>

<pre><code>a. conditional probability concept explained in pg. 485

b. the "language" (I take to mean "intuition" for asserting things in the probabilistic sense) pg. 486

c. the notion of "conditioning" found in pg. 492-494
</code></pre>

<h2><a id="user-content-motivation" class="anchor" href="#motivation" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Motivation</h2>

<p>Similar to the motivation behind <a href="https://github.com/rodricios/eatiht#motivation">eatiht</a>, I found that it took far too long to find a palpable theory-to-application example of what amounts to more than a 500 pages of words across 3 books, each spanning a large index of, in certain cases, <em>counter-intuitive</em> nomenclature; read the <a href="http://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama_michael_i_jordan/ckep3z6">light criticisms</a> made by Michael I. Jordan on the matter (he was recently named <a href="http://dataconomy.com/10-machine-learning-experts-you-need-to-know/">#2 machine learning expert "we need to know" on dataconomy.com</a>).</p>

<p>You can find similar thoughts being expressed <a href="http://brenocon.com/blog/2008/12/statistics-vs-machine-learning-fight/"><strong>in an article from 2008 (updated 2009)</strong></a> by <a href="http://brenocon.com">Brennan O'Connor</a></p>

<hr/>

<p><a href="#note-1"><em>This work is dedicated to my siblings</em></a>.</p>

<h2><a id="user-content-explain-like-im-5" class="anchor" href="#explain-like-im-5" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Explain like I'm 5<a href="#note-1">*</a></h2>

<p>*Warning! This explanation is literally intended for young kids - I'm actually trying to see if these concepts can be explained to an audience unaware of the nomenclature used within the statistical <a href="http://en.wikipedia.org/wiki/Natural_language_processing">nlp</a> and other machine learning fields. For example, my 7, 9, 11, 14 y.o. siblings, and basically anyone else who's ever read a story to a child - they would be a part of the target audience.</p>

<p>If you've found this readable and informative, please consider putting on the goofiest face and reading this to your kids, if you have any :) If you do, please send me your thoughts on the experience.</p>

<p>I'm only interested in lowering the barrier to entry. I should have included this note since the beginning (sorry to those who undoubtedly left with a bad taste in their mouth).</p>

<p>You can contact me at <a href="mailto:rodrigopala91@gmail.com">rodrigopala91@gmail.com</a></p>

<p>Thanks for reading,</p>

<p>Rodrigo</p>

<h2><a id="user-content-eli5" class="anchor" href="#eli5" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>ELI5</h2>

<p>No. I'm explaining this like you're 5. I know you're not <em>5</em> , <em>you guys... Chris, stop jumping on your sister's back</em>!</p>

<p>Ok, so I'm saying, <em>imagine I'm 5!</em></p>

<p>Oh, that was easy now huh? Let's just forget the <em>I'm 5</em> part.</p>

<p>Imagine a giant collection of books.</p>

<p>For example, all the Harry Potter and Hunger Games novels put together.</p>

<p>What if I asked you to go through all the pages and all the words in those pages?</p>

<p>Now I'm not asking you <em>four</em> to actually <em>read</em> the books. You know, just go through, beginning to end, and notice each word.</p>

<p>For every new word you see, write it down, and put a "1" next to it, and everytime you see a word <em>again</em>, add "1" more to the previous number.</p>

<p>So basically I'm asking y'all to keep count of how many times a word comes up.</p>

<p>Got it? If yes, cool! If not, find a sibling, friend, or adult near you and ask them to help you out :)</p>

<p>...</p>

<p>Say you start with <em>Harry Potter and the Sorcerer's Stone</em>:</p>

<pre><code>Mr. and Mrs. Dursley of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much...
</code></pre>

<p>And imagine that you're on the 5th word. This or something close to this is what you're going for:</p>

<pre><code>Mr.     -&gt; 1
and     -&gt; 1
Mrs.    -&gt; 1
Dursley -&gt; 1
of      -&gt; 1
</code></pre>

<p>Or if you're a <em>wannabe-Harry-Potter</em> fan, ah I'm just kidding!</p>

<p>If you started with <em>the-book-that-must-not-be-named</em> - I know you guys won't get it, but persons my age will :)</p>

<p>Alright! So you started with <em>The Hunger Games</em>:</p>

<pre><code>When I wake up, the other side of the bed is cold...
</code></pre>

<p>By the sixth word you have:</p>

<pre><code>When  -&gt; 1
I     -&gt; 1
wake  -&gt; 1
up    -&gt; 1
the   -&gt; 1
</code></pre>

<p>You have a long day ahead of you...</p>

<p>...</p>

<p><em>1,105,285 words later</em></p>

<p>Now that you're done tallying up all those words, why not order all these words by the <em>number of times you've seen them</em>?</p>

<p>See you next week!</p>

<p>...</p>

<p>Back so soon? You should have gotten something like this:</p>

<pre><code>psst*, remember, the format is:
 word -&gt; # of times the word appears

'the' -&gt; 80030
'of'  -&gt; 40025
'and' -&gt; 38313
'to'  -&gt; 28766
'in'  -&gt; 22050
'a'   -&gt; 21155
'that'-&gt; 12512
'he'  -&gt; 12401
'was' -&gt; 11410
'it'  -&gt; 10681
... there's a lot more words you've tallied up...
</code></pre>

<p>Those were the most common words.</p>

<p>Now on the <em>less-frequent</em> end, you'll find your words appearing not as often...</p>

<pre><code>... 29137 words later.
'przazdziecka' -&gt; 1
'disclosure'   -&gt; 1
'galvanism'    -&gt; 1
'repertoire'   -&gt; 1
'bravado'      -&gt; 1
'gal'          -&gt; 1
'ideological'  -&gt; 1
'guaiacol'     -&gt; 1
'expands'      -&gt; 1
'revolvers'    -&gt; 1
</code></pre>

<p>Yeah Chris? Oh, 'what does <em>lez freekend</em>' mean? Um, so it means something like: <em>you probably won't hear or read that word very often.</em></p>

<p>Now what if I asked you to help me find this word I'm looking for? And I know this word starts with the letters: 'th'.</p>

<p>I'm pretty sure you guys can do this much faster!</p>

<p>...</p>

<p><em>5 minutes later!</em></p>

<p>...</p>

<p>Not bad! You only had to go through 29157 unique words after all!</p>

<pre><code>'the'  -&gt; 80030
'that' -&gt; 12512
'this' -&gt; 4063
'they' -&gt; 3938
'there'-&gt; 2972
'their'-&gt; 2955
'them' -&gt; 2241
'then' -&gt; 1558
'these'-&gt; 1231
'than' -&gt; 1206
... 229 words more...
</code></pre>

<p>239 words, still kind of lot though huh? And you know your big brother, he's too lazy to do this work <em>by hand</em> (<em>cough</em> program it up  <em>cough</em>) ;)</p>

<p>So the word I'm looking for is on the tip of my tongue. I think the next letter is "i".</p>

<p><em>1 minute later</em></p>

<pre><code>'this'     -&gt; 4063
'think'    -&gt; 557
'things'   -&gt; 321
'thing'    -&gt; 303
'third'    -&gt; 239
'thin'     -&gt; 166
'thinking' -&gt; 137
'thirty'   -&gt; 123
'thick'    -&gt; 77
'thirds'   -&gt; 43
... 36 words more...
</code></pre>

<p><em>I scan through the first 10 words.</em> Oh, I just remembered that the next letter is 'r'.</p>

<p><em>You start taking out even more words.</em></p>

<p><em>10 seconds later.</em></p>

<pre><code>'third'      -&gt; 239
'thirty'     -&gt; 123
'thirds'     -&gt; 43
'thirteen'   -&gt; 32
'thirst'     -&gt; 13
'thirteenth' -&gt; 11
'thirdly'    -&gt; 8
'thirsty'    -&gt; 5
'thirtieth'  -&gt; 3
'thirties'   -&gt; 2
</code></pre>

<p>Aha, 'thirdly' was the word I was looking for! What, you never heard of the word "thirdly" before?</p>

<p>Now you might be saying to yourself, "<em>that's pretty cool!</em>", and you're right!</p>

<p>And you know what's cooler? <em>Making everyone's life a tiny bit easier</em> is! :)</p>

<p>But how can you do that with just <em>words</em>?</p>

<p>Aren't words boring and dull?</p>

<p>It's like all we do is talk, write, and think with <em>words</em>. I mean, how lame, I can't even describe to you this <em>autocomplete</em> thing-slash-idea-thing without having to write it out with <em>words</em>!</p>

<p>Ugh! I hate words!</p>

<p><em>Whoah, wait a minute! That was not cool of me! Let's relax for a minute.</em></p>

<p>Let's try to give an imaginary hug to the word-factory in our brains. That part of our brain works so hard, even when we don't ask it to. How nice of our brain to do that. Not!</p>

<p>What I'm trying to say is that sometimes it's not very nice for our brains to distract us, especially when we have homework or other, real-world problems like adult-homework.</p>

<p>...</p>

<p>So how about this:</p>

<p>As a mental exercise, let's just try to think about <em>what</em> the next sentence coming out of our own mouths <em>will be</em><a href="#note-2">*</a>.</p>

<p>Now if you're thinking about what will be coming out of my mouth, or out of your mouth, or your mouth, or your mouth, or your mouth, you're doing it wrong! (to readers who aren't one of my 4 younger siblings, that's how many I have).</p>

<p>Try your best to think about <em>what</em> the next sentence coming out of <em>your own</em> mouth will be.</p>

<p>...</p>

<p>Did you decide on your sentence? Good!</p>

<p>Now what if I asked you to give me two <strong>good</strong> reasons explaining <em>why</em> and <em>how</em> you chose the sentence you chose?</p>

<p>Wait, I can't even do that! Let's make it easier on ourselves. Let's try to only answer <em>why</em> and <em>how</em> we chose just the first word.</p>

<p>Still pretty hard huh?</p>

<p>If you thought it was pretty darn hard to give a <em>good and honest</em> reason as to why it is you chose the word you chose, it's alright. :)</p>

<p>But like all couch-scientists, let's just make a guess! My guess is: our brain is a <strong>probabilistic machine</strong>.</p>

<p>If you feel like you don't <em>get</em> what the word "probabilisitic" or "probability" means, sure you do! Just use the word "probably" in one of your sentences, but try to make some sense.</p>

<p>Ok, so what do I mean? Well, let's just consider the English language. Like most other things, the English language has rules.</p>

<p>The kind of rules that can be simplified down to:</p>

<p>1) "<strong><em>something</em></strong> <em>action</em> <strong><em>something</em></strong>".</p>

<p>2) Replace <strong><em>something</em></strong>'s and <strong><em>action</em></strong> with words that make sense to you.</p>

<p>Fair enough, right?</p>

<p>Now imagine that your brain essentially has those rules "branded" or "recorded" into itself. Ok, so now I'm starting to not make much sense huh?</p>

<p>How about this? How many times have you heard,</p>

<p>"<strong>Do</strong> your <strong>bed</strong>!"</p>

<p>"<strong>Brush</strong> your <strong>teeth</strong>!"</p>

<p>"<strong>Let's</strong> get <strong>food</strong>!"</p>

<p>While each one of you guys may have not heard those <em>exact</em> sentences, what I'm trying to say makes sense right? <em>That you probably heard certain sentences more often than others?</em></p>

<p>...</p>

<p>Now, imagine you could put "pause" right after the first word that comes out of your mouth.</p>

<p>Let's just say that first word is "the".</p>

<p>Now in the case that you stuttered for reasons outside your conscientious control (for example: "thhh thhe the"). No big deal, you meant to say "the", so let's <em>flatten</em> it to just that!</p>

<p>With that <em>word</em> said, what words do you <em>think</em> you might have said after it?</p>

<p>You might tell me, "<em>any word I want!</em></p>

<p>Of course you could have! I bet you spent a millisecond thinking about whether or not the next word you were going to say was going to be: <em>guaiacol</em>.</p>

<p>I <em>know</em> because I thought about using that word too!</p>

<p>I can remember the first time I heard (or read) <em>guaiacol</em> like it was yesterday. I read it in some funky article on the internet. I found the word in a list of words that don't appear too often in the English language.</p>

<p>After I read it, I was able to fit <em>guaiacol</em> nicely into that part of my brain where I... uhh.. was... able... uhh...</p>

<p>Oh, you <em>know</em>, that place in my brain where I get to choose whether I want to say <em>the apple</em>, <em>the automobile</em>, <em>the austronaut</em>, etc.</p>

<p>...</p>

<p>Ok, so clearly I'm no brainician, and that may or may not be the way our brain works.</p>

<p>But even though that idea might be wrong, the idea itself sounds like a pretty darn good way of suggesting the next word or words somebody is trying to <em>type</em>.</p>

<p>What if you had a way to count the number of times you've heard "apple" said after the word "the"?</p>

<p>Ask yourself the same question, but now with the word "automobile" instead of "apple".</p>

<p>What if you had the time to think about every possible word that you've ever heard spoken after the word "the"? I'd say it might have looked something like this:</p>

<pre><code>Words you might have heard following the word "the" and the number of times you might have heard it

'same'     -&gt; 996
'french'   -&gt; 688
'first'    -&gt; 652
'old'      -&gt; 591
'emperor'  -&gt; 581
'other'    -&gt; 528
'whole'    -&gt; 500
'united'   -&gt; 466
'room'     -&gt; 376
'most'     -&gt; 373

... 9331 more words...
</code></pre>

<p>Not impressed with your brain yet? Let's continue this little thought experiment further.</p>

<p>Imagine that you just said "the", and you could put pause after the first <em>letter</em> of the next word out of your mouth: "h".</p>

<p>Real quick, think of the shortest amount of time you can think of. Think of the shortest <em>second</em> you can think of. Now shorter than that too.</p>

<p>At this point, you can't even call that length of time a <em>second</em>. But in that length of time, your brain may have just done this:</p>

<pre><code>Every word you've ever heard coming after the word "the":

'house'   -&gt; 284
'head'    -&gt; 117
'hands'   -&gt; 101
'hand'    -&gt; 97
'horses'  -&gt; 71
'hill'    -&gt; 64
'highest' -&gt; 64
'high'    -&gt; 57
'history' -&gt; 56
'heart'   -&gt; 55
</code></pre>

<p>And that brain you got did this realllllyyyyyy fast. Faster than Google, Bing, Yahoo and any other company can ever hope to beat. And your brain did this without even asking for your permission. I think our brains are trying to control us you guys, oh no!</p>

<p>...</p>

<p>Thanks for reading this far folks. Please go to the <a href="https://github.com/rodricios/autocomplete#afterword">afterword</a> for some of the resources I've found useful in both building the intuition, and writing this article.</p>

<p>Also, if it's not too much to ask, consider following me or tweeting this to your friends and/or family, any support is appreciated :)</p>



<h2><a id="user-content-if-youre-not-5" class="anchor" href="#if-youre-not-5" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>If you're not 5</h2>

<p>The basic idea is this:</p>

<p>Assume you have a large collection of English-understandable text merged into a single string.</p>

<p>Start by transforming that string into a list of words (AKA <em>ngrams of word-legth</em>), and also (but not required) normalize each word ('The' -&gt; 'the').</p>

<p>Once you have a normalized list of words, you can start building a frequency distribution measuring the frequency of each word.</p>

<p>...</p>

<p>At this point you can start "predict" the "final state" of a word-in-progress. But consider the case where a user types in some query box:</p>

<pre><code>"The th"
</code></pre>

<p>And he intends to write:</p>

<pre><code>"The third"
</code></pre>

<p>With the above predictive model, you'll be suggesting something like:</p>

<pre><code>[
    ('the', 80030),
    ('they', 3938),
    ('there', 2972),
    ...
]
</code></pre>

<p>This explains one specific type of predictive model, which can be written as P(word), and you've just seen the pitfalls of using <strong>just</strong> this model.</p>

<p>Now for the next word, ask yourself, what's the probability that I'm going to type the word "apple" given that I wrote "tasty"?</p>

<p>In machine learning and AI books, you'll be presented <em>Conditional Probability</em> with the following equation:</p>

<pre><code>P(word A and word B) = P(word B | word A) * P(word A)
</code></pre>

<p>That equation addresses the problem that I mentioned.</p>

<p>We've handled P(wordA) already.</p>

<p>To handle P(word B | word A), which reads <em>probability of word A given word B *, I take a *literall</em> interpretation of the word "given", in that context, to mean the following:</p>

<p><em>"word A" is the key pointing to a probability distribution representing all the words that follow "word A"</em></p>

<p>Once we can represent this second model, we can also apply the <em>filtering</em> step - given that we know more letters in the second word, we can zone in on more precise suggestions.</p>

<hr/>

<h3><a id="user-content-afterword" class="anchor" href="#afterword" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Afterword</h3>

<p>notes: *I have to give a shout out to <a href="https://twitter.com/SamHarrisOrg">Sam Harris</a> for being, AFAIK, the first person or one of the firsts, in <a href="https://www.youtube.com/watch?v=pCofmZlC72g#t=1144">wonderfully putting into words</a> what I've borrowed and slightly adapted for this writing. <a href="http://www.samharris.org/">I highly recommend his work</a></p>

<p>Another shoutout to <a href="http://norvig.com">Peter Norvig</a> for inspiring me and probably many others with our own little "toy" programs. His <em>Occam's Razor</em> approach to problem solving will likely cause some confusion as it may appear that my work is an almost full on copy-paste of his <a href="http://norvig.com/spell-correct.html"><em>How to Write a Spell Checker</em></a>!</p>

<p>But I swear it's not! I actually I think I may have out-Norvig'ed Peter Norvig when it comes to describing <a href="http://en.wikipedia.org/wiki/Conditional_probability">conditional probability</a>: P(wordA &amp; wordB) = P(wordB | wordA)*P(wordA)</p>

<p>And another one to Rob Renaud's <a href="https://github.com/rrenaud/Gibberish-Detector">Gibberish Detector</a>. I, out of pure chance, ran into his project some time after running into Norvig's article. I can't describe <em>how much it helped</em> to intuitively understand what the heavy hitters of "AI" consider to be introductory material; this was greatly needed b/c at the time, I felt overwhelmed by my own desire to really understand this area, and everything else going on.</p>

<p>I do have a second article about this exact thing, only expressed differently (audience is non-programming), and it may or may not be posted soon! <del>Oh and the code too, that is if someone hasn't gotten to translating the above article to code before I can get to uploading the project :P I'm trying to get the kinks out of here and the code so it's simple, duh!</del></p>

<p>I dedicate this work to my sisters, Cat, Melissa and Christine, and my favorite brother, Christian :)</p>

<h4><a id="user-content-note-1" class="anchor" href="#note-1" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>note 1</h4>

<p><a href="#explain-like-im-5">go back</a></p>

<p><em>To avoid confusion, I wrote this section in the form of a letter to my younger siblings</em></p>

<h4><a id="user-content-note-2" class="anchor" href="#note-2" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>note 2</h4>

<p>*I'm borrowing, what I consider, <a href="https://www.youtube.com/watch?v=pCofmZlC72g#t=1144">one of the most beautiful thought experiments I've ever heard trying to describe one's self</a>. I'm a big fan of Sam Harris's work. Highly recommend!</p>
</article>
  </div></body></html>