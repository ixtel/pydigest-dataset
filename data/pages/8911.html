<html><body><div><div class="section-inner layoutSingleColumn"><h3 name="9040" id="9040" class="graf--h3 graf--first">Highway Networks with TensorFlow</h3><figure name="e19f" id="e19f" class="graf--figure graf-after--h3"><figcaption class="imageCaption">Photo from <a href="http://unsplash.com/" data-href="http://unsplash.com/" class="markup--anchor markup--figure-anchor" rel="nofollow">Unsplash</a></figcaption></figure><p name="52e4" id="52e4" class="graf--p graf-after--figure">This week I implemented <a href="http://arxiv.org/abs/1507.06228" data-href="http://arxiv.org/abs/1507.06228" class="markup--anchor markup--p-anchor" rel="nofollow">highway networks</a> to get an intuition for how they work. Highway networks, inspired by <a href="https://en.wikipedia.org/wiki/Long_short-term_memory" data-href="https://en.wikipedia.org/wiki/Long_short-term_memory" class="markup--anchor markup--p-anchor" rel="nofollow">LSTMs</a>, are a method of constructing networks with hundreds, even thousands, of layers. Let’s see how we construct them using TensorFlow.</p><p name="a353" id="a353" class="graf--p graf-after--p"><strong class="markup--strong markup--p-strong">TL;DR</strong> <a href="https://gist.github.com/jimfleming/4b7d3b29b9220845dcff" data-href="https://gist.github.com/jimfleming/4b7d3b29b9220845dcff" class="markup--anchor markup--p-anchor" rel="nofollow">Fully-connected highway notebook</a> and <a href="https://gist.github.com/jimfleming/b6b9130af971399786fa" data-href="https://gist.github.com/jimfleming/b6b9130af971399786fa" class="markup--anchor markup--p-anchor" rel="nofollow">convolutional highway notebook</a>.</p><h4 name="5568" id="5568" class="graf--h4 graf-after--p">Implementation</h4><p name="01fe" id="01fe" class="graf--p graf-after--h4">For comparison, let’s start with a standard fully-connected (or “dense”) layer. We need a weight matrix and a bias vector then we’ll compute the following for the layer output:</p><figure name="3210" id="3210" class="graf--figure graf-after--p"><figcaption class="imageCaption">Computing the output of a dense layer. (Bias omitted for simplicity and to match the paper.)</figcaption></figure><figure name="1a17" id="1a17" class="graf--figure graf--iframe graf-after--figure"><p class="iframeContainer"/></figure><p name="9394" id="9394" class="graf--p graf-after--figure">Here’s what a dense layer looks like as a graph in TensorBoard:</p><figure name="90f5" id="90f5" class="graf--figure graf-after--p"><figcaption class="imageCaption">A dense layer in TensorBoard.</figcaption></figure><p name="53e6" id="53e6" class="graf--p graf-after--figure">For the highway layer what we want are two “gates” that control the flow of information. The “transform” gate controls how much of the activation we pass through and the “carry” gate controls how much of the unmodified input we pass through. Otherwise, the layer largely resembles a dense layer with a few additions:</p><figure name="b371" id="b371" class="graf--figure graf-after--p"><figcaption class="imageCaption">Computing the highway layer output. (Bias omitted for simplicity and to match the paper.)</figcaption></figure><ul class="postList"><li name="896f" id="896f" class="graf--li graf-after--figure">An extra set of weights and biases to be learned for the gates.</li><li name="1f72" id="1f72" class="graf--li graf-after--li">The transform gate operation (<strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em">T</em></strong>).</li><li name="e9ba" id="e9ba" class="graf--li graf-after--li">The carry gate operation (<strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em">C</em></strong> or just <strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em">1 - T</em></strong>).</li><li name="0e70" id="0e70" class="graf--li graf-after--li">The layer output (<strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em">y</em></strong>) with the new gates.</li></ul><p name="e052" id="e052" class="graf--p graf-after--li">What happens is that when the transform gate is <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">1</em></strong>, we pass through our activation (<strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">H</em></strong>) and suppress the carry gate (since it will be <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">0</em></strong>). When the carry gate is <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">1</em></strong>, we pass through the unmodified input (<strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">x</em></strong>), while the activation is suppressed.</p><figure name="6f76" id="6f76" class="graf--figure graf--iframe graf-after--p"><p class="iframeContainer"/></figure><p name="e840" id="e840" class="graf--p graf-after--figure">Here’s what the highway layer graph looks in TensorBoard:</p><figure name="0f26" id="0f26" class="graf--figure graf-after--p"><figcaption class="imageCaption">A highway layer in TensorBoard.</figcaption></figure><p name="3594" id="3594" class="graf--p graf-after--figure">Using a highway layer in a network is also straightforward. One detail to keep in mind is that consecutive highway layers must be the same size but you can use fully-connected layers to change dimensionality. This becomes especially complicated in convolutional layers where each layer can change the output dimensions. We can use padding (<strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">‘SAME’</em></strong>) to maintain each layers dimensionality.</p><p name="ef5b" id="ef5b" class="graf--p graf-after--p">Otherwise, by simply using hyperparameters from the TensorFlow docs (i.e. no hyperparameter search) the fully-connected highway network performed much better than a fully-connected network. Using MNIST as my simple trial:</p><ul class="postList"><li name="8702" id="8702" class="graf--li graf-after--p">20 fully-connected layers fail to achieve more than 15% accuracy.</li><li name="e76a" id="e76a" class="graf--li graf-after--li">18 highway layers (with two fully-connected layers to transform the input and output) achieves ~95% accuracy. Which is also much better than a shallow network which only reaches 91%.</li></ul><p name="09f4" id="09f4" class="graf--p graf-after--li">Now that we have a highway network, I wanted to answer a few questions that came up for me while reading the paper. For instance, how deep will the network converge? The paper briefly mentions 1000 layers:</p><blockquote name="72d4" id="72d4" class="graf--pullquote pullquote graf-after--p">In pilot experiments, SGD did not stall for networks with more than 1000 layers. (2.2)</blockquote><p name="c8db" id="c8db" class="graf--p graf-after--pullquote"><strong class="markup--strong markup--p-strong">Can we train with 1000 layers on MNIST?</strong></p><p name="f683" id="f683" class="graf--p graf-after--p">Yes, also reaching around 95% accuracy. Try it out with a carry bias around <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">-20.0</em></strong> for MNIST (from the paper the network will only utilize ~15 layers anyway). The network can probably even go deeper since the it’s just learning to carry the last 980 layers or so. We can’t do much useful at or past 1000 layers so that seems sufficient for now.</p><p name="d8fb" id="d8fb" class="graf--p graf-after--p"><strong class="markup--strong markup--p-strong">What happens if you set very low or very high carry biases?</strong></p><p name="5740" id="5740" class="graf--p graf-after--p">In either extreme the network simply fails to converge in a reasonable amount of time. In the case of low biases (more positive), the network starts as if the carry gates aren’t present at all. In the case of high biases (more negative), we’re putting more emphasis on carrying and the network can take a long time to overcome that. Otherwise, the biases don’t seem to need to be exact, at least on this simple example. When in doubt start with high biases (more negative) since it’s easier to learn to overcome carrying than without carry gates (which is just a plain network).</p><h4 name="5005" id="5005" class="graf--h4 graf-after--p">Conclusion</h4><p name="fcab" id="fcab" class="graf--p graf-after--h4">Overall I was happy with how easy highway networks were to implement. They’re fully differentiable with only a single additional hyperparameter for the initial carry bias. One downside is that highway layers do require additional parameters for the transform weights and biases. However, since we can go deeper, the layers do not need to be as wide which can compensate.</p><p name="3027" id="3027" class="graf--p graf-after--p">Here’s are the complete notebooks if you want to play with the code: f<a href="https://gist.github.com/jimfleming/4b7d3b29b9220845dcff" data-href="https://gist.github.com/jimfleming/4b7d3b29b9220845dcff" class="markup--anchor markup--p-anchor" rel="nofollow">ully-connected highway notebook</a> and <a href="https://gist.github.com/jimfleming/b6b9130af971399786fa" data-href="https://gist.github.com/jimfleming/b6b9130af971399786fa" class="markup--anchor markup--p-anchor" rel="nofollow">convolutional highway notebook</a>.</p><p name="2a86" id="2a86" class="graf--p graf-after--p graf--last">More questions? Reach out to me <a href="https://twitter.com/jimmfleming" data-href="https://twitter.com/jimmfleming" class="markup--anchor markup--p-anchor" rel="nofollow">@jimmfleming</a>.</p></div></div></body></html>