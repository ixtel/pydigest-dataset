<html><body><div><article class="col-md-10 col-md-offset-1">
            

This is a post about image analysis using my new favorite Python import: <a href="http://scikit-image.org" title="scikit-image - collection of algorithms for image processing in Python." target="_blank"><code>scikit-image</code></a>.

<h1>Intro</h1>
<p>Take a couple words, alter them a bit and you've got a CAPTCHA. You've also got an image which is practically unidentifiable by even the most state of the art algorithms. Image analysis is hard, and even a simple task like distinguishing <a href="http://www.kaggle.com/c/dogs-vs-cats">cats from dogs</a> requires a large amount of graduate level mathematics.</p>
<p>Yet incredible progress has been made on these types of problems. One amazing use of machine vision has been for <a href="http://blog.yhathq.com/posts/quality-control-in-r.html" target="_blank">quality control</a> in manufacturing. For an industry that relies heavily on optimizing automated processes, image analysis has demonstrated extremely promising results, as noted by <a href="http://sightmachine.com/?utm_source=yhathq&amp;utm_medium=blog&amp;utm_content=textlink&amp;utm_campaign=skimage" title="Sight Machine - merging machine vision and Big Data to put you in control of quality" target="_blank">Sight Machine</a> CEO Jon Sobel in a recent WIRED article.</p>
<blockquote>
<p>"The new computer vision, liberated from its hardware shackles and empowered by connectivity, unlimited data storage and Big Data-style statistical analysis, is beginning to change the role of vision in manufacturing. Instead of being a reactive tool to detect defects, computer vision is becoming a data collection tool supporting defect prevention initiatives, improving understanding of complex processes, and enabling greater collaboration across entire supply chains in real time."</p>
</blockquote>
<blockquote>
<p>Jon Sobel; <a href="http://www.wired.com/insights/2014/01/liberating-machine-vision-machines/?utm_source=yhathq&amp;utm_medium=blog&amp;utm_content=textlink&amp;utm_campaign=skimage"><em>Liberating Machine Vision From the Machines</em></a></p>
</blockquote>
<p><sub><b>Note: </b>If you haven't heard of Sight Machine before, go watch the 2 min video on the homepage. Prepare for awesome.</sub></p>
<p>Machine vision makes contributions to the <a href="http://www.ri.cmu.edu/pub_files/pub4/matthies_larry_2007_1/matthies_larry_2007_1.pdf" target="_blank">mars rover</a>, analyzing MRIs, detecting structural inefficiency and energy loss in buildings and neighborhoods (<a href="http://www.essess.com/technology/scale/?utm_source=yhathq&amp;utm_medium=blog&amp;utm_content=textlink&amp;utm_campaign=skimage" title="Essess - thermal imaging technology" target="_blank">check out Essess</a>), and numerous consumer products. If you're going to buy a video game console in the near future, it's more likely than not to have some sort of image tracking mechanism built right in. Google now allows <a href="http://blog.yhathq.com/posts/digit-recognition-with-node-and-python.html">handwritten input</a> for a large portion of its services, and if you haven't spent an evening <a href="https://support.google.com/websearch/answer/166331">Google Goggling</a> everything in your apartment, I'd highly recommend it.</p>
<p><img src="http://3.bp.blogspot.com/--gjfTd-RRqE/UmXovHhqLsI/AAAAAAAABFw/cls8qfjr9m0/s1600/handwriting3.png"/></p>
<p><sup> Image source: http://googledrive.blogspot.com/2013/10/handwritingindocs.html </sup></p>
<p>Facebook passed <a href="http://investor.fb.com/annuals.cfm">240 billion</a> photos back in 2012. Instagram reached <a href="http://instagram.com/press/">16 billion</a> in it's three years as a company. Hubble made <a href="http://www.spacetelescope.org/about/faq/">a million</a> observations in a decade. Images haven't been left out of the recent data boom, and where there's data, there will always be data scientists ready to do something cool with it.</p>
<p>Unfortunately, for me, machine vision brings up a lot of time spent in a CS lab doing battle with MATLAB licenses. You can imagine how thrilled I was to see this on my timeline:</p>
<blockquote>
<blockquote class="twitter-tweet" lang="en"><p>
skimage - image processing in Python | <a href="http://t.co/hmxlktPXUW">http://t.co/hmxlktPXUW</a>
</p>
— yhat (<span class="citation">@YhatHQ</span>) <a href="https://twitter.com/YhatHQ/statuses/425651656923942913">January 21, 2014</a>
</blockquote>


</blockquote>
<p>Let's read in some images.</p>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<p class="prompt input_prompt">
In [1]:
</p>
<div class="input_area box-flex1">
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">skimage.io</span> <span class="kn">as</span> <span class="nn">io</span>
<span class="o">%</span><span class="k">matplotlib</span> <span class="n">inline</span>
</pre></div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<p class="prompt input_prompt">
In [3]:
</p>
<div class="input_area box-flex1">
<div class="highlight"><pre><span class="n">mandrill</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s">'mandrill.png'</span><span class="p">)</span>

<span class="n">io</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">mandrill</span><span class="p">)</span>
<span class="n">io</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">lenna</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s">'Lenna.png'</span><span class="p">)</span>
<span class="n">io</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">lenna</span><span class="p">)</span>
<span class="n">io</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

</div>
</div>



</div>



<p class="text_cell_render border-box-sizing rendered_html">
<h1>
Lenna and Mandrill
</h1>

</p>
<div class="text_cell_render border-box-sizing rendered_html">
<p>Mandrill and Lenna are two <a href="http://en.wikipedia.org/wiki/Standard_test_image" title="standard test image - wikipedia" target="_blank">classic specimens</a> used by researchers in image processing.</p>
<p>Lenna has a <a href="http://www.cs.cmu.edu/~chuck/lennapg/pcs_mirror/may_june01.pdf" title="" communication="" and="" an="" information="" age="" madonna="" target="_blank">weird and interesting history</a> involving a 1972 Playboy centerfold. It's worth a read. <a href="http://www.cs.cmu.edu/~chuck/lennapg/lenna.shtml" target="_blank">More Here</a></p>
<p>I could find out less about Mandrill, unfortunately. But according to <a href="https://groups.google.com/d/msg/sci.optics/0EKq5Hp5o90/IrYrPVGb9FgJ" title="google groups discussion about mandrill image processing sample photo" target="_blank">a thread on google groups</a>, Mandrill comes from a National Geographic that was lying around the lab and was chosen for it's range of colors. <a href="http://www.blendernation.com/2007/04/28/a-critical-history-of-computer-graphics-and-animation/" title="A Critical History of Computer Graphics and Animation" target="_blank">Read More</a></p>
</div>
<div class="text_cell_render border-box-sizing rendered_html">

<h2 id="why-image-processing">Why Image Processing?</h2>
<p>Emphasizing important traits and diluting noisy ones is the backbone of good feature design. In the context of machine vision, this means that image preprocessing plays a huge role. Before extracting features from an image, it's extremely useful to be able to augment it so that aspects which are important to the machine learning task stand out.</p>
<p><code>scikit-image</code> holds a wide library of image processing algorithms: filters, transforms, point detection. Frankly, it's wonderful that an open source package like this exists.</p>
<p>Doing pretty displays for this blog will require a little <code>matplotlib</code> customization. The following <code>pyplot</code> function takes a list of images and displays them side by side.</p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<p class="prompt input_prompt">
In [3]:
</p>
<div class="input_area box-flex1">
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">show_images</span><span class="p">(</span><span class="n">images</span><span class="p">,</span><span class="n">titles</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">"""Display a list of images"""</span>
    <span class="n">n_ims</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">titles</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span> <span class="n">titles</span> <span class="o">=</span> <span class="p">[</span><span class="s">'(</span><span class="si">%d</span><span class="s">)'</span> <span class="o">%</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n_ims</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">image</span><span class="p">,</span><span class="n">title</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">images</span><span class="p">,</span><span class="n">titles</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n_ims</span><span class="p">,</span><span class="n">n</span><span class="p">)</span> <span class="c"># Make subplot</span>
        <span class="k">if</span> <span class="n">image</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span> <span class="c"># Is image grayscale?</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">gray</span><span class="p">()</span> <span class="c"># Only place in this blog you can't replace 'gray' with 'grey'</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
        <span class="n">a</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">n</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">fig</span><span class="o">.</span><span class="n">get_size_inches</span><span class="p">())</span> <span class="o">*</span> <span class="n">n_ims</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

</div>
</div>

</div>
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="handling-colors">Handling Colors</h2>
<p>There are a lot of conventions with which to store colored images in computer memory, but the particular image I've imported uses the common <a href="http://en.wikipedia.org/wiki/RGB_color_model">RGB color model</a>, where each pixel holds intensity values for red, green, and blue. In Python images are just <code>numpy.ndarray</code>s. And though Python normally only allows for a handful of <a href="http://docs.python.org/2/library/stdtypes.html#numeric-types-int-float-long-complex">numeric types</a>, images use NumPy's wide array of data types to store each color in a 8 bit unsigned integer. Practically, this means that within the RGB convention, color values are restricted to the range 0 to 255 (<span class="math">\(2^0\)</span> to <span class="math">\(2^8 - 1\)</span>).</p>
<p>Because three color values need to be stored, color images require more than just a <span class="math">\(y\)</span> and <span class="math">\(x\)</span> dimension. Since a third dimension is added for color, to access a particular pixel's value for a RGB image, the following convention is used:</p>
<pre><code>image[ #ycoordinate , #xcoordinate , #red/green/blue ]</code></pre>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<p class="prompt input_prompt">
In [4]:
</p>
<div class="input_area box-flex1">
<div class="highlight"><pre><span class="c"># Create an image (10 x 10 pixels)</span>
<span class="n">rgb_image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span><span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span> <span class="c"># &lt;- unsigned 8 bit int</span>

<span class="n">rgb_image</span><span class="p">[:,:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">255</span> <span class="c"># Set red value for all pixels</span>
<span class="n">rgb_image</span><span class="p">[:,:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">255</span> <span class="c"># Set green value for all pixels</span>
<span class="n">rgb_image</span><span class="p">[:,:,</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>   <span class="c"># Set blue value for all pixels</span>

<span class="n">show_images</span><span class="p">(</span><span class="n">images</span><span class="o">=</span><span class="p">[</span><span class="n">rgb_image</span><span class="p">],</span><span class="n">titles</span><span class="o">=</span><span class="p">[</span><span class="s">"Red plus Green equals..."</span><span class="p">])</span>
</pre></div>

</div>
</div>



</div>


<div class="text_cell_render border-box-sizing rendered_html">
<p>Remember, there's no yellow coming from your computer monitor.</p>
<p><a href="http://docs.scipy.org/doc/numpy/reference/arrays.indexing.html">NumPy indexing</a> makes separating out each color layer of an image easy. The slicing operator '<code>:</code>' can be used to access all values of a specified dimension while a <code>tuple</code> is used to requests a subset. In order to view a particular color in isolation, I can set the other two values to 0 with these conventions.</p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<p class="prompt input_prompt">
In [5]:
</p>
<div class="input_area box-flex1">
<div class="highlight"><pre><span class="n">red</span><span class="p">,</span> <span class="n">green</span><span class="p">,</span> <span class="n">blue</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span> <span class="n">image</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span> <span class="n">image</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</pre></div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<p class="prompt input_prompt">
In [6]:
</p>
<div class="input_area box-flex1">
<div class="highlight"><pre><span class="n">red</span><span class="p">[:,:,(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">green</span><span class="p">[:,:,(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">blue</span><span class="p">[:,:,(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<p class="prompt input_prompt">
In [7]:
</p>
<div class="input_area box-flex1">
<div class="highlight"><pre><span class="n">show_images</span><span class="p">(</span><span class="n">images</span><span class="o">=</span><span class="p">[</span><span class="n">red</span><span class="p">,</span> <span class="n">green</span><span class="p">,</span> <span class="n">blue</span><span class="p">],</span> <span class="n">titles</span><span class="o">=</span><span class="p">[</span><span class="s">'Red Intensity'</span><span class="p">,</span> <span class="s">'Green Intensity'</span><span class="p">,</span> <span class="s">'Blue Intensity'</span><span class="p">])</span>
<span class="k">print</span> <span class="s">'Note: lighter areas correspond to higher intensities</span><span class="se">\n</span><span class="s">'</span>
</pre></div>

</div>
</div>

<div class="cell border-box-sizing code_cell vbox">


<div class="vbox output_wrapper">
<div class="output vbox">




<div class="hbox output_area"><p class="prompt"/>
<div class="box-flex1 output_subarea output_stream output_stdout">
<pre>
Note: Lighter areas correspond to higher intensities

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you've been wondering what's wrong with my axes, image conventions dictate that the coordinate <span class="math">\((0,0)\)</span> is in the top left of the image, meaning the <span class="math">\(y\)</span>-axis is reversed from "normal" (blasphemous, I know).</p>
<h2 id="grayscaling">Grayscaling</h2>
<p>Most image processing algorithms assume a two dimensional matrix, not an image with the third dimension of color. To bring the image into two dimensions, we need to summarize the three colors into a single value. This process is more commonly know as grayscaling, where the resulting image only holds different intensities of gray. The <code>skimage</code> module <code>color</code> has the function to do this (even with your preferred spelling of 'gray'/'grey'). The user needn't worry about how to weight each color when producing gray, there's already a <a href="http://www.w3.org/Graphics/Color/sRGB">standard</a> for this conversion.</p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<p class="prompt input_prompt">
In [8]:
</p>
<div class="input_area box-flex1">
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">skimage.color</span> <span class="kn">import</span> <span class="n">rgb2gray</span>

<span class="n">gray_image</span> <span class="o">=</span> <span class="n">rgb2gray</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="n">show_images</span><span class="p">(</span><span class="n">images</span><span class="o">=</span><span class="p">[</span><span class="n">image</span><span class="p">,</span><span class="n">gray_image</span><span class="p">],</span>
            <span class="n">titles</span><span class="o">=</span><span class="p">[</span><span class="s">"Color"</span><span class="p">,</span><span class="s">"Grayscale"</span><span class="p">])</span>

<span class="k">print</span> <span class="s">"Colored image shape:</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">image</span><span class="o">.</span><span class="n">shape</span>
<span class="k">print</span> <span class="s">"Grayscale image  shape:</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">gray_image</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

</div>
</div>

<div class="vbox output_wrapper">
<div class="output vbox">




<div class="hbox output_area"><p class="prompt"/>
<div class="box-flex1 output_subarea output_stream output_stdout">
<pre>
Colored image shape:
(480, 480, 3)
Grayscale image  shape:
(480, 480)

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="text_cell_render border-box-sizing rendered_html">
<p>As demonstrated, grayscaling doesn't just alter the color scheme, it fully reduces the dimensionality of the image. So with grayscale I can access a pixel value just by specifying an <span class="math">\((x,y)\)</span> coordinate. Now that the image has been reduced to 2D there are a number of transformations and filters which <code>skimage</code> provides.</p>
<p>Each pixel of a grayscale image holds a float value between <span class="math">\(0\)</span> and <span class="math">\(1\)</span>. However, this doesn't mean that a given image will use that entire spectrum, and the distribution will most commonly clump around specific values.</p>
</div>
<div class="text_cell_render border-box-sizing rendered_html">
<h1>
Histogram equalization
</h1>
<p><a href="http://en.wikipedia.org/wiki/Histogram_equalization" title="histogram equalization on wikipedia" target="_blank">Histogram equalization</a> takes a grayscale image and attempts to distribute intensity more evenly along the range of possible values. Pixels still rank the same (a coordinate with a higher value than another will still have a higher value after the transform), but the image as a whole becomes far more contrasted and normalized.</p>
</div>

<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<p class="prompt input_prompt">
In [9]:
</p>
<div class="input_area box-flex1">
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">skimage.exposure</span> <span class="kn">import</span> <span class="n">equalize_hist</span>

<span class="n">equalized_image</span> <span class="o">=</span> <span class="n">equalize_hist</span><span class="p">(</span><span class="n">gray_image</span><span class="p">)</span>

<span class="n">show_images</span><span class="p">(</span><span class="n">images</span><span class="o">=</span><span class="p">[</span><span class="n">gray_image</span><span class="p">,</span><span class="n">equalized_image</span><span class="p">],</span>
            <span class="n">titles</span><span class="o">=</span><span class="p">[</span><span class="s">"Grayscale"</span><span class="p">,</span><span class="s">"Histogram Equalized"</span><span class="p">])</span>
</pre></div>

</div>
</div>



</div>
<div class="text_cell_render border-box-sizing rendered_html">
<p>Drawing the density plots show that the original grayscale image (green density plot) only used intensities between ~<span class="math">\(0.05\)</span> and ~<span class="math">\(0.8\)</span>. That's a whole lot of the possible values not being use. The resulting image (orange density plot) is also distributed far more evenly, causing pixels which started quite close in intensity, which were therefore relatively hard to differentiate, to be more distinctly separated.</p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<p class="prompt input_prompt">
In [10]:
</p>
<div class="input_area box-flex1">
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">ggplot</span> <span class="kn">import</span> <span class="o">*</span>

<span class="n">ggplot</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(),</span><span class="n">aes</span><span class="p">(</span><span class="n">fill</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">))</span> <span class="o">+</span> \
    <span class="n">geom_density</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">gray_image</span><span class="o">.</span><span class="n">flatten</span><span class="p">()),</span><span class="n">color</span><span class="o">=</span><span class="s">'green'</span><span class="p">)</span> <span class="o">+</span> \
    <span class="n">geom_density</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">equalized_image</span><span class="o">.</span><span class="n">flatten</span><span class="p">()),</span><span class="n">color</span><span class="o">=</span><span class="s">'orange'</span><span class="p">)</span> <span class="o">+</span> \
    <span class="n">ggtitle</span><span class="p">(</span><span class="s">"Histogram Equalization Process</span><span class="se">\n</span><span class="s">(From Green to Orange)"</span><span class="p">)</span> <span class="o">+</span> \
    <span class="n">xlab</span><span class="p">(</span><span class="s">"pixel intensity"</span><span class="p">)</span> <span class="o">+</span> \
    <span class="n">ylab</span><span class="p">(</span><span class="s">"density"</span><span class="p">)</span>
</pre></div>

</div>
</div>



</div>
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="binarizing-and-blurring">Binarizing and Blurring</h2>
<p>Sometimes, it helps to simplify an image even further than grayscale by binarizing it. Binarization results in each pixel hold only one of two values (more commonly recognized as a pure black and white image). The object is to separate the foreground from the background to make feature generation even easier. A very simple way of doing this is to just choose a threshold, putting every pixel above that value to 1, and every pixel below it to 0. For this demonstration, I've chosen the mean of the grayscale image. So every pixel above the mean is set to white and those below are set to black. Since <code>skimage</code> images are just multi dimensional <code>numpy</code> arrays, a <code>np.where</code> statement is enough to accomplish this goal.</p>
<p>More complex binarization methods put far more thought into this process than I'm exercising here. For instance, the threshold can be set to a local minimum somewhere near the center of pixel distribution. This makes the splitting a little less arbitrary. Differences in illumination can also confuse a thresholding method, because illuminated foreground objects will hold very different pixel values than those that are not. In those cases, it may be more appropriate to consider local subimages and binarize there rather than interpreting the whole image at once.</p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<p class="prompt input_prompt">
In [11]:
</p>
<div class="input_area box-flex1">
<div class="highlight"><pre><span class="n">binary_image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">gray_image</span> <span class="o">&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">gray_image</span><span class="p">),</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">)</span>

<span class="n">show_images</span><span class="p">(</span><span class="n">images</span><span class="o">=</span><span class="p">[</span><span class="n">gray_image</span><span class="p">,</span><span class="n">binary_image</span><span class="p">],</span>
            <span class="n">titles</span><span class="o">=</span><span class="p">[</span><span class="s">"Grayscale"</span><span class="p">,</span><span class="s">"Binary"</span><span class="p">])</span>
</pre></div>

</div>
</div>



</div>
<div class="text_cell_render border-box-sizing rendered_html">
<p>An issue for the image above is that the binary may capture more detail than is helpful. For instance if the objective is to identify prominent features of the face, the position of every whisker isn't necessary. Blurring the image seems like a reasonable alternative. Scikit-image's Gaussian filter takes a weighted average of surrounding coordinates so individual pixels incorporate local intensities into their own. This produces a pretty recognizable blur effect. Raising the <code>sigma</code> argument will use a wider "window" of surrounding pixels, resulting in greater blur.</p>
<p>The equalized image (rather than the pure grayscale) has been used to maintain a high level of contrast through the filtering.</p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<p class="prompt input_prompt">
In [12]:
</p>
<div class="input_area box-flex1">
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">skimage.filter</span> <span class="kn">import</span> <span class="n">gaussian_filter</span>

<span class="n">blurred_image</span> <span class="o">=</span> <span class="n">gaussian_filter</span><span class="p">(</span><span class="n">equalized_image</span><span class="p">,</span><span class="n">sigma</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">really_blurred_image</span> <span class="o">=</span> <span class="n">gaussian_filter</span><span class="p">(</span><span class="n">equalized_image</span><span class="p">,</span><span class="n">sigma</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>

<span class="n">show_images</span><span class="p">(</span><span class="n">images</span><span class="o">=</span><span class="p">[</span><span class="n">equalized_image</span><span class="p">,</span><span class="n">blurred_image</span><span class="p">,</span><span class="n">really_blurred_image</span><span class="p">],</span>
            <span class="n">titles</span><span class="o">=</span><span class="p">[</span><span class="s">"Equalized"</span><span class="p">,</span><span class="s">"3 Sigma Blur"</span><span class="p">,</span><span class="s">"6 Sigma Blur"</span><span class="p">])</span>
</pre></div>

</div>
</div>



</div>
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="detecting-corners">Detecting Corners</h2>
<p>Ultimately, images have to be summarized somehow before feeding them to a machine learning algorithm. Many strategies use point of interest detection, where an algorithm is able to say "<em>Hey there's something special here!</em>" A commonly used flavor of these methods is corner detection. Scikit-image has a whole bunch of algorithms, but I've chosen the <a href="http://en.wikipedia.org/wiki/Corner_detection#The_Harris_.26_Stephens_.2F_Plessey_.2F_Shi.E2.80.93Tomasi_corner_detection_algorithm">Harris method</a> for its simplicity. The Harris algorithm itself doesn't produce corners, but returns a response matrix holding confidence values (the higher the value, the more likely that coordinate is a corner). The <code>corner_peaks()</code> function then turns those values into a list of coordinates.</p>
<p>Another <code>pyplot</code> function is necessary to plot corner coordinates on top of an image.</p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<p class="prompt input_prompt">
In [13]:
</p>
<div class="input_area box-flex1">
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">skimage.feature</span> <span class="kn">import</span> <span class="n">corner_harris</span><span class="p">,</span><span class="n">corner_peaks</span>

<span class="c"># More pyplot!</span>
<span class="k">def</span> <span class="nf">show_corners</span><span class="p">(</span><span class="n">corners</span><span class="p">,</span><span class="n">image</span><span class="p">,</span><span class="n">title</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">"""Display a list of corners overlapping an image"""</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    <span class="c"># Convert coordinates to x and y lists</span>
    <span class="n">y_corner</span><span class="p">,</span><span class="n">x_corner</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">corners</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_corner</span><span class="p">,</span><span class="n">y_corner</span><span class="p">,</span><span class="s">'o'</span><span class="p">)</span> <span class="c"># Plot corners</span>
    <span class="k">if</span> <span class="n">title</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">0</span><span class="p">)</span> <span class="c"># Images use weird axes</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">fig</span><span class="o">.</span><span class="n">get_size_inches</span><span class="p">())</span> <span class="o">*</span> <span class="mf">1.5</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">print</span> <span class="s">"Number of corners:"</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">corners</span><span class="p">)</span>

<span class="c"># Make a checker board</span>
<span class="n">checkers</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span><span class="mi">100</span><span class="p">),</span><span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
<span class="n">ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))[::</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">checkers</span><span class="p">[</span><span class="n">ind</span><span class="p">,:]</span> <span class="o">=</span> <span class="bp">True</span>
<span class="n">checkers</span><span class="p">[:,</span><span class="n">ind</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">invert</span><span class="p">(</span><span class="n">checkers</span><span class="p">[:,</span><span class="n">ind</span><span class="p">])</span>
<span class="n">checkers</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">checkers</span><span class="p">,</span><span class="mf">1.</span><span class="p">,</span><span class="mf">0.</span><span class="p">)</span>

<span class="c"># Run Harris</span>
<span class="n">checkers_corners</span> <span class="o">=</span> <span class="n">corner_peaks</span><span class="p">(</span><span class="n">corner_harris</span><span class="p">(</span><span class="n">checkers</span><span class="p">),</span><span class="n">min_distance</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">show_corners</span><span class="p">(</span><span class="n">checkers_corners</span><span class="p">,</span><span class="n">checkers</span><span class="p">)</span>
</pre></div>

</div>
</div>



</div>
<div class="text_cell_render border-box-sizing rendered_html">
<p>As one would hope, corner detection works well with simple examples like the one above. How does the algorithm do with the original image?</p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<p class="prompt input_prompt">
In [14]:
</p>
<div class="input_area box-flex1">
<div class="highlight"><pre><span class="n">corners</span> <span class="o">=</span> <span class="n">corner_peaks</span><span class="p">(</span><span class="n">corner_harris</span><span class="p">(</span><span class="n">image</span><span class="p">),</span><span class="n">min_distance</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">show_corners</span><span class="p">(</span><span class="n">corners</span><span class="p">,</span><span class="n">image</span><span class="p">,</span>
             <span class="n">title</span><span class="o">=</span><span class="s">"Harris Corner Algorithm"</span><span class="p">)</span>
</pre></div>

</div>
</div>



</div>
<div class="text_cell_render border-box-sizing rendered_html">
<p>For certain tasks like image alignment, its best when corner detection picks up prominent and constant meta features like the eyes or noise. However, when running Harris on the original image many of the hairs and whiskers were (rightly) detected as corners. In this case less detail is better, and running Harris on the blurred image seems like a reasonable alternative.</p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<p class="prompt input_prompt">
In [15]:
</p>
<div class="input_area box-flex1">
<div class="highlight"><pre><span class="n">blurred_corners</span> <span class="o">=</span> <span class="n">corner_peaks</span><span class="p">(</span><span class="n">corner_harris</span><span class="p">(</span><span class="n">blurred_image</span><span class="p">),</span><span class="n">min_distance</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
<span class="n">show_corners</span><span class="p">(</span><span class="n">blurred_corners</span><span class="p">,</span><span class="n">blurred_image</span><span class="p">,</span>
             <span class="n">title</span><span class="o">=</span><span class="s">"Harris Corner Algorithm on Binary Blurred"</span><span class="p">)</span>
</pre></div>

</div>
</div>



</div>
<div class="text_cell_render border-box-sizing rendered_html">
<p>Overlapping the original image with the corners detected in the blurred one, it becomes clear that Harris has guessed many more corners near prominent features.</p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<p class="prompt input_prompt">
In [16]:
</p>
<div class="input_area box-flex1">
<div class="highlight"><pre><span class="n">show_corners</span><span class="p">(</span><span class="n">blurred_corners</span><span class="p">,</span><span class="n">image</span><span class="p">,</span>
             <span class="n">title</span><span class="o">=</span><span class="s">"Blurred Corners Overlapping Original Image"</span><span class="p">)</span>
</pre></div>

</div>
</div>



</div>
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="how-the-harris-algorithm-works">How The Harris Algorithm Works</h2>
<p>Okay, so what just happened?</p>
<p>The Harris algorithm operates by considering gradients. Gradient images display information about the <em>rate of change</em> from one pixel to the next in either the horizontal or vertical directions. So if a pixel has a much higher intensity than the pixel before it, that pixel will have a large gradient value. The <a href="http://en.wikipedia.org/wiki/Sobel_operator">Sobel filter</a> is a good way of producing two gradients from a grayscale image.</p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<p class="prompt input_prompt">
In [14]:
</p>
<div class="input_area box-flex1">
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">skimage.filter</span> <span class="kn">import</span> <span class="n">hsobel</span><span class="p">,</span><span class="n">vsobel</span>

<span class="n">h_gradient</span> <span class="o">=</span> <span class="n">hsobel</span><span class="p">(</span><span class="n">gray_image</span><span class="p">)</span>
<span class="n">v_gradient</span> <span class="o">=</span> <span class="n">vsobel</span><span class="p">(</span><span class="n">gray_image</span><span class="p">)</span>

<span class="n">show_images</span><span class="p">(</span><span class="n">images</span><span class="o">=</span><span class="p">[</span><span class="n">gray_image</span><span class="p">,</span><span class="n">v_gradient</span><span class="p">,</span><span class="n">h_gradient</span><span class="p">],</span>
            <span class="n">titles</span><span class="o">=</span><span class="p">[</span><span class="s">"Grayscale"</span><span class="p">,</span><span class="s">"Vertical Gradient"</span><span class="p">,</span><span class="s">"Horizontal Gradient"</span><span class="p">])</span>
</pre></div>

</div>
</div>



</div>
<div class="text_cell_render border-box-sizing rendered_html">
<p>If a high gradient means a high rate of change in pixel intensity, we can consider the three following situations for any given <span class="math">\((x,y)\)</span> coordinate. 1. Low gradient level in both the horizontal and vertical direction. 2. High gradient level in either the horizontal or vertical direction but low in the other. 3. High gradient level in both the horizontal and vertical direction.</p>
<p>For situation (1) it's likely there's nothing of interest at that particular coordinate; the pixel is the same as the ones around it! (2) probably indicates an edge because the gradient corresponds to a lot of change in one direction but not the other. Therefore (3) would mean the meeting of two edges, which certainly sounds like a corner.</p>
<p>Alas, though the Harris corner algorithm is one of the simplest corner detectors, it uses quite complicated linear algebra to achieve this. Though I'd love to show off my <span class="math">\(\LaTeX\)</span> skills, instead what I'll do is pick coordinates with large values from each axis gradient and see which ones appear in both images. <strong>Just to be clear:</strong> this is an example for intuition, a pretty atrocious approximation, and if one of my old professors comes asking you didn't hear this from me.</p>
</div>
<div class="cell border-box-sizing code_cell vbox">
<div class="input hbox">
<p class="prompt input_prompt">
In [15]:
</p>
<div class="input_area box-flex1">
<div class="highlight"><pre><span class="n">top_n</span> <span class="o">=</span> <span class="mi">4000</span>
<span class="c"># Take coordinates of top n "largest" derivative values from each image</span>
<span class="n">h_large</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">h_gradient</span> <span class="o">&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">h_gradient</span><span class="o">.</span><span class="n">flatten</span><span class="p">())[</span><span class="o">-</span><span class="n">top_n</span><span class="p">]))</span>
<span class="n">v_large</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">v_gradient</span> <span class="o">&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">v_gradient</span><span class="o">.</span><span class="n">flatten</span><span class="p">())[</span><span class="o">-</span><span class="n">top_n</span><span class="p">]))</span>

<span class="c"># Intersection of coordinates</span>
<span class="n">estimated_corners</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">h_large</span><span class="p">)</span> <span class="o">&amp;</span> <span class="nb">set</span><span class="p">(</span><span class="n">v_large</span><span class="p">))</span>

<span class="n">show_corners</span><span class="p">(</span><span class="n">estimated_corners</span><span class="p">,</span><span class="n">gray_image</span><span class="p">)</span>
</pre></div>

</div>
</div>



</div>
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="what-now">What Now?</h2>
<p>Once these point of interests have been generated, they are used very differently depending on the task. For aligning two images or stitching several together (like in a panorama) simple <a href="http://en.wikipedia.org/wiki/Least_squares">least squares</a> optimization of the points can be sufficient. For image classification, more advanced summarizations are used. Though I won't be going over them in this blog, there are several links below if you're feeling ambitious!</p>
<h2 id="additional-reading">Additional Reading</h2>

</div>

        
    </div>
</article></div></body></html>