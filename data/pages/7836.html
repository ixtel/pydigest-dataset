<html><body><div><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-neural-style" class="anchor" href="#neural-style" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>neural-style</h1>

<p>An implementation of <a href="http://arxiv.org/pdf/1508.06576v2.pdf">neural style</a> in TensorFlow.</p>

<p>This implementation is a lot simpler than a lot of the other ones out there,
thanks to TensorFlow's really nice API and <a href="https://en.wikipedia.org/wiki/Automatic_differentiation">automatic differentiation</a>.</p>

<p>TensorFlow doesn't support <a href="https://en.wikipedia.org/wiki/Limited-memory_BFGS">L-BFGS</a> (which is what the original authors
used), so we use <a href="http://arxiv.org/abs/1412.6980">Adam</a>. This may require require a little bit more
hyperparameter tuning to get nice results.</p>

<p>TensorFlow seems to be <a href="https://github.com/soumith/convnet-benchmarks">slower</a> than a lot of the other
deep learning frameworks out there. I'm sure this implementation could be
improved, but it would probably take improvements in TensorFlow itself as well
to get it to operate at the same speed as other implementations. As of now, it
seems to be around 3x slower than implementations using Torch.</p>

<h2><a id="user-content-running" class="anchor" href="#running" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Running</h2>

<p><code>python neural_style.py --content &lt;content file&gt; --styles &lt;style file&gt; --output &lt;output file&gt;</code></p>

<p>(run <code>python neural_style.py --help</code> to see a list of all options)</p>

<h2><a id="user-content-example-1" class="anchor" href="#example-1" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Example 1</h2>

<p>Running it for 500-2000 iterations seems to produce nice results. With certain
images or output sizes, you might need some hyperparameter tuning (especially
<code>--content-weight</code>, <code>--style-weight</code>, and <code>--learning-rate</code>).</p>

<p>The following example was run for 1000 iterations to produce the result (with
default parameters):</p>

<p><a href="/anishathalye/neural-style/blob/master/examples/1-output.jpg" target="_blank"><img src="/anishathalye/neural-style/raw/master/examples/1-output.jpg" alt="output"/></a></p>

<p>These were the input images used (me sleeping at a hackathon and Starry Night):</p>

<p><a href="/anishathalye/neural-style/blob/master/examples/1-content.jpg" target="_blank"><img src="/anishathalye/neural-style/raw/master/examples/1-content.jpg" alt="input-content"/></a></p>

<p><a href="/anishathalye/neural-style/blob/master/examples/1-style.jpg" target="_blank"><img src="/anishathalye/neural-style/raw/master/examples/1-style.jpg" alt="input-style"/></a></p>

<h2><a id="user-content-example-2" class="anchor" href="#example-2" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Example 2</h2>

<p>The following example demonstrates style blending, and was run for 1000
iterations to produce the result (with style blend weight parameters 0.8 and
0.2):</p>

<p><a href="/anishathalye/neural-style/blob/master/examples/2-output.jpg" target="_blank"><img src="/anishathalye/neural-style/raw/master/examples/2-output.jpg" alt="output"/></a></p>

<p>The content input image was a picture of the Stata Center at MIT:</p>

<p><a href="/anishathalye/neural-style/blob/master/examples/2-content.jpg" target="_blank"><img src="/anishathalye/neural-style/raw/master/examples/2-content.jpg" alt="input-content"/></a></p>

<p>The style input images were Picasso's "Dora Maar" and Starry Night, with the
Picasso image having a style blend weight of 0.8 and Starry Night having a
style blend weight of 0.2:</p>

<p><a href="/anishathalye/neural-style/blob/master/examples/2-style1.jpg" target="_blank"><img src="/anishathalye/neural-style/raw/master/examples/2-style1.jpg" alt="input-style"/></a>
<a href="/anishathalye/neural-style/blob/master/examples/2-style2.jpg" target="_blank"><img src="/anishathalye/neural-style/raw/master/examples/2-style2.jpg" alt="input-style"/></a></p>

<h2><a id="user-content-requirements" class="anchor" href="#requirements" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Requirements</h2>



<h2><a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>License</h2>

<p>Copyright (c) 2015 Anish Athalye. Released under GPLv3. See
<a href="/anishathalye/neural-style/blob/master/LICENSE.txt">LICENSE.txt</a> for details.</p>
</article>
  </div></body></html>