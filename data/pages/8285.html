<html><body><div><section class="text-content">
          <p>Neural networks provide a vast array of functionality in the realm of statistical modeling, from data transformation to classification and regression. Unfortunately, due to the computational complexity and generally large magnitude of data involved, the training of so called <em>deep learning</em> models has been historically relegated to only those with considerable computing resources. However with the advancement of GPU computing, and now a large number of easy-to-use frameworks, training such networks is fully accessible to anybody with a simple knowledge of Python and a personal computer. In this post we’ll go through the process of training your first neural network in Python using an exceptionally readable framework called <a href="http://chainer.org">Chainer</a>. You can follow along this post through the tutorial here or via the <a href="https://github.com/stitchfix/Algorithms-Notebooks">Jupyter Notebook</a>.</p>

<p>We’ll start by making one of the simplest neural networks possible, a linear regression. Then we’ll move into more complex territory by training a model to classify the <a href="http://colah.github.io/posts/2014-10-Visualizing-MNIST/">MNIST</a> hand written digits dataset (a standard benchmark for deep learning models).</p>

<p>Should you wish to execute the code examples below, you will need to install Chainer, Matplotlib, and NumPy. These are easily installed through the Python package manager, <code class="highlighter-rouge">pip</code>.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>pip install numpy
<span class="gp">$ </span>pip install matplotlib
<span class="gp">$ </span>pip install chainer
</code></pre>
</div>

<p>Make sure to upgrade your version of Cython if you have previously installed it or have it installed via Anaconda. If you don’t do this you may receive a cryptic error during the Chainer installation process.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>pip install --upgrade cython
</code></pre>
</div>
<p>If you have <a href="http://jupyter.org">Jupyter</a> installed, then you can just download the notebook structured around this post and run it on your local machine. First clone the <a href="https://github.com/stitchfix/Algorithms-Notebooks">stitchfix/Algorithms-Notebooks</a> repository on GitHub and the notebook will be contained in the chainer-blog folder.</p>

<h1 id="part-1-basics-of-chainer">Part 1: Basics of Chainer</h1>
<p>To start, we will begin with a discussion of the three basic objects in Chainer, the <code class="highlighter-rouge">chainer.Function</code>, <code class="highlighter-rouge">chainer.Optimizer</code> and the <code class="highlighter-rouge">chainer.Variable</code>.</p>

<p>First we need to import a few things, which will be used throughout the notebook, including Matplotlib, NumPy, and some Chainer specific modules we will cover as we go.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c">#Matplotlib and NumPy</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="c">#Chainer Specific</span>
<span class="kn">from</span> <span class="nn">chainer</span> <span class="kn">import</span> <span class="n">FunctionSet</span><span class="p">,</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">optimizers</span><span class="p">,</span> <span class="n">serializers</span>
<span class="kn">import</span> <span class="nn">chainer.functions</span> <span class="kn">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">chainer.links</span> <span class="kn">as</span> <span class="nn">L</span>

</code></pre>
</div>

<h2 id="examining-chainer-variables-and-functions">Examining Chainer Variables and Functions</h2>
<p>Let’s begin by making two Chainer variables, which are just wrapped NumPy arrays, named <code class="highlighter-rouge">a</code> and <code class="highlighter-rouge">b</code>. We’ll make them one value arrays, so that we can do some scalar operations with them.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c"># Create 2 chainer variables then sum their squares</span>
<span class="c"># and assign it to a third variable.</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="o">**</span><span class="mi">2</span>
</code></pre>
</div>

<p>In Chainer, <code class="highlighter-rouge">Variable</code> objects are both part symbolic and part numeric. They contain values in the <code class="highlighter-rouge">data</code> attribute, but also information about the chain of operations that has been performed on them. This history is extremely useful when trying to train neural networks because by calling the <code class="highlighter-rouge">backward()</code> method on a variable we can perform backpropagation or (reverse-mode) auto-differentiation, which provides our chosen optimizer with all the information needed to successfully update the weights of our neural networks.</p>

<p>This process can happen, because Chainer variables store the sequence of Chainer functions that act upon them and each function contains an analytic expression of its derivative. Some functions you will use will be parametric and contained in <code class="highlighter-rouge">chainer.links</code> (imported here as <code class="highlighter-rouge">L</code>). These types of functions are the ones whose parameters will be updated with each training iteration of our network. Other functions will be non-parametric, contained in <code class="highlighter-rouge">chainer.functions</code> (imported here as <code class="highlighter-rouge">F</code>), and merely perform a predefined mathematical operation on a variable. Even the addition and subtraction operations are promoted to Chainer functions and the history of operations of each variable is stored as part of the variable itself. This allows us to calculate derivatives of any variable with respect to any another.</p>

<p>Let’s see how this works by example. Below we will:</p>

<ol>
  <li>
    <p>Inspect the value of our previously defined variables above by using the data attribute.</p>
  </li>
  <li>
    <p>Backpropagate using the <code class="highlighter-rouge">backward()</code> method on the third variable, <code class="highlighter-rouge">c</code>.</p>
  </li>
  <li>
    <p>Inspect the derivatives of the variables stored in the <code class="highlighter-rouge">grad</code> attribute.</p>
  </li>
</ol>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c"># Inspect the value of your variables.</span>
<span class="k">print</span><span class="p">(</span><span class="s">"a.data: {0}, b.data: {1}, c.data: {2}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">c</span><span class="o">.</span><span class="n">data</span><span class="p">))</span>
</code></pre>
</div>

<div class="output">
    
    <div align="center">
            <pre>a.data: [ 3.], b.data: [ 4.], c.data: [ 25.]</pre>
    </div>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c"># Now call backward() on the sum of squares.</span>

<span class="n">c</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>


<span class="c"># And inspect the gradients.</span>

<span class="k">print</span><span class="p">(</span><span class="s">"dc/da = {0}, dc/db = {1}, dc/dc = {2}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">c</span><span class="o">.</span><span class="n">grad</span><span class="p">))</span>
</code></pre>
</div>

<div class="output">
    
    <div align="center">
            <pre>dc/da = [ 6.], dc/db = [ 8.], dc/dc = [ 1.]</pre>
    </div>
</div>

<h1 id="part-2-doing-linear-regression-in-chainer">Part 2: Doing Linear Regression in Chainer</h1>

<p>Now that we know a little bit about the basics of what Chainer is doing, let’s use it to train the most basic possible neural network, a linear regression network. Of course the solution to the least squares optimization involved here can be calculated analytically via the normal equations much more efficiently, but this process will demonstrate the basic components of each network you’ll train going forward.</p>

<p>This network has no <em>hidden</em> units and involves just one input node, one output node, and a linear function linking the two of them.</p>

<p>To train our regression we will go through the following steps:</p>

<ol>
  <li>
    <p>Generate random linearly dependent datasets.</p>
  </li>
  <li>
    <p>Construct a function to perform a forward pass through the network with a Chainer Link.</p>
  </li>
  <li>
    <p>Make a function to do the network training.</p>
  </li>
</ol>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c"># Generate linearly related datasets x and y.</span>

<span class="n">x</span> <span class="o">=</span> <span class="mi">30</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">7</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="mi">10</span>
<span class="n">y</span> <span class="o">+=</span> <span class="mi">10</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'x'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'y'</span><span class="p">)</span>
</code></pre>
</div>

<p><img src="/assets/images/blog/chainer-post/line_scatter.png" alt="image"/></p>

<p>In general, the structure you’ll want to keep common to all neural networks that you make in Chainer involves making a <code class="highlighter-rouge">forward</code> function, which takes in your different parametric link functions and runs the data through them all in sequence.</p>

<p>Then writing a <code class="highlighter-rouge">train</code> function, which runs that forward pass over your batches of data for a number of full passes through the data called <em>epochs</em>, and then after each forward pass calculates a specified loss/objective function and updates the weights of the network using an <code class="highlighter-rouge">optimizer</code> and the gradients calculated through the <code class="highlighter-rouge">backward</code> method.</p>

<p>At the start Chainer practitioners often define the structure they’ll be working with by specifying the Link layers (here we’ll only need one). Then they will specify an optimizer to use, by instantiating one of the optimizer classes. Finally, they’ll tell the optimizer to keep track of and update the parameters of the specified model layers by calling the setup method on the optimizer instance with the layer to be tracked as an argument.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c"># Setup linear link from one variable to another.</span>

<span class="n">linear_function</span> <span class="o">=</span> <span class="n">L</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="c"># Set x and y as chainer variables, make sure to reshape</span>
<span class="c"># them to give one value at a time.</span>
<span class="n">x_var</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">y_var</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

<span class="c"># Setup the optimizer.</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizers</span><span class="o">.</span><span class="n">MomentumSGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">linear_function</span><span class="p">)</span>

<span class="c"># Define a forward pass function taking the data as input.</span>
<span class="c"># and the linear function as output.</span>
<span class="k">def</span> <span class="nf">linear_forward</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">linear_function</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>


<span class="c"># Define a training function given the input data, target data,</span>
<span class="c"># and number of epochs to train over.</span>
<span class="k">def</span> <span class="nf">linear_train</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span><span class="n">n_epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">):</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="c"># Get the result of the forward pass.    </span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">linear_forward</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>

        <span class="c"># Calculate the loss between the training data and target data.</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">train_target</span><span class="p">,</span><span class="n">output</span><span class="p">)</span>

        <span class="c"># Zero all gradients before updating them.</span>
        <span class="n">linear_function</span><span class="o">.</span><span class="n">zerograds</span><span class="p">()</span>

        <span class="c"># Calculate and update all gradients.</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c"># Use the optmizer to move all parameters of the network</span>
        <span class="c"># to values which will reduce the loss.</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
</code></pre>
</div>

<h2 id="plot-results-during-training">Plot Results During Training</h2>

<p>The code below will train the model for 5 epochs at a time and plot the line which is given by the current network parameters in the linear link. You can see how the model converges to the correct solution as the line starts out as light blue and moves toward its magenta final state.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c"># This code is supplied to visualize your results.</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">150</span><span class="p">):</span>    
    <span class="n">linear_train</span><span class="p">(</span><span class="n">x_var</span><span class="p">,</span> <span class="n">y_var</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">linear_forward</span><span class="p">(</span><span class="n">x_var</span><span class="p">)</span><span class="o">.</span><span class="n">data</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">cool</span><span class="p">(</span><span class="n">i</span><span class="o">/</span><span class="mf">150.</span><span class="p">),</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.4</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span><span class="mi">3</span><span class="p">)</span>


<span class="n">slope</span> <span class="o">=</span> <span class="n">linear_function</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="n">intercept</span> <span class="o">=</span> <span class="n">linear_function</span><span class="o">.</span><span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Final Line: {0:.3}x + {1:.3}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">slope</span><span class="p">,</span> <span class="n">intercept</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'x'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'y'</span><span class="p">)</span>
</code></pre>
</div>

<p><img src="/assets/images/blog/chainer-post/line_fit.png" alt="image"/></p>

<h2 id="bonus-points">BONUS POINTS</h2>
<p>If you’ve been following along doing your own code executions, try turning the above linear regression into a logistic regression. Use the following binary dataset for your dependent variable.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="n">y_log</span> <span class="o">=</span> <span class="n">y</span><span class="o">&gt;</span><span class="n">y</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="n">y_var</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">y_log</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</code></pre>
</div>

<h3 id="hint">HINT</h3>
<p>You’ll want to make use of the <code class="highlighter-rouge">F.sigmoid()</code> and <code class="highlighter-rouge">F.sigmoid_cross_entropy()</code> functions.</p>

<h1 id="part-3-train-a-handwritten-digit-classifier">Part 3: Train a Handwritten Digit Classifier</h1>

<p>For this third and final part we will use the MNIST handwritten digit dataset and try to classify the correct numerical value written in a \(28\times28\) pixel image. This is a classic benchmark for supervised deep learning architectures.</p>

<p>For this problem we will alter our simple linear regressors by now including some <em>hidden</em> linear neural network layers as well as introducing some non-linearity through the use of a non-linear <em>activation function</em>. This type of architecture is called a <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">Multilayer Perceptron</a> (MLP), and has been around for quite some time. Let’s see how it performs on the task at hand.</p>

<p>The following code will take care of the downloading, importing, and structuring of the MNIST set for you. In order for this to work, you’ll need to download this <a href="https://raw.githubusercontent.com/pfnet/chainer/master/examples/mnist/data.py"><code class="highlighter-rouge">data.py</code></a> file from the Chainer <a href="https://github.com/pfnet/chainer">github repository</a> and place it in the root directory of your script/notebook.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c"># functions for importing the MNIST dataset.</span>
<span class="kn">import</span> <span class="nn">data</span>

<span class="c"># We'll first import the data as a variable mnist.</span>
<span class="c"># (If this is the first time you've run this function</span>
<span class="c"># it could take a minute or two)</span>

<span class="n">mnist</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">load_mnist_data</span><span class="p">()</span>
</code></pre>
</div>

<p>Now let’s inspect the images to see what they look like.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">example</span> <span class="o">=</span> <span class="n">mnist</span><span class="p">[</span><span class="s">'data'</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">mnist</span><span class="p">[</span><span class="s">'target'</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">example</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Target Number: {0}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">"off"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre>
</div>

<p><img src="/assets/images/blog/chainer-post/MNIST.png" alt="image"/></p>

<p>Finally, we will separate the feature and target datasets and then make a train/test in order to properly validate the model at the end.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c"># Separate the two parts of the MNIST dataset</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">mnist</span><span class="p">[</span><span class="s">'data'</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">/</span> <span class="mi">255</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">mnist</span><span class="p">[</span><span class="s">'target'</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

<span class="c"># Make a train/test split.</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="p">[</span><span class="mi">60000</span><span class="p">])</span>
<span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="p">[</span><span class="mi">60000</span><span class="p">])</span>
</code></pre>
</div>

<p>With that out of the way, we can focus on training our MLP. Like it’s namesake implies, the MLP has a number of different layers and Chainer has a nice way to wrap up the layers of our network so that they can all be bundled together into a single object.</p>



<p>This handy object takes keyword arguments of named layers in the network so that we can later reference them. It works like this.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>model = FunctionSet(layer1=&lt;place link here&gt;, layer2=&lt;place link here&gt;, ...etc.)
</code></pre>
</div>

<p>Then the layers exist as properties of the class instance and all can be simultaneously told to be optimized by feeding the FunctionSet instance to the optimizers setup method:
<code class="highlighter-rouge">
optimizer.setup(model)
</code></p>

<p>With that tip, let’s move on to structuring our classifier. We need to get from a \(28\times28\) pixel image down to a 10-dimenisional <a href="https://en.wikipedia.org/wiki/Simplex">simplex</a> (where all the numbers in the output sum to one). In the output each dimension will represent a probability of the image being a specific digit according to our MLP classifier.</p>

<h2 id="the-mlp-structure">The MLP Structure</h2>
<p>To make it simple here, let’s start by including only 3 link layers in our network (you should feel free to mess around with this at your leisure later though to see what changes make for better classifier performance).</p>

<p>We’ll need a link taking the input images which are \(28 \times 28=784\) down to some other (probably lower) dimension, then a link stepping down the dimension even further, and finally we want to end up stepping down to the 10 dimensions at the end (with the constraint that they sum to 1).</p>

<p>Additionally, since compositions of <a href="https://en.wikipedia.org/wiki/Linear_function">linear functions</a> are linear and the benefit of deep learning models are their ability to approximate arbitrary nonlinear functions, it wouldn’t do us much good to stack repeated linear layers together without adding some nonlinear function to send them through&lt;/a&gt;<sup><a href="#footnote2">2</a></sup>.</p>

<p>So the forward pass we want will alternate between a linear transformation between data layers and a non-linear <a href="https://en.wikipedia.org/wiki/Activation_function"><em>activation function</em></a>. In this way the network can learn non-linear models of the data to (hopefully) better predict the target outcomes. Finally, we’ll use a <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a> <a href="https://en.wikipedia.org/wiki/Cross_entropy">cross-entropy</a> loss function&lt;/a&gt;<sup><a href="#footnote3">3</a></sup> to compare the vector output of the network to the integer target and then backpropagate based on the calculated loss.</p>

<p>Thus the final forward pass structure will look like:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>out = linear_layer1(data)
out = relu(out)
out = linear_layer2(out)
out = relu(out)
out = linear_layer3(out)
</code></pre>
</div>

<p>And when it comes time to train our model, with this size of data, we’ll want to batch process a number of samples and aggregate their loss collectively before updating the weights.</p>

<h3 id="define-the-model">Define the Model</h3>

<p>To start we define the model by declaring the set of links and the optimizer to be used during training.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c"># Declare the model layers together as a FunctionSet</span>
<span class="n">mnist_model</span> <span class="o">=</span> <span class="n">FunctionSet</span><span class="p">(</span>
                                  <span class="n">linear1</span><span class="o">=</span><span class="n">L</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">300</span><span class="p">),</span>
                                  <span class="n">linear2</span><span class="o">=</span><span class="n">L</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
                                  <span class="n">linear3</span><span class="o">=</span><span class="n">L</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
                         <span class="p">)</span>

<span class="c"># Instantiate an optimizer (you should probably use an</span>
<span class="c"># Adam optimizer here for best performance)</span>
<span class="c"># and then setup the optimizer on the FunctionSet.</span>
<span class="n">mnist_optimizer</span> <span class="o">=</span> <span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">()</span>
<span class="n">mnist_optimizer</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">mnist_model</span><span class="p">)</span>
</code></pre>
</div>

<h3 id="construct-training-functions">Construct Training Functions</h3>

<p>Now we construct the proper functions to structure the forward pass, define the batch processing for training, and predict the number represented in the MNIST images after training.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>
<span class="c"># Construct a forward pass through the network,</span>
<span class="c"># moving sequentially through a layer then activation function</span>
<span class="c"># as stated above.</span>
<span class="k">def</span> <span class="nf">mnist_forward</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>

    <span class="n">out1</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">out2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out1</span><span class="p">)</span>
    <span class="n">out3</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">out2</span><span class="p">)</span>
    <span class="n">out4</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out3</span><span class="p">)</span>
    <span class="n">final</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">linear3</span><span class="p">(</span><span class="n">out4</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">final</span>



<span class="c"># Make a training function which takes in training data and targets</span>
<span class="c"># as an input.</span>
<span class="k">def</span> <span class="nf">mnist_train</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">batchsize</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>

    <span class="n">data_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c"># loop over epochs</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'epoch </span><span class="si">%</span><span class="s">d'</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

        <span class="c"># randomly shuffle the indices of the training data</span>
        <span class="n">shuffler</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">data_size</span><span class="p">)</span>

        <span class="c"># loop over batches</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">data_size</span><span class="p">,</span> <span class="n">batchsize</span><span class="p">):</span>
            <span class="n">x_var</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">shuffler</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">batchsize</span><span class="p">]])</span>
            <span class="n">y_var</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">shuffler</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">batchsize</span><span class="p">]])</span>

            <span class="n">output</span> <span class="o">=</span> <span class="n">mnist_forward</span><span class="p">(</span><span class="n">x_var</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
            <span class="n">model</span><span class="o">.</span><span class="n">zerograds</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax_cross_entropy</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y_var</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">mnist_optimizer</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>

<span class="c"># Make a prediction function, using a softmax and argmax in order to</span>
<span class="c"># match the target space so that we can validate.</span>
<span class="k">def</span> <span class="nf">mnist_predict</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">mnist_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre>
</div>

<h3 id="train-the-model">Train the Model</h3>
<p>We can now train the network (I’d recommend a low number of epochs and a high batch size to start with in order to reduce training time, then these can be altered later to increase validation performance).</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="n">mnist_train</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">mnist_model</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</code></pre>
</div>

<h3 id="make-a-prediction">Make a Prediction</h3>
<p>The last thing we must do is validate the model on our held out set of test images to make sure that the model is not overfitting.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c"># Call your prediction function on the test set</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">mnist_predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">mnist_model</span><span class="p">)</span>

<span class="c"># Compare the prediction to the ground truth target values.</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">pred</span><span class="o">==</span><span class="n">y_test</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c"># Print out test accuracy</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Test accuracy: </span><span class="si">%</span><span class="s">f"</span> <span class="o">%</span> <span class="n">accuracy</span><span class="p">)</span>
</code></pre>
</div>


<p>Thus, by simply training for around 5 epochs&lt;/a&gt;<sup><a href="#footnote4">4</a></sup> we obtain a model that has an error rate of only 3.4% in testing!</p>

<h2 id="model-persistence">Model Persistence</h2>
<p>Should you wish to save your trained model for future use, Chainer’s most recent release provides the ability to serialize link elements and optimizer states into an hdf5 format via:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="n">serializers</span><span class="o">.</span><span class="n">save_hdf5</span><span class="p">(</span><span class="s">'test.model'</span><span class="p">,</span> <span class="n">mnist_model</span><span class="p">)</span>
<span class="n">serializers</span><span class="o">.</span><span class="n">save_hdf5</span><span class="p">(</span><span class="s">'test.state'</span><span class="p">,</span> <span class="n">mnist_optimizer</span><span class="p">)</span>
</code></pre>
</div>

<p>Then we can quickly restore the state of our previous model and resume training from that point by loading the serialized files.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>
<span class="n">serializers</span><span class="o">.</span><span class="n">load_hdf5</span><span class="p">(</span><span class="s">'my_model.model'</span><span class="p">,</span> <span class="n">model_name</span><span class="p">)</span>
<span class="n">serializers</span><span class="o">.</span><span class="n">load_hdf5</span><span class="p">(</span><span class="s">'my_optimizer.state'</span><span class="p">,</span> <span class="n">optimizer_name</span><span class="p">)</span>
</code></pre>
</div>

<h1 id="conclusion">Conclusion</h1>

<p>Over the course of this post you have hopefully gained an appreciation for the utility of using neural network frameworks like Chainer. It’s worth noting that the entire routine we developed for structuring and training a high-performance MNIST classifier with a 3.4% error rate was accomplished in just over 25 lines of code and around 10 seconds of CPU training time, in which it processed a total of 300,000 images. Of course, in my effort to prioritize simplicity I have opted for readability over elegance. Thus, many of the techniques above should be altered in practice beyond the learning stage. The <a href="http://docs.chainer.org/en/stable/">Chainer documentation</a> is rather extensive and covers both best practices and more advanced topics in greater detail, including modularity through object orienting.</p>

<hr/>

<p><span>
<a name="footnote1"/>
<sup>1</sup>
FunctionSet is now depricated in the most recent Chainer release, with the preferred alternative being to make your own class that inherits from the Chainer Chain class. However, in the interest of making this post as simple as possible I’ve opted to use the older method and wrap the links with FunctionSet. You should refer to Chainer’s own <a href="http://docs.chainer.org/en/stable/tutorial/basic.html" target="_blank">tutorial documentation</a>  for a more elegant treatment.
<a href="#footnote1-return">←</a>
</span></p>

<p><span>
<a name="footnote2"/>
<sup>2</sup>
We’ll use <a href="https://en.wikipedia.org/wiki/Rectifier_%28neural_networks%29">rectified linear units</a> for this example due to their computational efficiency and easy backpropagation.
<a href="#footnote2-return">←</a>
</span></p>

<p><span>
<a name="footnote3"/>
<sup>3</sup>
This will help impose the constraint that the output values should sum to one.
<a href="#footnote3-return">←</a>
</span></p>

<p><span>
<a name="footnote4"/>
<sup>4</sup>
This process takes under 10 seconds on my 3.1 GHz dual-core i7.
<a href="#footnote4-return">←</a>
</span></p>

        </section>
      </div></body></html>