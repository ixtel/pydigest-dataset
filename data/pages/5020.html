<html><body><div><div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Metropolis-Hastings-Algorithm">The Metropolis-Hastings Algorithm<a class="anchor-link" href="#The-Metropolis-Hastings-Algorithm">¶</a></h2><p>The key to success in applying the Gibbs sampler to the estimation of Bayesian posteriors is being able to specify the form of the complete conditionals of
${\bf \theta}$, because the algorithm cannot be implemented without them. In practice, the posterior conditionals cannot always be neatly specified.</p>
<p>Taking a different approach, the Metropolis-Hastings algorithm generates <strong><em>candidate</em></strong>  state transitions from an alternate distribution, and <em>accepts</em> or <em>rejects</em> each candidate probabilistically.</p>
<p>Let us first consider a simple Metropolis-Hastings algorithm for a single parameter, $\theta$. We will use a standard sampling distribution, referred to as the <em>proposal distribution</em>, to produce candidate variables $q_t(\theta^{\prime} | \theta)$. That is, the generated value, $\theta^{\prime}$, is a <em>possible</em> next value for
$\theta$ at step $t+1$. We also need to be able to calculate the probability of moving back to the original value from the candidate, or
$q_t(\theta | \theta^{\prime})$. These probabilistic ingredients are used to define an <em>acceptance ratio</em>:</p><p>
$$\begin{gathered}
\begin{split}a(\theta^{\prime},\theta) = \frac{q_t(\theta^{\prime} | \theta) \pi(\theta^{\prime})}{q_t(\theta | \theta^{\prime}) \pi(\theta)}\end{split}\notag\\\begin{split}\end{split}\notag\end{gathered}$$</p><p>The value of $\theta^{(t+1)}$ is then determined by:</p><p>
$$\theta^{(t+1)} = \left\{\begin{array}{l@{\quad \mbox{with prob.} \quad}l}\theta^{\prime} &amp; \text{with probability } \min(a(\theta^{\prime},\theta^{(t)}),1) \\ \theta^{(t)} &amp; \text{with probability } 1 - \min(a(\theta^{\prime},\theta^{(t)}),1) \end{array}\right.$$</p><p>This transition kernel implies that movement is not guaranteed at every step. It only occurs if the suggested transition is likely based on the acceptance ratio.</p>
<p>A single iteration of the Metropolis-Hastings algorithm proceeds as follows:</p>
<ol>
<li><p>Sample $\theta^{\prime}$ from $q(\theta^{\prime} | \theta^{(t)})$.</p>
</li>
<li><p>Generate a Uniform[0,1] random variate $u$.</p>
</li>
<li><p>If $a(\theta^{\prime},\theta) &gt; u$ then
$\theta^{(t+1)} = \theta^{\prime}$, otherwise
$\theta^{(t+1)} = \theta^{(t)}$.</p>
</li>
</ol>
<p>The original form of the algorithm specified by Metropolis required that
$q_t(\theta^{\prime} | \theta) = q_t(\theta | \theta^{\prime})$, which reduces $a(\theta^{\prime},\theta)$ to
$\pi(\theta^{\prime})/\pi(\theta)$, but this is not necessary. In either case, the state moves to high-density points in the distribution with high probability, and to low-density points with low probability. After convergence, the Metropolis-Hastings algorithm describes the full target posterior density, so all points are recurrent.</p>
<h3 id="Random-walk-Metropolis-Hastings">Random-walk Metropolis-Hastings<a class="anchor-link" href="#Random-walk-Metropolis-Hastings">¶</a></h3><p>A practical implementation of the Metropolis-Hastings algorithm makes use of a random-walk proposal.
Recall that a random walk is a Markov chain that evolves according to:</p><p>
$$
\theta^{(t+1)} = \theta^{(t)} + \epsilon_t \\
\epsilon_t \sim f(\phi)
$$</p><p>As applied to the MCMC sampling, the random walk is used as a proposal distribution, whereby dependent proposals are generated according to:</p><p>
$$\begin{gathered}
\begin{split}q(\theta^{\prime} | \theta^{(t)}) = f(\theta^{\prime} - \theta^{(t)}) = \theta^{(t)} + \epsilon_t\end{split}\notag\\\begin{split}\end{split}\notag\end{gathered}$$</p><p>Generally, the density generating $\epsilon_t$ is symmetric about zero,
resulting in a symmetric chain. Chain symmetry implies that
$q(\theta^{\prime} | \theta^{(t)}) = q(\theta^{(t)} | \theta^{\prime})$,
which reduces the Metropolis-Hastings acceptance ratio to:</p><p>
$$\begin{gathered}
\begin{split}a(\theta^{\prime},\theta) = \frac{\pi(\theta^{\prime})}{\pi(\theta)}\end{split}\notag\\\begin{split}\end{split}\notag\end{gathered}$$</p><p>The choice of the random walk distribution for $\epsilon_t$ is frequently a normal or Student’s $t$ density, but it may be any distribution that generates an irreducible proposal chain.</p>
<p>An important consideration is the specification of the <strong>scale parameter</strong> for the random walk error distribution. Large values produce random walk steps that are highly exploratory, but tend to produce proposal values in the tails of the target distribution, potentially resulting in very small acceptance rates. Conversely, small values tend to be accepted more frequently, since they tend to produce proposals close to the current parameter value, but may result in chains that <strong><em>mix</em></strong> very slowly.</p>
<p>Some simulation studies suggest optimal acceptance rates in the range of 20-50%. It is often worthwhile to optimize the proposal variance by iteratively adjusting its value, according to observed acceptance rates early in the MCMC simulation .</p>

</div>
</div></body></html>