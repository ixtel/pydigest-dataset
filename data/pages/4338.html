<html><body><div><div class="col-sm-12 post-content"> <p class="page-post-summary"> Whoosh is written in python. PyPy was designed to speed up python programs. Batch indexing with Whoosh could be a match made in Heaven. </p> <p><strong>Whoosh</strong> (<a href="https://bitbucket.org/mchaput/whoosh/wiki/Home">source</a>) is an embeddable search engine written in pure Python. <strong>PyPy</strong> (<a href="http://pypy.org/">homepage</a>) is a fast and compliant alternative runtime for the Python language. Its Just-In-Time compiler helps long running python tasks run faster in many situations than standard CPython. It also support for stackless support, consumes less memory overall and can be sand boxed for untrusted code.</p> <h3>Contents</h3> <ol> <li><a href="/blog/whoosh-python-vs-pypy-at-indexing.html#what">What is Indexing</a></li> <li><a href="/blog/whoosh-python-vs-pypy-at-indexing.html#why">Why is indexing slow?</a></li> <li><a href="/blog/whoosh-python-vs-pypy-at-indexing.html#speed">Speed up Indexing!</a></li> <li><a href="/blog/whoosh-python-vs-pypy-at-indexing.html#whyPyPy">Why should I use PyPy?</a></li> <li><a href="/blog/whoosh-python-vs-pypy-at-indexing.html#pyvpy">PyPy vs. Python</a></li> <li><a href="/blog/whoosh-python-vs-pypy-at-indexing.html#data">The Data</a></li> <li><a href="/blog/whoosh-python-vs-pypy-at-indexing.html#test">The Test</a></li> <li><a href="/blog/whoosh-python-vs-pypy-at-indexing.html#results">PyPy vs. Python: Results</a></li> <li><a href="/blog/whoosh-python-vs-pypy-at-indexing.html#conclusion">Conclusion</a></li> </ol> <p><a name="what"/></p> <h2>What is indexing?</h2> <p>The step of getting data into a search engine is called indexing. It performs all the necessary operations to turn the raw information into tokens that can be optimally stored for future retrieval.</p> <ol> <li>First the information is read/loaded in from an external source.</li> <li>Different analyzers take the data and coerce it into formats designed for different querying styles.</li> <li>Token filters will then take everything the analyzers found and apply transformations, such as lowercasing or stripping whitespace.</li> <li>The formatted tokens, organized into Documents, are then available to the search engine for referencing and linking to future data. They can be written to disk or stored in cache for awhile to continue indexing documents without going back and forth.</li> <li>When the documents are committed to the index optimizations and associations are formed to link new and existing records together. Optimizing the index is computationally heavy but reduces the file size and makes future queries faster. </li> </ol> <p><a name="why"/></p> <h2>Why is indexing slow?</h2> <p>Indexing is slow so querying is fast. The fastest method to index can be thought of doing nothing at all, and traversing the original content each time you search against it. By creating better indexes in advance future searches won't take as long, and better relationships between documents can be made.</p> <p>Indexing allows the content to be distributed to specific fields, as seen in the [Designing a Better Schema]/blog/whoosh-building-a-better-schema.html) post, which can also impact the index-vs-query relationship. <code>TEXT</code> fields are inherently more complex and larger because of the positional and fragment data stored in each document. Different analyzers will influence the token count and size, but anything with phrase or N-gram searching will always take more time to index.</p> <p>In Whoosh index commits are single threaded so large/complex data operations can take some time. Adding a single text field of 300 bytes to a dataset of 165,000 records will take 5-6x longer to index and increase the index size by more than 7x. Choosing which data is worth storing in the search index will help batch indexing. Remember that a Whoosh <code>Schema</code> can add more fields later. You can always process non-text data in advance and slowly add it in over time to get "something" available for searching right away.</p> <p><a name="speed"/></p> <h2>Speed Up Indexing</h2> <p>Indexing will always take some time. Better indexes will take more, and are larger, but will give more completeness to the available methods of querying the data.</p> <p>The easiest ways to speed up indexing with Whoosh:</p> <ol> <li>Add more processors and RAM. </li> </ol> <p>When retrieving the index writer, there are keyword arguments for <code>proc</code> and <code>limitmb</code>. <code>proc</code> is the number of processes that will be spawned to work on indexing. <code>limitmb</code> is the maximum memory assigned, before interpreter overhead, allotted <strong>per process</strong> for the purpose of index cache.</p> <p><em>Warning: this does not work on Windows. See <a href="https://bitbucket.org/mchaput/whoosh/issue/414/whooshwritersindexwriter-procs-1-does-not">Issue #414</a>.</em></p> <ol> <li>Use an alternative writer. </li> </ol> <p>In <code>whoosh.writing</code> there is an <code>AsyncWriter</code> and a <code>BufferedWriter</code>. The <code>AsyncWriter</code> will spawn a thread while committing to disk to the main thread can continue processing while it's written. This offered a significant performance boost for Whoosh on Windows; Linux was negligible. A <code>BufferedWriter</code> is used to periodically write new documents to the index. It has two constraints, <code>period</code> and <code>limit</code>. The first is the number of seconds between writing while the document threshold is below limit. <code>limit</code> is the number of records to commit, if available, before the period interval is reached.</p> <p>This is useful when writing many single documents quickly. It saves the overhead of constantly requesting a writer for each document.</p> <ol> <li>Use the <code>StemmingAnalyzer</code> cache.</li> </ol> <p>Indexing fields with the Stemming Analyzer have the capability to utilize its cache. Indexing speed decreases the larger the cache, however reaching the maximum cache can slow down indexing up to 200% compared to an unbounded cache. If a few hundred megabytes of RAM isn't a big deal, considering setting <code>cachesize=-1</code>.</p> <ol> <li>Use the <code>multisegment</code> parameter in the <code>writer</code>.</li> </ol> <p>When combined with multiple processes, the multisegment attribute will create at least one new sub-segment for each indexing process spawned. This allows for each writer to have an exclusive lock on their indexing segment preventing file access collisions. It's recommended to only use this in large batch processing, if no other indexing method is used the number of files will increase forever. Calling an optimize commit will re-merge the multisegment indexes.</p> <ol> <li>Index less data.</li> </ol> <p>Reducing the amount of data will affect indexing time. Reducing the complexity of the indexed data will reduce indexing time as well. More detail is covered in <a href="/blog/whoosh-building-a-better-schema.html">Designing a Better Schema</a>.</p> <ol> <li>Don't index on Windows.</li> </ol> <p>Spoiler alert, Whoosh doesn't work very good on Windows. They do some clever tricks with threads and processes, as well as using a lock file, that make a Windows environment sub-optimal. For smaller batches it worked ok, but it was slow. For larger batches the process would crash around 150k documents. The multi-process parameters don't work at all and the best python environment on Windows (3.4.3) had disappointing results for available modules. Harder to get the data in, harder to use, and it will probably fail anyway.</p> <ol> <li>Use PyPy!</li> </ol> <p>I'll tell you. :)</p> <p><a name="whyPyPy"/></p> <h2>Why should I use PyPy?</h2> <p>PyPy was designed for tasks exactly like this. Whoosh is entirely written in python, PyPy can only speed up code written in python...and it's <a href="http://speed.pypy.org/">really really good</a> at it. PyPy consumes less memory with the same data structures, allowing us to increase the memory allowed to the indexers; which has a direct relation to the overall time taken to index documents.</p> <p>The PyPy runtime is actively developed and it's getting faster all the time. If it makes your program faster today, chances of simply upgrading the binary for more performance are really good. There are executables for Windows, Linux and ARM -- it's small and simple enough to get to not even try.</p> <p><a name="pyvpy"/></p> <h2>PyPy vs. Python</h2> <p>Because it seemed like such a good fit I was ready to increase my confidence in PyPy or be completely shocked.</p> <p><a name="data"/></p> <h3>The Data</h3> <p>The data came from <a href="http://commoncrawl.org">commoncrawl.org</a>. They have monthly archives for the past couple years of web scraped information. The dataset I chose was from the <a href="http://blog.commoncrawl.org/2015/03/february-2015-crawl-archive-available/">February 2015 Crawl Archive</a> because it was the newest dump in their older WARC format. The specific dataset was for February 26th in the WARC files archive. Uncompressed it represents 4.3 GiB of HTTP requests, responses and environment meta data.</p> <p>The responses were mostly raw HTML, but had enough unique content that works could easily be extracted for the test.</p> <p><a name="test"/></p> <h3>The Test</h3> <p>Uncompressed and analyzed the WARC archive contained 167,395 records. The index tests were performed with an <code>IndexWriter</code> with different configurations. The test was designed to do multi-segment indexing and merge every 25,000 documents. After the final commit another commit where <code>optimize=True</code> was performed to clean up and solidify the index.</p> <p>The final commit took roughly ~40-50% of the test time. Without an optimized index the final results could be halved.</p> <h4><strong>Schema</strong></h4> <ul> <li>Checksum ID</li> <li>Numeric ID</li> <li>Content length of HTTP response</li> <li>Payload size of HTTP response, different from content length on non-utf8 charsets.</li> <li>HTML/JSON/etc payload as full-text index</li> <li>Response origin</li> <li>Indexed date (Whoosh)</li> <li>Archive date (WARC)</li> </ul> <p>class WarcSchema(SchemaClass): chk_id = ID(stored = True, unique = True) num_id = NUMERIC( unique = True, stored = True, numtype = int, bits = 32, signed = False)</p> <pre><code>content_length = NUMERIC(
    stored = True, numtype = int, bits = 64, signed = False)

payload_size = NUMERIC(
    stored = False, numtype = int, bits = 32, signed = False)

content_type = TEXT(stored = False)

payload = TEXT(
    stored = False,
    analyzer = StemmingAnalyzer(
        minsize = 3,
        cachesize = 1000 * 1000 * 5
    )
)

ip_address = ID(stored = True)

indx_date = DATETIME(stored = True)
arch_date = DATETIME(stored = True)
</code></pre> <h4><strong>Environment</strong></h4> <p>I originally tried running the tests on both Windows and Linux but gave up when none of them were finishing with memory errors. For smaller batch sizes the Windows indexes were fine, but for the relatively "small" 4.3 GiB I wasn't impressed.</p> <p>I also ran a series of tests comparing the performance of reading and writing from an SSD and a RAM-disk. However, the results were negligible so I abandoned those results as well and opted for SSD storage.</p> <p>Each index was created from scratch in a fresh environment with no artifacts of the previous runs.</p>  <p>CPython version <a href="https://www.python.org/downloads/">2.7.6</a>, PyPy version <a href="http://pypy.org/download.html">2.5.1</a>.</p> <p><a name="results"/></p> <h2>PyPy vs. Python: Results</h2> <p>The first tests that ran were on the CPython interpreter.</p> <p>Comparing 2 cores (orange) to 3 (green) as interesting. In the beginning when there were less than 80k documents in the index three processes were performing better, as expected. However, as soon as the index got larger it became worse with the two CPU runs beating the other by ~4%. This lends itself to optimization steps every 25k records. Individual commits are still single threaded leaving an additional core idle waiting for the main process to finish.</p> <p>Another surprising comparison is two cores with 1024 MiB of RAM (red) to 512 MiB RAM (orange). In the preliminary runs it was easy to see that Whoosh processes (specifically the <code>writers</code>) loved memory. They'd take all you could give. Up until the last step the 1024 instance was edging out the 512 by a few seconds...however, at the end of the process it didn't matter much. I ran the test twice and the results were within 1 second of each other, so I'm not quite sure why it would "give up" at the end when it had more resources all along.</p> <p>Enter PyPy.</p> <p>The dual core 512 MiB CPython (orange) compared to its PyPy twin (purple) was more interesting. </p> <p>For the first 25 thousand documents the CPython was, on average, 31% faster than PyPy. I expected PyPy to be a little slower for a few thousand documents, but not by that much. PyPy does have some warm up time to fully utilize its JIT, after execution paths are established it can determine patterns and optimize the code over time. The smallest disparity between the two was immediately before the optimization commit. CPython lead PyPy by ~1.3 seconds (or 18%). After the merge the next 1000 documents were clocked at 2.74x the elapsed runtime before writing. Although it took PyPy an extra second to reach the same milestone, it returned a single second faster at 2.21x the total runtime. Post commit PyPy had a 4.3% lead.</p> <p>PyPy hit the next commit at 50k documents after 24.95 seconds, CPython at 25.24 seconds. The optimized write took PyPy 22.17 seconds, while CPythons took 39.14 seconds; 43.3% longer.</p> <p>Over the next 100 thousand documents PyPy kept a lead between 9 %and 14%, aside from the 50-75k range where it was 33% faster.</p> <p>It hit the last unoptimized index 48 seconds sooner and 71 seconds faster after the final cleanup. Overall PyPy was <strong>7.7% faster than CPython with 512 MiB of RAM per indexing process while including the payload <code>TEXT</code> field.</strong></p> <p>Increasing the RAM to 1024 MiB (brown) allowed PyPy to finish after 12 minutes and 45 seconds. Compared to CPython's (red) 15 minutes 30 seconds, it finished 17.7% faster (2 minutes 45 seconds).</p> <p><img alt="Whoosh Indexing Times: WARC File, text payload." src="/mediafiles/image/Whoosh_Indexing_Times_-_WARC_File_text_payload..png"/></p> <p>Running the tests again without indexing the payload content we can see a very similar trend as was shown above. Notice that before the 125k document index operation PyPy with the full text field was beating meta data only indexing in CPython.</p> <p><img alt="Whoosh Indexing Times : WARC File, meta data only." src="/mediafiles/image/Whoosh_Indexing_Times_-_WARC_File_meta_data_only._1.png"/></p> <p><a name="conclusion"/></p> <h2>Conclusion</h2> <p>The data was pretty unique, but it was consistent between both runtimes. While indexing within my hardware limits it's sufficiently evident to me that PyPy is a much better implementation to run with Whoosh. A few percent over minutes and seconds isn't much time, but over hours and days it could make a huge difference. I have documented Whoosh scalability yet...so I can't, in good faith, endorse it for something that might take days to Index.</p> <p>Based on the patterns and reliability of ~30 indexing runs, I can confidently recommend Whoosh with PyPy on a system with more than one CPU, and as much RAM as you can give it. Utilizing a <code>StemmingAnalyzer</code> with cache instead of the default or other options can also reduce indexing time. Utilizing sub-segments with <code>multisegment=True</code> is also beneficial when paired with another indexing strategy to clean up the limitless files generated in your search index folders.</p> <p>Thanks!</p>  </div> </div></body></html>