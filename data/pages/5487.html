<html><body><div><div class="section-inner layoutSingleColumn"><p name="ae7e" id="ae7e" class="graf--p graf--first">With 1.4 version improvements, <a href="https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html" data-href="https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html" class="markup--anchor markup--p-anchor" rel="nofollow"><strong class="markup--strong markup--p-strong">Spark DataFrames</strong></a><strong class="markup--strong markup--p-strong"> could become the new Pandas</strong>, making <em class="markup--em markup--p-em">ancestral</em> <a href="https://ogirardot.wordpress.com/2015/05/29/rdds-are-the-new-bytecode-of-apache-spark/" data-href="https://ogirardot.wordpress.com/2015/05/29/rdds-are-the-new-bytecode-of-apache-spark/" class="markup--anchor markup--p-anchor" rel="nofollow">RDDs look like Bytecode</a>.</p><p name="95a2" id="95a2" class="graf--p graf-after--p">I use heavily Pandas (and Scikit-learn) for Kaggle competitions. <strong class="markup--strong markup--p-strong">Nobody won a Kaggle challenge with Spark yet</strong>, but I’m convinced it will happen. That’s why it’s time to prepare the future, and start using it.</p><p name="17c1" id="17c1" class="graf--p graf-after--p">Spark DataFrames are available in the <a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html" data-href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html" class="markup--anchor markup--p-anchor" rel="nofollow"><em class="markup--em markup--p-em">pyspark.sq</em></a><em class="markup--em markup--p-em">l</em> package (strange, and historical name : it’s no more only about SQL !).</p><p name="2711" id="2711" class="graf--p graf-after--p">I’m not a Spark specialist at all, but here are a few things I noticed when I had a first try. On my GitHub, you can find <a href="https://github.com/christophebourguignat/notebooks/blob/master/Spark-Pandas-Differences.ipynb" data-href="https://github.com/christophebourguignat/notebooks/blob/master/Spark-Pandas-Differences.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow"><strong class="markup--strong markup--p-strong">the IPython Notebook companion</strong></a><strong class="markup--strong markup--p-strong"> </strong>of this post.</p><figure name="cf13" id="cf13" class="graf--figure graf-after--p"><figcaption class="imageCaption">Spark DataFrames are available in the pyspark.sql package, and <a href="http://fr.slideshare.net/databricks/spark-sqlsse2015public" data-href="http://fr.slideshare.net/databricks/spark-sqlsse2015public" class="markup--anchor markup--figure-anchor" rel="nofollow">it’s not only about SQL</a></figcaption></figure><h4 name="3335" id="3335" class="graf--h4 graf-after--figure">Reading</h4><p name="ad64" id="ad64" class="graf--p graf-after--h4">With Pandas, you easily read CSV files with <em class="markup--em markup--p-em">read_csv()</em>.</p><p name="1bd7" id="1bd7" class="graf--p graf-after--p">Out of the box, Spark DataFrame supports reading data from popular <em class="markup--em markup--p-em">professional</em> formats, like JSON files, Parquet files, Hive table — be it from local file systems, distributed file systems (HDFS), cloud storage (S3), or external relational database systems.</p><figure name="e717" id="e717" class="graf--figure graf-after--p"><figcaption class="imageCaption">Spark natively supports several formats. Third-party extensions are needed for Avro, CSV, ElasticSearch, Cassandra</figcaption></figure><p name="4dda" id="4dda" class="graf--p graf-after--figure">But <strong class="markup--strong markup--p-strong">CSV is not supported natively by Spark</strong>. You have to use a separate library : <a href="https://github.com/databricks/spark-csv" data-href="https://github.com/databricks/spark-csv" class="markup--anchor markup--p-anchor" rel="nofollow">spark-csv</a>.</p><figure name="ae15" id="ae15" class="graf--figure graf-after--p"/><h4 name="f8d0" id="f8d0" class="graf--h4 graf-after--figure">Counting</h4><p name="8a2f" id="8a2f" class="graf--p graf-after--h4"><em class="markup--em markup--p-em">sparkDF.count()</em> and <em class="markup--em markup--p-em">pandasDF.count() </em>are not the exactly the same.</p><p name="8dec" id="8dec" class="graf--p graf-after--p">The first one returns the <strong class="markup--strong markup--p-strong">number of rows</strong>, and the second one returns the <strong class="markup--strong markup--p-strong">number of non NA/null</strong> observations for each column.</p><p name="7947" id="7947" class="graf--p graf-after--p">Not that Spark doesn’t support <em class="markup--em markup--p-em">.shape</em> yet — very often used in Pandas.</p><figure name="7820" id="7820" class="graf--figure graf-after--p"/><h4 name="7483" id="7483" class="graf--h4 graf-after--figure">Viewing</h4><p name="650f" id="650f" class="graf--p graf-after--h4">In Pandas, to have a tabular view of the content of a DataFrame, you typically use <em class="markup--em markup--p-em">pandasDF.head(5)</em>, or <em class="markup--em markup--p-em">pandasDF.tail(5)</em>. In IPython Notebooks, it displays a nice array with continuous borders.</p><p name="eb49" id="eb49" class="graf--p graf-after--p">In Spark, you have <em class="markup--em markup--p-em">sparkDF.head(5)</em>, but it has an <strong class="markup--strong markup--p-strong">ugly output</strong>. You should prefer <em class="markup--em markup--p-em">sparkDF.show(5)</em>. Note that you cannot view the last lines (<em class="markup--em markup--p-em">.tail()</em> does no exist yet, because long to do in distributed environment)</p><figure name="12b4" id="12b4" class="graf--figure graf-after--p"/><h4 name="edff" id="edff" class="graf--h4 graf-after--figure">Inferring types</h4><p name="fd49" id="fd49" class="graf--p graf-after--h4">With Pandas, you rarely have to bother with types : they are <strong class="markup--strong markup--p-strong">inferred for you</strong>.</p><p name="6ae7" id="6ae7" class="graf--p graf-after--p">With Spark DataFrames loaded from CSV files, <strong class="markup--strong markup--p-strong">default types are assumed to be “strings”</strong>.</p><p name="0b2b" id="0b2b" class="graf--p graf-after--p">EDIT : in spark-csv, there is a ‘inferSchema’ option (disabled by default), but I didn’t manage to make it work. <a href="https://github.com/databricks/spark-csv/issues/110" data-href="https://github.com/databricks/spark-csv/issues/110" class="markup--anchor markup--p-anchor" rel="nofollow">It doesn’t seem to be functional</a> in the 1.1.0 version.</p><figure name="a2e4" id="a2e4" class="graf--figure graf-after--p"/><p name="14c0" id="14c0" class="graf--p graf-after--figure">To change types with Spark, you can use the <em class="markup--em markup--p-em">.cast()</em>method, or equivalently <em class="markup--em markup--p-em">.astype()</em>, which is <a href="https://issues.apache.org/jira/browse/SPARK-7394" data-href="https://issues.apache.org/jira/browse/SPARK-7394" class="markup--anchor markup--p-anchor" rel="nofollow">an alias gently created for those like me</a> coming from the Pandas world ;). Note that you must <strong class="markup--strong markup--p-strong">create a new column, and drop the old one</strong> (<a href="https://issues.apache.org/jira/browse/SPARK-6635" data-href="https://issues.apache.org/jira/browse/SPARK-6635" class="markup--anchor markup--p-anchor" rel="nofollow">some improvements exist to allow “in place”-like changes</a>, but it is not yet available with the Python API).</p><figure name="5f61" id="5f61" class="graf--figure graf-after--p"/><h4 name="552a" id="552a" class="graf--h4 graf-after--figure">Describing</h4><p name="48c0" id="48c0" class="graf--p graf-after--h4">In Pandas and Spark, <em class="markup--em markup--p-em">.describe()</em> generate various summary statistics. They give slightly different results for two reasons :</p><ul class="postList"><li name="bb19" id="bb19" class="graf--li graf-after--p">In Pandas, NaN values are excluded. In Spark, NaN values make that computation of mean and standard deviation fail</li><li name="4bc3" id="4bc3" class="graf--li graf-after--li">standard deviation is not computed in the same way. <strong class="markup--strong markup--li-strong">Unbiased (or corrected) standard deviation</strong> by default in Pandas, and <strong class="markup--strong markup--li-strong">uncorrected standard</strong> deviation in Spark. The difference is the <a href="https://en.wikipedia.org/wiki/Standard_deviation" data-href="https://en.wikipedia.org/wiki/Standard_deviation" class="markup--anchor markup--li-anchor" rel="nofollow">use of N-1 instead of N </a>on the denominator</li></ul><figure name="8f41" id="8f41" class="graf--figure graf-after--li"/><h4 name="5379" id="5379" class="graf--h4 graf-after--figure">Wrangling</h4><p name="79a0" id="79a0" class="graf--p graf-after--h4">In Machine Learning, it is usual to create new columns resulting from a calculus on already existing columns (features engineering).</p><p name="57eb" id="57eb" class="graf--p graf-after--p">In Pandas, you can use the ‘[ ]’ operator. In Spark you can’t — DataFrames are immutable. You should use <em class="markup--em markup--p-em">.withColumn()</em>.</p><figure name="7d6a" id="7d6a" class="graf--figure graf-after--p"/><h4 name="1ceb" id="1ceb" class="graf--h4 graf-after--figure">Concluding</h4><p name="5335" id="5335" class="graf--p graf-after--h4">Spark and Pandas DataFrames are very similar. Still, <strong class="markup--strong markup--p-strong">Pandas API remains more convenient and powerful</strong> — but the gap is <strong class="markup--strong markup--p-strong">shrinking quickly</strong>.</p><p name="4339" id="4339" class="graf--p graf-after--p">Despite its intrinsic design constraints (immutability, distributed computation, lazy evaluation, …), <strong class="markup--strong markup--p-strong">Spark wants to mimic Pandas</strong> as much as possible (up to the method names). My guess is that this goal will be achieved soon.</p><p name="b7b7" id="b7b7" class="graf--p graf-after--p">And with <a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html" data-href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html" class="markup--anchor markup--p-anchor" rel="nofollow">Spark.ml</a>, mimicking scikit-learn, Spark may become <strong class="markup--strong markup--p-strong">the perfect one-stop-shop tool for industrialized Data Science</strong>.</p><p name="0952" id="0952" class="graf--p graf-after--p"><em class="markup--em markup--p-em">Thanks to </em><a href="https://ogirardot.wordpress.com/" data-href="https://ogirardot.wordpress.com/" class="markup--anchor markup--p-anchor" rel="nofollow"><em class="markup--em markup--p-em">Olivier Girardot</em></a><em class="markup--em markup--p-em"> for helping to improve this pos</em>t.</p><p name="5da6" id="5da6" class="graf--p graf-after--p">EDIT 1 : Olivier just released a new post giving more insights : <a href="https://ogirardot.wordpress.com/2015/07/31/from-pandas-to-apache-sparks-dataframe/" data-href="https://ogirardot.wordpress.com/2015/07/31/from-pandas-to-apache-sparks-dataframe/" class="markup--anchor markup--p-anchor" rel="nofollow">From Pandas To Apache Spark Dataframes</a></p><p name="6b05" id="6b05" class="graf--p graf-after--p graf--last">EDIT 2 : Here is another post on the same topic : <a href="https://lab.getbase.com/pandarize-spark-dataframes/" data-href="https://lab.getbase.com/pandarize-spark-dataframes/" class="markup--anchor markup--p-anchor" rel="nofollow">Pandarize Your Spark Dataframes</a></p></div></div></body></html>