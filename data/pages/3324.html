<html><body><div><div class="entry-content clearfix">












<p><a href="https://realpython.com/blog/python/web-scraping-with-scrapy-and-mongodb">Last time</a> we implemented a basic web scraper that downloaded the latest questions from StackOverflow and stored the results in MongoDB. <strong>In this article we’ll extend our scraper so that it crawls through the pagination links at the bottom of each page and scrapes the questions (question title and URL) from each page.</strong></p>




<p><strong>Updates:</strong></p>

<ol>
<li>09/06/2015 – Updated to the latest version of <a href="http://doc.scrapy.org/en/1.0/">Scrapy</a> (v1.0.3) and <a href="http://api.mongodb.org/python/3.0.3/">PyMongo</a> (v3.0.3) – cheers!</li>
</ol>





<blockquote><p>Before you start any scraping job, review the site’s terms of use policy and respect the robots.txt file. Also, adhere to ethical scraping practices by not flooding a site with numerous requests over a short span of time. <strong>Treat any site you scrape as if it were your own.</strong></p></blockquote>

<hr/>


<p>This is a collaboration piece between the folks at Real Python and György – a Python enthusiast and software developer, currently working at a big data company and seeking a new job at the same time. You can ask him questions on twitter – <a href="https://twitter.com/kissgyorgy">@kissgyorgy</a>.</p>

<a name="Getting.Started"/>
<h2>Getting Started</h2>

<p>There are two possible ways to continue from where we left off.</p>

<p>The first is to extend our existing Spider by extracting every next page link from the response in the <code>parse_item</code> method with an xpath expression and just <code>yield</code> a <code>Request</code> object with a callback to the same <code>parse_item</code> method. This way scrapy will automatically make a new request to the link we specify. You can find more information on this method in the <a href="http://doc.scrapy.org/en/1.0/topics/spiders.html#spiders">Scrapy documentation</a>.</p>

<p>The other, much simpler option is to utilize a different type of spider – the <code>CrawlSpider</code> (<a href="http://doc.scrapy.org/en/1.0/topics/spiders.html#crawlspider">link</a>). It’s an extended version of the basic <code>Spider</code>, designed exactly for our use case.</p>

<a name="The.CrawlSpider"/>
<h2>The CrawlSpider</h2>

<p>We’ll be using the same Scrapy project from the last tutorial, so grab the code from the <a href="https://github.com/realpython/stack-spider/releases/tag/v1">repo</a> if you need it.</p>

<a name="Create.the.Boilerplate"/>
<h3>Create the Boilerplate</h3>

<p>Within the “stack” directory, start by <a href="http://doc.scrapy.org/en/1.0/topics/commands.html#std:command-genspider">generating</a> the spider boilerplate from the <code>crawl</code> template:</p>

<figure class="code"><figcaption><span/></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"><span class="nv">$ </span>scrapy genspider stack_crawler stackoverflow.com -t crawl
</span><span class="line">Created spider <span class="s1">'stack_crawler'</span> using template <span class="s1">'crawl'</span> in module:
</span><span class="line">  stack.spiders.stack_crawler
</span></code></pre></td></tr></table></div></figure>


<p>The Scrapy project should now look like this:</p>

<figure class="code"><figcaption><span/></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">├── scrapy.cfg
</span><span class="line">└── stack
</span><span class="line">    ├── __init__.py
</span><span class="line">    ├── items.py
</span><span class="line">    ├── pipelines.py
</span><span class="line">    ├── settings.py
</span><span class="line">    └── spiders
</span><span class="line">        ├── __init__.py
</span><span class="line">        ├── stack_crawler.py
</span><span class="line">        └── stack_spider.py
</span></code></pre></td></tr></table></div></figure>


<p>And the <em>stack_crawler.py</em> file should look like this:</p>

<figure class="code"><figcaption><span/></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="c"># -*- coding: utf-8 -*-</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">scrapy</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">scrapy.contrib.linkextractors</span> <span class="kn">import</span> <span class="n">LinkExtractor</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">scrapy.contrib.spiders</span> <span class="kn">import</span> <span class="n">CrawlSpider</span><span class="p">,</span> <span class="n">Rule</span>
</span><span class="line">
</span><span class="line"><span class="kn">from</span> <span class="nn">stack.items</span> <span class="kn">import</span> <span class="n">StackItem</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">class</span> <span class="nc">StackCrawlerSpider</span><span class="p">(</span><span class="n">CrawlSpider</span><span class="p">):</span>
</span><span class="line">    <span class="n">name</span> <span class="o">=</span> <span class="s">'stack_crawler'</span>
</span><span class="line">    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s">'stackoverflow.com'</span><span class="p">]</span>
</span><span class="line">    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">'http://www.stackoverflow.com/'</span><span class="p">]</span>
</span><span class="line">
</span><span class="line">    <span class="n">rules</span> <span class="o">=</span> <span class="p">(</span>
</span><span class="line">        <span class="n">Rule</span><span class="p">(</span><span class="n">LinkExtractor</span><span class="p">(</span><span class="n">allow</span><span class="o">=</span><span class="s">r'Items/'</span><span class="p">),</span> <span class="n">callback</span><span class="o">=</span><span class="s">'parse_item'</span><span class="p">,</span> <span class="n">follow</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
</span><span class="line">    <span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">parse_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
</span><span class="line">        <span class="n">i</span> <span class="o">=</span> <span class="n">StackItem</span><span class="p">()</span>
</span><span class="line">        <span class="c">#i['domain_id'] = response.xpath('//input[@id="sid"]/@value').extract()</span>
</span><span class="line">        <span class="c">#i['name'] = response.xpath('//div[@id="name"]').extract()</span>
</span><span class="line">        <span class="c">#i['description'] = response.xpath('//div[@id="description"]').extract()</span>
</span><span class="line">        <span class="k">return</span> <span class="n">i</span>
</span></code></pre></td></tr></table></div></figure>


<p>We just need to make a few updates to this boilerplate…</p>

<a name="Update.the..code.start_urls..code..list"/>
<h3>Update the <code>start_urls</code> list</h3>

<p>First, add the first page of questions to the <code>start_urls</code> list:</p>

<figure class="code"><figcaption><span/></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
</span><span class="line">    <span class="s">'http://stackoverflow.com/questions?pagesize=50&amp;sort=newest'</span>
</span><span class="line"><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>


<a name="Update.the..code.rules..code..list"/>
<h3>Update the <code>rules</code> list</h3>

<p>Next, we need to tell the spider where it can find the next page links by adding a regular expression to the <code>rules</code> attribute:</p>

<figure class="code"><figcaption><span/></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="n">rules</span> <span class="o">=</span> <span class="p">[</span>
</span><span class="line">    <span class="n">Rule</span><span class="p">(</span><span class="n">LinkExtractor</span><span class="p">(</span><span class="n">allow</span><span class="o">=</span><span class="s">r'questions\?page=[0-9]&amp;sort=newest'</span><span class="p">),</span>
</span><span class="line">         <span class="n">callback</span><span class="o">=</span><span class="s">'parse_item'</span><span class="p">,</span> <span class="n">follow</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</span><span class="line"><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>


<p>Scrapy will now automatically request new pages based on those links and pass the response to the <code>parse_item</code> method to extract the questions and titles.</p>

<blockquote><p>If you’re paying close attention, this regex limits the crawling to the first 9 pages since for this demo we do not want to scrape <strong>all</strong> 176,234 pages!</p></blockquote>

<a name="Update.the..code.parse_item..code..method"/>
<h3>Update the <code>parse_item</code> method</h3>

<p>Now we just need to write how to parse the pages with xpath, which we already did in the last tutorial – so just copy it over:</p>

<figure class="code"><figcaption><span/></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">parse_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
</span><span class="line">    <span class="n">questions</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">'//div[@class="summary"]/h3'</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="k">for</span> <span class="n">question</span> <span class="ow">in</span> <span class="n">questions</span><span class="p">:</span>
</span><span class="line">        <span class="n">item</span> <span class="o">=</span> <span class="n">StackItem</span><span class="p">()</span>
</span><span class="line">        <span class="n">item</span><span class="p">[</span><span class="s">'url'</span><span class="p">]</span> <span class="o">=</span> <span class="n">question</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span>
</span><span class="line">            <span class="s">'a[@class="question-hyperlink"]/@href'</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">        <span class="n">item</span><span class="p">[</span><span class="s">'title'</span><span class="p">]</span> <span class="o">=</span> <span class="n">question</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span>
</span><span class="line">            <span class="s">'a[@class="question-hyperlink"]/text()'</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">        <span class="k">yield</span> <span class="n">item</span>
</span></code></pre></td></tr></table></div></figure>


<p>That’s it for the spider, but do <strong>not</strong> start it just yet.</p>

<a name="Add.a.Download.Delay"/>
<h3>Add a Download Delay</h3>

<p>We need to be nice to StackOverflow (and any site, for that matter) by setting a <a href="http://doc.scrapy.org/en/1.0/topics/settings.html#std:setting-DOWNLOAD_DELAY">download delay</a> in <em>settings.py</em>:</p>

<figure class="code"><figcaption><span/></figcaption></figure>


<p>This tells Scrapy to wait at least 5 seconds between every new request it makes. You’re essentially rate limiting yourself. If you do not do this, StackOverflow will rate limit you; and if you continue to scrape the site without imposing a rate limit, your IP address could be banned. So, be nice – <em>Treat any site you scrape as if it were your own.</em></p>

<p>Now there is only one thing left to do – store the data.</p>

<a name="MongoDB"/>
<h2>MongoDB</h2>

<p>Last time we only downloaded 50 questions, but since we are grabbing a lot more data this time, we want to avoid adding duplicate questions to the database. We can do that by using a MongoDB <a href="http://docs.mongodb.org/v3.0/reference/method/db.collection.update/#upsert-option">upsert</a>, which means we update the question title if it is already in the database and insert otherwise.</p>

<p>Modify the <code>MongoDBPipeline</code> we defined earlier:</p>

<figure class="code"><figcaption><span/></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">class</span> <span class="nc">MongoDBPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="n">connection</span> <span class="o">=</span> <span class="n">pymongo</span><span class="o">.</span><span class="n">MongoClient</span><span class="p">(</span>
</span><span class="line">            <span class="n">settings</span><span class="p">[</span><span class="s">'MONGODB_SERVER'</span><span class="p">],</span>
</span><span class="line">            <span class="n">settings</span><span class="p">[</span><span class="s">'MONGODB_PORT'</span><span class="p">]</span>
</span><span class="line">        <span class="p">)</span>
</span><span class="line">        <span class="n">db</span> <span class="o">=</span> <span class="n">connection</span><span class="p">[</span><span class="n">settings</span><span class="p">[</span><span class="s">'MONGODB_DB'</span><span class="p">]]</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">collection</span> <span class="o">=</span> <span class="n">db</span><span class="p">[</span><span class="n">settings</span><span class="p">[</span><span class="s">'MONGODB_COLLECTION'</span><span class="p">]]</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
</span><span class="line">        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">item</span><span class="p">:</span>
</span><span class="line">            <span class="k">if</span> <span class="ow">not</span> <span class="n">data</span><span class="p">:</span>
</span><span class="line">                <span class="k">raise</span> <span class="n">DropItem</span><span class="p">(</span><span class="s">"Missing data!"</span><span class="p">)</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">collection</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s">'url'</span><span class="p">:</span> <span class="n">item</span><span class="p">[</span><span class="s">'url'</span><span class="p">]},</span> <span class="nb">dict</span><span class="p">(</span><span class="n">item</span><span class="p">),</span> <span class="n">upsert</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</span><span class="line">        <span class="n">log</span><span class="o">.</span><span class="n">msg</span><span class="p">(</span><span class="s">"Question added to MongoDB database!"</span><span class="p">,</span>
</span><span class="line">                <span class="n">level</span><span class="o">=</span><span class="n">log</span><span class="o">.</span><span class="n">DEBUG</span><span class="p">,</span> <span class="n">spider</span><span class="o">=</span><span class="n">spider</span><span class="p">)</span>
</span><span class="line">        <span class="k">return</span> <span class="n">item</span>
</span></code></pre></td></tr></table></div></figure>


<blockquote><p>For simplicity, we did not optimize the query and did not deal with indexes since this is not a production environment.</p></blockquote>

<a name="Test"/>
<h2>Test</h2>

<p>Start the spider!</p>

<figure class="code"><figcaption><span/></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"><span class="nv">$ </span>scrapy crawl stack_crawler
</span></code></pre></td></tr></table></div></figure>


<p>Now sit back and watch your database fill with data!</p>

<figure class="code"><figcaption><span/></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"><span class="nv">$ </span>mongo
</span><span class="line">MongoDB shell version: 3.0.4
</span><span class="line">&gt; use stackoverflow
</span><span class="line">switched to db stackoverflow
</span><span class="line">&gt; db.questions.count<span class="o">()</span>
</span><span class="line">447
</span><span class="line">&gt;
</span></code></pre></td></tr></table></div></figure>


<a name="Conclusion"/>
<h2>Conclusion</h2>

<p>You can download the entire source code from the <a href="https://github.com/realpython/stack-spider/releases/tag/v2">Github repository</a>. Comment below with questions. Cheers!</p>

<blockquote><p>Looking for more web scraping? Be sure to check out the <a href="https://realpython.com/courses">Real Python courses</a>. Looking to hire a professional web scraper? Check out <a href="http://www.goscrape.com/">GoScrape</a>.</p></blockquote>
</div>


      </div></body></html>