<html><body><div><div class="span8">

      <div class="post-top">
	
	<div>
	  <h1>Bubbles: Python ETL Framework (prototype)</h1>
	  
	</div>
      </div>

    

    <h2 id="introduction-and-etl">Introduction and ETL</h2>

<p>The abbreviation <em>ETL</em> stands for <em>extract, transform and load</em>. What is it
good for? For everything between data sources and fancy visualisations. In the
data warehouse the data will spend most of the time going through some kind of
ETL, before they reach their final state. ETL is mostly automated,
reproducible and should be designed in a way that it is not difficult to track
how the data move around the data processing pipes.</p>

<p>Data warehouse stands and falls on ETLs.</p>

<h2 id="bubbles">Bubbles</h2>

<p><a href="http://bubbles.databrewery.org">Bubbles</a> is, or rather is meant to be, a
framework for ETL written in Python, but not necessarily meant to be used from
Python only. Bubbles is meant to be based rather on metadata describing the
data processing pipeline (ETL) instead of script based description. The
principles of the framework can be summarized as:</p>

<ul>
  <li>ETL is described as a data processing pipeline which is an <a href="https://en.wikipedia.org/wiki/Directed_graph">directed
graph</a></li>
  <li>Processing operations are nodes in the graph, such as <em>aggregation</em>,
<em>filtering</em>, <em>dataset comparison (diff)</em>, <em>conversion</em>, …</li>
  <li>Nodes might have multiple different inputs and a single output (there might
be multiple outgoing connections, but all of them are the same) – the inputs
are considered <em>operands</em> to the operation and the output is the operation
<em>result</em>.</li>
  <li>Data do not flow, if it is not necessary</li>
</ul>

<p>The pipeline is described in a such way, that it is technology agnostic – the
ETL developer, the person who wants data to be processed, does not have to
care about how to access and work with data in particular data store, he can
just focus on his task – deliver the data in the form that he needs to be
delivered.</p>

<h3 id="data-objects-and-data-store">Data Objects and Data Store</h3>

<p>The core of Bubbles are <em>data objects</em> – abstract concept of datasets which
might have multiple internal representations. What actually flows between the
nodes are not data itself, but those virtual representations of data and their
compositions. Data are fetched only if it is really necessary – if there is no
other option how to compose the data, such as join between a database table
and a CSV file.</p>

<p>Here are few objects with different representations:</p>

<p><img src="/img/posts/bubbles/bubbles-object_representations.png" alt="Object Representations"/></p>

<p>The objects are:</p>

<ul>
  <li>object which originates from a <em>CSV file</em>, can be processed mainly using
the python iterators, however retains its text CSV nature, just in case some
of the nodes might know how to work with it more efficiently, for example
row filtering without actually parsing the CSV into row objects</li>
  <li><em>SQL object representing a table</em> – it can be composed into other SQL
statements or can be used directly as a Python iterable</li>
  <li><em>MongoDB collection</em> – similar to the previous SQL table, can be iterated as
raw stream of documents</li>
  <li><em>SQL statement</em> which might be a result of previous operations or our custom
complex query. It can be used as such statement and composed with further
operations, or the data can be fetched and iterated over in Python. Since
this SQL object comes from a known database (PostgreSQL in this case) which
implements a <a href="http://www.postgresql.org/docs/current/static/sql-copy.html">COPY</a>
command that generates CSV output, we can treat that object as such and
provide the option to use CSV representation as well</li>
  <li><em>Twitter API object</em> – an exampple of a data object that actually does not
exists for us as a physical table, we do not even know from how many
original tables the Twitter is feeding us the data and we do not have to
care at all. We are just fine that we can have an impression of iterable
dataset.</li>
</ul>

<p>To be more concrete, take a simple filtering for example. Say we have sample
of Tweets stored in a SQL database,
<a href="http://docs.mongodb.org/manual/tutorial/query-documents/">MongoDB</a>
and obviously <a href="https://dev.twitter.com/docs/api/1/get/statuses/user_timeline">on Twitter</a>.
We want to get all tweets by OKFN. In SQL we use a SQL driver, connect to the
database and do:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>SELECT * FROM  WHERE screen_name = 'okfn'
</code></pre>
</div>

<p>in Mongo we use a mongodb driver, connect to the database and do:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>db.tweets.find(
    { },
    { screen_name: 'okfn'}
)
</code></pre>
</div>

<p>and in Twitter we just issue the following HTTP request:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>https://api.twitter.com/1/statuses/user_timeline.json?screen_name=okfn
</code></pre>
</div>

<p>We asked for the same data object – <em>a tweet</em> in three different data stores.
We had to use three different approaches. It does not look bad for us, “the
tech people”. What if we can just write:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>p = Pipeline(...)
p.source("data", "tweets")
p.filter_value("screen_name", "okfn")
p.pretty_print()

p.run()
</code></pre>
</div>

<p>The <code class="highlighter-rouge">"data"</code> is the data store. We as ETL designers do not have to worry what
kind of data store it is, how to talk to it, how to get data from it.</p>

<p>Now we would like to count the tweets, so let us add <code class="highlighter-rouge">aggregate()</code> operation,
which by default yields only record count:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>p = Pipeline(...)
p.source("data", "tweets")
p.filter_value("screen_name", "okfn")
p.aggregate()
p.pretty_print()

p.run()
</code></pre>
</div>

<p>What happens here? For example, in the SQL case the <code class="highlighter-rouge">COUNT()</code> aggregation
function will be used. For twitter, because our backend does not know better,
the tweets will have to be pulled all from the Twitter API and counted
one-by-one. Which is sad, but good for our example. The objective was to
deliver the desired result, which happened.</p>

<h3 id="context">Context</h3>

<p>One thing is missing in my examples above: <code class="highlighter-rouge">Pipeline(...)</code> – the pipeline
works in a context. We need to provide the description of data stores. For
example:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>stores = { "data": {"type": "sql", "url": "postgresl://localhost/twitter" }}
</code></pre>
</div>

<p>Stores have an interface for getting datasets by name or creating new
datasets. Dataset might be:</p>

<ul>
  <li><em>table</em> in a <em>SQL</em> store</li>
  <li><em>collection</em> in a <em>MongoDB</em> store</li>
  <li><em>CSV file</em> in a store represented by a directory of CSV files</li>
  <li><em>JSON newlline delimited</em> file in a store represented by a directory of
JSOND files</li>
  <li>resource collection over an API, such as the Twitter example above</li>
  <li>dataset from a <a href="http://data.okfn.org/doc/data-package">datapackage</a></li>
</ul>

<p>ETL designer should not care about the underlying implementation, he should
care only about having “a set of data that look like a table”. Object dataset
responds to methods such as <code class="highlighter-rouge">object_names()</code> or <code class="highlighter-rouge">get_object(name)</code>.</p>

<h3 id="operations">Operations</h3>

<p>The ETL operations work on data objects provided as operands. An operation
returns another data object. As mentioned above, the flow of data is just
virtual. That means that when we are filtering the data, the framework might
be actually composing a SQL <code class="highlighter-rouge">WHERE</code> statement instead of just pulling the data
out of the database and filtering them row-by-rown in Python.</p>

<p>Similar with fields in the dataset – if we want to keep just certain columns,
why to pass them around all in the first place? Why not to ask only for those
that we actually need at the end? That is what Bubbles should do. Therefore
the <code class="highlighter-rouge">keep_fields()</code> operation just selects certain columns when used in the
SQL context.</p>

<p>There might be multiple implementations of the same operation. Which
implementation (function) is used is determined at the time of pipeline
execution. <code class="highlighter-rouge">aggregate()</code> might be in-python row-by-row aggregation using a
dictionary or it might be <code class="highlighter-rouge">SUM()</code> or <code class="highlighter-rouge">AVG()</code> with <code class="highlighter-rouge">GROUP BY</code> statement in SQL,
depending on which kind of object is passed to the operation.</p>

<p>In the following image you might see how the most appropriate operation is
chosen for you depending on the data source. You can also see, that for
certain representations the operations are combined together to produce just
single data query for the source system:</p>

<p><img src="/img/posts/bubbles/bubbles-operation_representations.png" alt="Operations and Object&#10;Representations"/></p>

<h2 id="examples">Examples</h2>

<p><a href="https://gist.github.com/Stiivi/5937938">Here is an example</a> of Bubbles
framework in action: “list customer details of customers who ordered something
between year 2011 and 2013”. You might see that the source is a directory of
CSV files. For comparison on the SQL example we <code class="highlighter-rouge">create()</code> a table, so the
rest of the pipeline will hapen as SQL, not in Python.</p>

<p><a href="https://gist.github.com/Stiivi/5907305">Another example</a> showing aggregation
joining of details.</p>

<p><a href="https://gist.github.com/Stiivi/9104719">An example</a> that uses a data package
(<a href="http://data.okfn.org/doc/data-package">according to spec</a>) as a data store:</p>

<p>The pipeline looks like this:</p>

<p><img src="/img/posts/bubbles/bubbles-join_example.png" alt="Pipeline Example"/></p>

<p>The Python source code for the pipeline:</p>

<div class="highlighter-rouge"><pre class="highlight"><code># Aggregate population per independence type for every year
# Sources: Population and Country Codes datasets
#
 
from bubbles import Pipeline
 
# List of stores with datasets. In this example we are using the "datapackage"
# store
stores = {
    "source": {"type": "datapackages", "url": "."}
}
 
p = Pipeline(stores=stores)
# Set the source dataset 
p.source("source", "population")
 
# Prepare another dataset and keep just relevant fields
cc = p.fork(empty=True)
cc.source("source", "country-codes")
cc.keep_fields(["ISO3166-1-Alpha-3", "is_independent"])
 
# Join them – left inner join
p.join_details(pop, "Country Code", "ISO3166-1-Alpha-3")
 
# Aggregate Value by status and year
p.aggregate(["is_independent", "Year"],
                       [["Value", "sum"]],
                       include_count=True)
 
# Sort for nicer output...
p.sort(["is_independent", "Year"])
 
# Print pretty table.
p.pretty_print()
 
p.run()
</code></pre>
</div>



<p>I have been using Python as a scripting language to define my pipelines.
Observant reader might have noticed, that all I did was just composition of
some messages, which is true. The <code class="highlighter-rouge">p</code> Pipeline object contains just a graph
and the <code class="highlighter-rouge">run()</code> method uses an execution engine to resolve the graph and pick
the appropriate operations for the given thask. That means, my whole
processing pipeline does not need to be written in Python at all. It migh be
described as a JSON for example, it might even be generated from some
graphical user inteface for flow based programming.</p>

<p>There is more into metadata in Bubbles than mentioned in this blog post.
The framework understands higher level metadata, such as analytical – role of
a field from data analysis perspective. For example the <code class="highlighter-rouge">aggregate()</code>
operation might by default aggregate all fields that are of analytical type
<code class="highlighter-rouge">measure</code> and that information is passed on. Which results in less writing and
less noise on the side of pipeline designer.</p>

<h2 id="summary">Summary</h2>

<p>Why should someone who just wants to achieve his goal of extracting,
transforming and presenting the data care about the underlying technology and
query language?  Mostly these days when we are dealing with so many systems it
is an unnecessary distraction. Moreover, many ETL blocks are generic and reusable,
why we would have to write the same code for every system we use?</p>

<p>Having an abstract ETL framework allows us to share transformations, cleaning
methods, quality checks and more much easier.</p>

<p>In addition, it leaves the optimization of the process to the operation
writers, to the people with technical skills, who know when it is good to move
data over the networks and through the disks, or if we can just compose an
operation and issue a sigle statment that the source system understands.</p>

<h3 id="future">Future</h3>

<p>The bubbles is still just a prototype, for the brave ones. But I would love to
see it as a Python ETL/data integration framework. The short term needs and
objectives are:</p>

<ul>
  <li>Simpler pipeline definition interface, more functional programming oriented</li>
  <li>Larger library of higher level reusable components, such as dimension
loaders (there is
<a href="https://en.wikipedia.org/wiki/Slowly_changing_dimension">more</a> into <code class="highlighter-rouge">UPSERT</code>
that many of us think, but that is another story)</li>
  <li>Easier way to write operations.</li>
  <li>Larger variety of supported backends and services</li>
</ul>

<p>If anyone is willing to help to prototype, I will gladly guide him/her. Let us
build a python open source integration framework together. Extensible.
Understandable. Focused on the use, way of thinking and pipeline design
workflow.</p>

<p>Links:</p>





    

    
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
  </div></body></html>