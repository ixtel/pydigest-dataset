<html><body><div><div class="wall_post_text"><a href="/feed?section=search&amp;q=%23python">#python</a> <a href="/feed?section=search&amp;q=%23pynsk">#pynsk</a><p>Извлечение информации: скачиваем список ссылок с помощью asyncio</p><p>Порой возникают рутинные задачи, которые не хочется делать руками. Примером такой задачи может являться - скачать множество страниц по ссылкам. Если 5 ссылок еще вручную сохранить можно, а если их 1000? или 6250, как было в моем случае. </p><p>На Python эту задачу можно с помощью модуля asyncio и aiohttp. </p><p>Вот такой код можно написать за пару минут:</p><p>import asyncio</p><p>import aiohttp</p><p>@asyncio.coroutine</p><p>def download(url):</p><p>....try:</p><p>........response = yield from aiohttp.request('GET', url)</p><p>........data = yield from response.text()</p><p>........yield from save_page(url, data) # сохраняем страницу в файл, или еще куда</p><p>....except Exception as e:</p><p>........return None</p><p>@asyncio.coroutine</p><p>def download_parallel(urls):</p><p>....tasks = [asyncio.Task(download(url)) for url in urls]</p><p>....yield from asyncio.gather(*tasks)</p><p>urls = [&lt;список ссылок&gt;]</p><p>loop = asyncio.get_event_loop()</p><p>loop.run_until_complete(download_parallel(urls))</p></div></div></body></html>