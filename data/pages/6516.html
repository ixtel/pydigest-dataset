<html><body><div><article class="post-content">
  <p>Every year there is a big festival in Edinburgh called <a href="https://tickets.edfringe.com/?gclid=Cj0KEQjwzPSrBRC_oOXfxPWP6t0BEiQARqav2KeTlfbBfcnFIHJN6llKStyz-2p1di-dm69gm88C3B0aAivP8P8HAQ">the fringe
festival</a>.
I blogged about this a while ago, <a href="http://drvinceknight.blogspot.co.uk/2013/08/a-very-brief-and-shallow-analysis-of.html">in that post I did a very basic bit of
natural language
processing</a>
aiming to try and identify what made things funny.
In this blog post Iâ€™m going to push that a bit further by building a
classification model that aims to predict if a joke is funny or not.
(tldr: I donâ€™t really succeed but but thatâ€™s mainly because I have very little
data - having more data would not necessarily guarantee success either but the code and approach is whatâ€™s worth taking from this postâ€¦ ðŸ˜ª).</p>

<p>If you want to skip the brief description and go straight to look at the code
you can find the <a href="https://github.com/drvinceknight/EdinburghFringeJokes/blob/master/nlp-of-jokes.ipynb">ipython notebook on github
here</a> and <a href="https://cloud.sagemath.com/projects/a4f27edc-8528-4c7f-adf7-b6c790e29349/files/nlp-of-jokes.html">on cloud.sagemath here</a>.</p>

<p>The <strong>data</strong> comes from a series of <a href="http://www.bbc.co.uk/news/uk-scotland-edinburgh-east-fife-28838287">BBC
articles</a>
which reports (more or less every year since 2011?) the top ten jokes at the
fringe festival. This does in fact only give 60 odd jokes to work withâ€¦</p>

<p>Here is the latest winner (by Tim Vine):</p>

<blockquote>
  <p>I decided to sell my Hooverâ€¦ well it was just collecting dust.</p>
</blockquote>

<p>After cleaning it up slightly Iâ€™ve thrown that all in a <a href="https://github.com/drvinceknight/EdinburghFringeJokes/blob/master/jokes.json"><code class="highlighter-rouge">json</code> file here</a>.
So in order to import the data in to a panda data frame I just run:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">read_json</span><span class="p">(</span><span class="s">'jokes.json'</span><span class="p">)</span> <span class="c"># Loading the json file</span></code></pre></figure>

<p>Pandas is great, Iâ€™ve been used to creating my own bespoke classes for handling
data but in general just using pandas does the exact right job.
At this point I basically follow along with
<a href="http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/">this post on sentiment analysis of twitter</a> which makes use of the ridiculously powerful <code class="highlighter-rouge">nltk</code> library.</p>

<p>We can use the <code class="highlighter-rouge">nltk</code> library to â€˜tokeniseâ€™ and get rid of common words:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">commonwords</span> <span class="o">=</span> <span class="p">[</span><span class="n">e</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">nltk</span><span class="o">.</span><span class="n">corpus</span><span class="o">.</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s">'english'</span><span class="p">))]</span> <span class="c"># &lt;- Need to download the corpus: import nltk; nltk.download()</span>
<span class="n">commonwords</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="s">'M'</span><span class="p">,</span> <span class="s">'VE'</span><span class="p">])</span>  <span class="c">#Â Adding a couple of things that need to be removed</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">tokenize</span><span class="o">.</span><span class="n">RegexpTokenizer</span><span class="p">(</span><span class="s">r'</span><span class="err">\</span><span class="s">w+'</span><span class="p">)</span>  <span class="c">#Â To be able to strip out unwanted things in strings</span>
<span class="n">string_to_list</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">[</span><span class="n">el</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span> <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">if</span> <span class="n">el</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">commonwords</span><span class="p">]</span>
<span class="n">df</span><span class="p">[</span><span class="s">'Joke'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'Raw_joke'</span><span class="p">]</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">string_to_list</span><span class="p">)</span></code></pre></figure>

<p>Note that this requires downloading one of the awesome corpuses
(<a href="https://www.google.co.uk/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;es_th=1&amp;ie=UTF-8#q=plural%20of%20corpus&amp;es_th=1">thats apparently the right way to say that</a>) from nltk.</p>

<p>Here is how this looks:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">joke</span> <span class="o">=</span> <span class="s">'I decided to sell my Hoover... well it was just collecting dust.'</span>
<span class="n">string_to_list</span><span class="p">(</span><span class="n">joke</span><span class="p">)</span></code></pre></figure>

<p>which gives:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="p">[</span><span class="s">'DECIDED'</span><span class="p">,</span> <span class="s">'SELL'</span><span class="p">,</span> <span class="s">'HOOVER'</span><span class="p">,</span> <span class="s">'WELL'</span><span class="p">,</span> <span class="s">'COLLECTING'</span><span class="p">,</span> <span class="s">'DUST'</span><span class="p">]</span></code></pre></figure>

<h2 id="we-can-now-get-started-on-building-a-classifier">We can now get started on building a classifier</h2>

<p>Here is the general idea of what will be happening:</p>

<p><img src="/unpeudemath/assets/images/description_of_ratio_learning_for_nlp_jokes.svg" alt=""/></p>

<p>First of all we need to build up the â€˜featuresâ€™ of each joke,
in other words pull the words out in to a nice easy format.</p>

<p>To do that we need to find all the words from our training data set, another way of describing this is that we need to build up our dictionary:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s">'Year'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'Year'</span><span class="p">]</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_all_words</span><span class="p">(</span><span class="n">dataframe</span><span class="p">):</span>
    <span class="s">"""
    A function that gets all the words from the Joke column in a given dataframe
    """</span>
    <span class="n">all_words</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">jk</span> <span class="ow">in</span> <span class="n">dataframe</span><span class="p">[</span><span class="s">'Joke'</span><span class="p">]:</span>
        <span class="n">all_words</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">jk</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">all_words</span>

<span class="n">all_words</span> <span class="o">=</span> <span class="n">get_all_words</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s">'Year'</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">2013</span><span class="p">])</span>  <span class="c"># This uses all jokes before 2013 as our training data set.</span></code></pre></figure>

<p>We then build something that will tell us for each joke which of the overall words is in it:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">extract_features</span><span class="p">(</span><span class="n">joke</span><span class="p">,</span> <span class="n">all_words</span><span class="p">):</span>
    <span class="n">words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">joke</span><span class="p">)</span>
    <span class="n">features</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
        <span class="n">features</span><span class="p">[</span><span class="s">'contains(</span><span class="si">%</span><span class="s">s)'</span> <span class="o">%</span> <span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">word</span> <span class="ow">in</span> <span class="n">all_words</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">features</span></code></pre></figure>

<p><em>Once we have done that, we just need to decide what we will call a <strong>funny</strong> joke</em>. For this purpose
Weâ€™ll use a <code class="highlighter-rouge">funny_threshold</code> and any joke that ranks above the
<code class="highlighter-rouge">funny_threshold</code> in any given year will be considered <strong>funny</strong>:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">funny_threshold</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">df</span><span class="p">[</span><span class="s">'Rank'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'Rank'</span><span class="p">]</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s">'Funny'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'Rank'</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">funny_threshold</span></code></pre></figure>

<p>Now we just need to create a tuple for each joke that puts the features mentioned earlier and a classification (if the joke was funny or not) together:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s">'Labeled_Feature'</span><span class="p">]</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'Features'</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="s">'Funny'</span><span class="p">])</span></code></pre></figure>

<p>We can now (<strong>in one line of code!!!!</strong>) create a classifier:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">classifier</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">NaiveBayesClassifier</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s">'Year'</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">2013</span><span class="p">][</span><span class="s">'Labeled_Feature'</span><span class="p">])</span></code></pre></figure>

<p>This classifier will take into account all the words in a given joke and spit out if itâ€™s funny or not.
It can also give us some indication as to what makes a joke funny or not:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">classifier</span><span class="o">.</span><span class="n">show_most_informative_features</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span></code></pre></figure>

<p>Here is the output of that:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">Most</span> <span class="n">Informative</span> <span class="n">Features</span>
     <span class="n">contains</span><span class="p">(</span><span class="n">GOT</span><span class="p">)</span> <span class="o">=</span> <span class="bp">True</span>   <span class="bp">False</span> <span class="p">:</span> <span class="bp">True</span>   <span class="o">=</span>  <span class="mf">2.4</span> <span class="p">:</span> <span class="mf">1.0</span>
    <span class="n">contains</span><span class="p">(</span><span class="n">KNOW</span><span class="p">)</span> <span class="o">=</span> <span class="bp">True</span>    <span class="bp">True</span> <span class="p">:</span> <span class="bp">False</span>  <span class="o">=</span>  <span class="mf">1.7</span> <span class="p">:</span> <span class="mf">1.0</span>
  <span class="n">contains</span><span class="p">(</span><span class="n">PEOPLE</span><span class="p">)</span> <span class="o">=</span> <span class="bp">True</span>   <span class="bp">False</span> <span class="p">:</span> <span class="bp">True</span>   <span class="o">=</span>  <span class="mf">1.7</span> <span class="p">:</span> <span class="mf">1.0</span>
     <span class="n">contains</span><span class="p">(</span><span class="n">SEX</span><span class="p">)</span> <span class="o">=</span> <span class="bp">True</span>   <span class="bp">False</span> <span class="p">:</span> <span class="bp">True</span>   <span class="o">=</span>  <span class="mf">1.7</span> <span class="p">:</span> <span class="mf">1.0</span>
   <span class="n">contains</span><span class="p">(</span><span class="n">NEVER</span><span class="p">)</span> <span class="o">=</span> <span class="bp">True</span>   <span class="bp">False</span> <span class="p">:</span> <span class="bp">True</span>   <span class="o">=</span>  <span class="mf">1.7</span> <span class="p">:</span> <span class="mf">1.0</span>
      <span class="n">contains</span><span class="p">(</span><span class="n">RE</span><span class="p">)</span> <span class="o">=</span> <span class="bp">True</span>    <span class="bp">True</span> <span class="p">:</span> <span class="bp">False</span>  <span class="o">=</span>  <span class="mf">1.6</span> <span class="p">:</span> <span class="mf">1.0</span>
  <span class="n">contains</span><span class="p">(</span><span class="n">FRIEND</span><span class="p">)</span> <span class="o">=</span> <span class="bp">True</span>    <span class="bp">True</span> <span class="p">:</span> <span class="bp">False</span>  <span class="o">=</span>  <span class="mf">1.6</span> <span class="p">:</span> <span class="mf">1.0</span>
     <span class="n">contains</span><span class="p">(</span><span class="n">SAY</span><span class="p">)</span> <span class="o">=</span> <span class="bp">True</span>    <span class="bp">True</span> <span class="p">:</span> <span class="bp">False</span>  <span class="o">=</span>  <span class="mf">1.6</span> <span class="p">:</span> <span class="mf">1.0</span>
  <span class="n">contains</span><span class="p">(</span><span class="n">BOUGHT</span><span class="p">)</span> <span class="o">=</span> <span class="bp">True</span>    <span class="bp">True</span> <span class="p">:</span> <span class="bp">False</span>  <span class="o">=</span>  <span class="mf">1.6</span> <span class="p">:</span> <span class="mf">1.0</span>
     <span class="n">contains</span><span class="p">(</span><span class="n">ONE</span><span class="p">)</span> <span class="o">=</span> <span class="bp">True</span>    <span class="bp">True</span> <span class="p">:</span> <span class="bp">False</span>  <span class="o">=</span>  <span class="mf">1.5</span> <span class="p">:</span> <span class="mf">1.0</span></code></pre></figure>

<p>This immediately gives us some information:</p>

<ul>
  <li>If your joke is about <code class="highlighter-rouge">SEX</code> is it more likely to <em>not</em> be funny.</li>
  <li>If your joke is about <code class="highlighter-rouge">FRIEND</code>s is it more likely to <em>be</em> funny.</li>
</ul>

<p>Thatâ€™s all very nice but we can now (theoretically - again, I really donâ€™t have enough data for this) start using the mathematical model to tell you if something is funny:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">joke</span> <span class="o">=</span> <span class="s">'Why was 10 afraid of 7? Because 7 8 9'</span>
<span class="n">classifier</span><span class="o">.</span><span class="n">classify</span><span class="p">(</span><span class="n">extract_features</span><span class="p">(</span><span class="n">string_to_list</span><span class="p">(</span><span class="n">joke</span><span class="p">),</span> <span class="n">get_all_words</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s">'Year'</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">2013</span><span class="p">])))</span></code></pre></figure>

<p>That joke is apparently funny (the output of above is <code class="highlighter-rouge">True</code>). The following joke however is apparently not (the output of below if <code class="highlighter-rouge">False</code>):</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">joke</span> <span class="o">=</span> <span class="s">'Your mother is ...'</span>
<span class="k">print</span> <span class="n">classifier</span><span class="o">.</span><span class="n">classify</span><span class="p">(</span><span class="n">extract_features</span><span class="p">(</span><span class="n">string_to_list</span><span class="p">(</span><span class="n">joke</span><span class="p">),</span> <span class="n">get_all_words</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s">'Year'</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">2013</span><span class="p">])))</span></code></pre></figure>

<p>As you can see in the <a href="https://github.com/drvinceknight/EdinburghFringeJokes/blob/master/nlp-of-jokes.ipynb">ipython notebook</a>
it is then very easy to measure how good the predictions are
(I used the data from years before 2013 to predict 2014).</p>

<h2 id="results">Results</h2>

<p>Here is a plot of the accuracy of the classifier for changing values of <code class="highlighter-rouge">funny_threshold</code>:</p>

<p><img src="/unpeudemath/assets/images/varying_values_of_funny_threshold.png" alt=""/></p>

<p>Youâ€™ll notice a couple of things:</p>

<ul>
  <li>When the threshold is 0 or 1: the classifier works perfectly.
This makes sense: all the jokes are either funny or not so itâ€™s very easy
for the classifier to do well.</li>
  <li>There seems to be a couple of regions where the classifier does
particularly poorly: just after a value of 4. Indeed there are points where
the classifier does worse than flipping a coin.</li>
  <li>At a value of 4, the classifier does particularly well!</li>
</ul>

<p>Now, one final thing Iâ€™ll take a look at is what happens if I start
randomly selecting a portion of the entire data set to be the training set:</p>

<p><img src="/unpeudemath/assets/images/description_of_random_ratio_learning_for_nlp_jokes.svg" alt=""/></p>

<p>Below are 10 plots that correspond to 50 repetitions of the above where I
randomly sample a ratio of the data set to be the training set:</p>

<p><img src="/unpeudemath/assets/images/joke_classification_moving_ratio_threshold-1.png" alt=""/></p>

<p><img src="/unpeudemath/assets/images/joke_classification_moving_ratio_threshold-2.png" alt=""/></p>

<p><img src="/unpeudemath/assets/images/joke_classification_moving_ratio_threshold-3.png" alt=""/></p>

<p><img src="/unpeudemath/assets/images/joke_classification_moving_ratio_threshold-4.png" alt=""/></p>

<p><img src="/unpeudemath/assets/images/joke_classification_moving_ratio_threshold-5.png" alt=""/></p>

<p><img src="/unpeudemath/assets/images/joke_classification_moving_ratio_threshold-6.png" alt=""/></p>

<p><img src="/unpeudemath/assets/images/joke_classification_moving_ratio_threshold-7.png" alt=""/></p>

<p><img src="/unpeudemath/assets/images/joke_classification_moving_ratio_threshold-8.png" alt=""/></p>

<p><img src="/unpeudemath/assets/images/joke_classification_moving_ratio_threshold-9.png" alt=""/></p>

<p><img src="/unpeudemath/assets/images/joke_classification_moving_ratio_threshold-10.png" alt=""/></p>

<p>Finally (although itâ€™s really not helpful), here are all of those on a single plot:</p>

<p><img src="/unpeudemath/assets/images/joke_classification_moving_ratio_threshold-all.png" alt=""/></p>

<p><strong>First of all: all those plots are basically one line of <a href="http://stanford.edu/~mwaskom/software/seaborn/"><code class="highlighter-rouge">seaborn</code> code</a> which is ridiculously cool</strong>. Seaborn is basically magic:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">sns</span><span class="o">.</span><span class="n">tsplot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">steps</span><span class="p">)</span></code></pre></figure>

<p>Second of all, it looks like the lower bound of the classifiers is around .5.
Most of them start of at .5, in other words they are as good as flipping a coin
before we let them learn from anything, which makes sense.
Finally it seems that the threshold of 4 classifier seems to be the only one that
gradually improves as more data is given to it. Thatâ€™s perhaps indicating that
something interesting is happening there but that investigation would be for another day.</p>

<p><strong>All of the conclusions about the actual data should certainly not be taken seriously:
I simply do not have enough data.</strong> But, the overall process and code is what is worth taking away.
<strong>Itâ€™s pretty neat that the variety of awesome python libraries
lets you do this sort of thing more or less out of the box.</strong></p>

<p>Please do take a look at <a href="https://github.com/drvinceknight/EdinburghFringeJokes">this github repository</a>
but Iâ€™ve also just put the notebook on <a href="https://cloud.sagemath.com/">cloud.sagemath</a> so assuming you
<code class="highlighter-rouge">pip install</code> the libraries and get the data etc you can play around with this right in your browser:</p>

<p><a href="https://cloud.sagemath.com/projects/a4f27edc-8528-4c7f-adf7-b6c790e29349/files/nlp-of-jokes.html">Here is the notebook on cloud.sagemath</a>.</p>

  </article>

</div></body></html>