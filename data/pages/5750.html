<html><body><div><section class="post-content">
            <p>In this post, we’ll be using the K-nearest neighbors algorithm to predict how many points NBA players scored in the 2013-2014 season.  Along the way, we’ll learn about euclidean distance and figure out which NBA players are the most similar to Lebron James.  If you want to follow along, you can grab the dataset in csv format <a href="https://www.dropbox.com/s/b3nv38jjo5dxcl6/nba_2013.csv?dl=0">here</a>.</p>

<h2 id="a-look-at-the-data">A look at the data</h2>

<p>Before we dive into the algorithm, let’s take a look at our data.  Each row in the data contains information on how a player performed in the 2013-2014 NBA season.</p>

<p>Here are some selected columns from the data:</p>

<ul>
  <li><code>player</code> – name of the player</li>
  <li><code>pos</code> – the position of the player</li>
  <li><code>g</code> – number of games the player was in</li>
  <li><code>gs</code> – number of games the player started</li>
  <li><code>pts</code> – total points the player scored</li>
</ul>

<p>There are many more columns in the data, mostly containing information about average player game performance over the course of the season.  See <a href="http://www.databasebasketball.com/about/aboutstats.htm">this site</a> for an explanation of the rest of them.</p>

<p>We can read our dataset in and figure out which columns are present:</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">"nba_2013.csv"</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">csvfile</span><span class="p">:</span>
    <span class="n">nba</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">csvfile</span><span class="p">)</span>

<span class="c"># The names of all the columns in the data.</span>
<span class="k">print</span><span class="p">(</span><span class="n">nba</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="p">)</span></code></pre></div>

<pre><code>['player' 'pos' 'age' 'bref_team_id' 'g' 'gs' 'mp' 'fg' 'fga' 'fg.' 'x3p'
 'x3pa' 'x3p.' 'x2p' 'x2pa' 'x2p.' 'efg.' 'ft' 'fta' 'ft.' 'orb' 'drb'
 'trb' 'ast' 'stl' 'blk' 'tov' 'pf' 'pts' 'season' 'season_end']
</code></pre>

<h2 id="knn-overview">KNN overview</h2>

<p>The k-nearest neighbors algorithm is based around the simple idea of predicting unknown values by matching them with the most similar known values.</p>

<p>Let’s say that we have 3 different types of cars.  We know the name of the car, its horsepower, whether or not it has racing stripes, and whether or not it’s fast.:</p>

<pre><code>car,horsepower,racing_stripes,is_fast
Honda Accord,180,False,False
Yugo,500,True,True
Delorean DMC-12,200,True,True
</code></pre>

<p>Let’s say that we now have another car, but we don’t know how fast it is:</p>

<pre><code>car,horsepower,racing_stripes,is_fast
Chevrolet Camaro,400,True,Unknown
</code></pre>

<p>We want to figure out if the car is fast or not.  In order to predict if it is with k nearest neighbors, we first find the most similar known car.  In this case, we would compare the <code>horsepower</code> and <code>racing_stripes</code> values to find the most similar car, which is the <code>Yugo</code>.  Since the Yugo is fast, we would predict that the Camaro is also fast.  This is an example of 1-nearest neighbors – we only looked at the most similar car, giving us a k of 1.</p>

<p>If we performed a 2-nearest neighbors, we would end up with 2 <code>True</code> values (for the Delorean and the Yugo), which would average out to <code>True</code>.  The Delorean and Yugo are the two most similar cars, giving us a k of 2.</p>

<p>If we did 3-nearest neighbors, we would end up with 2 <code>True</code> values and a <code>False</code> value, which would average out to <code>True</code>.</p>

<p>The number of neighbors we use for k-nearest neighbors (k) can be any value less than the number of rows in our dataset.  In practice, looking at only a few neighbors makes the algorithm perform better, because the less similar the neighbors are to our data, the worse the prediction will be.</p>

<h2 id="euclidean-distance">Euclidean distance</h2>

<p>Before we can predict using KNN, we need to find some way to figure out which data rows are “closest” to the row we’re trying to predict on.</p>

<p>A simple way to do this is to use Euclidean distance.  The formula is </p>

<p>Let’s say we have these two rows (True/False has been converted to 1/0), and we want to find the distance between them:</p>

<pre><code>car,horsepower,is_fast
Honda Accord,180,0
Chevrolet Camaro,400,1
</code></pre>

<p>We would first only select the numeric columns.  Then the distance becomes , which is about equal to <code>220</code>.</p>

<p>We can use the principle of euclidean distance to find the most similar NBA players to <a href="https://en.wikipedia.org/wiki/LeBron_James">Lebron James</a>.</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># Select Lebron James from our dataset</span>
<span class="n">selected_player</span> <span class="o">=</span> <span class="n">nba</span><span class="p">[</span><span class="n">nba</span><span class="p">[</span><span class="s">"player"</span><span class="p">]</span> <span class="o">==</span> <span class="s">"LeBron James"</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c"># Choose only the numeric columns (we'll use these to compute euclidean distance)</span>
<span class="n">distance_columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'age'</span><span class="p">,</span> <span class="s">'g'</span><span class="p">,</span> <span class="s">'gs'</span><span class="p">,</span> <span class="s">'mp'</span><span class="p">,</span> <span class="s">'fg'</span><span class="p">,</span> <span class="s">'fga'</span><span class="p">,</span> <span class="s">'fg.'</span><span class="p">,</span> <span class="s">'x3p'</span><span class="p">,</span> <span class="s">'x3pa'</span><span class="p">,</span> <span class="s">'x3p.'</span><span class="p">,</span> <span class="s">'x2p'</span><span class="p">,</span> <span class="s">'x2pa'</span><span class="p">,</span> <span class="s">'x2p.'</span><span class="p">,</span> <span class="s">'efg.'</span><span class="p">,</span> <span class="s">'ft'</span><span class="p">,</span> <span class="s">'fta'</span><span class="p">,</span> <span class="s">'ft.'</span><span class="p">,</span> <span class="s">'orb'</span><span class="p">,</span> <span class="s">'drb'</span><span class="p">,</span> <span class="s">'trb'</span><span class="p">,</span> <span class="s">'ast'</span><span class="p">,</span> <span class="s">'stl'</span><span class="p">,</span> <span class="s">'blk'</span><span class="p">,</span> <span class="s">'tov'</span><span class="p">,</span> <span class="s">'pf'</span><span class="p">,</span> <span class="s">'pts'</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">euclidean_distance</span><span class="p">(</span><span class="n">row</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    A simple euclidean distance function</span>
<span class="sd">    """</span>
    <span class="n">inner_value</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">distance_columns</span><span class="p">:</span>
        <span class="n">inner_value</span> <span class="o">+=</span> <span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-</span> <span class="n">selected_player</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">inner_value</span><span class="p">)</span>

<span class="c"># Find the distance from each player in the dataset to lebron.</span>
<span class="n">lebron_distance</span> <span class="o">=</span> <span class="n">nba</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">euclidean_distance</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code></pre></div>

<h2 id="normalizing-columns">Normalizing columns</h2>

<p>You may have noticed that <code>horsepower</code> in the cars example had a much larger impact on the final distance than <code>racing_stripes</code> did.  This is because <code>horsepower</code> values are much larger in absolute terms, and thus dwarf the impact of <code>racing_stripes</code> values in the euclidean distance calculations.</p>

<p>This can be bad, because a variable having larger values doesn’t necessarily make it better at predicting what rows are similar.</p>

<p>A simple way to deal with this is to normalize all the columns to have a mean of 0, and a standard deviation of 1.  This will ensure that no single column has a dominant impact on the euclidean distance calculations.</p>

<p>To set the mean to 0, we have to find the mean of a column, then subtract the mean from every value in the column.  To set the standard deviation to 1, we divide every value in the column by the standard deviation.  The formula is .</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># Select only the numeric columns from the NBA dataset</span>
<span class="n">nba_numeric</span> <span class="o">=</span> <span class="n">nba</span><span class="p">[</span><span class="n">distance_columns</span><span class="p">]</span>

<span class="c"># Normalize all of the numeric columns</span>
<span class="n">nba_normalized</span> <span class="o">=</span> <span class="p">(</span><span class="n">nba_numeric</span> <span class="o">-</span> <span class="n">nba_numeric</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="n">nba_numeric</span><span class="o">.</span><span class="n">std</span><span class="p">()</span></code></pre></div>

<h2 id="finding-the-nearest-neighbor">Finding the nearest neighbor</h2>

<p>We now know enough to find the nearest neighbor of a given row in the NBA dataset.  We can use the <code>distance.euclidean</code> function from <code>scipy.spatial</code>, a much faster way to calculate euclidean distance.</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">scipy.spatial</span> <span class="kn">import</span> <span class="n">distance</span>

<span class="c"># Fill in NA values in nba_normalized</span>
<span class="n">nba_normalized</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c"># Find the normalized vector for lebron james.</span>
<span class="n">lebron_normalized</span> <span class="o">=</span> <span class="n">nba_normalized</span><span class="p">[</span><span class="n">nba</span><span class="p">[</span><span class="s">"player"</span><span class="p">]</span> <span class="o">==</span> <span class="s">"LeBron James"</span><span class="p">]</span>

<span class="c"># Find the distance between lebron james and everyone else.</span>
<span class="n">euclidean_distances</span> <span class="o">=</span> <span class="n">nba_normalized</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="n">distance</span><span class="o">.</span><span class="n">euclidean</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">lebron_normalized</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c"># Create a new dataframe with distances.</span>
<span class="n">distance_frame</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="p">{</span><span class="s">"dist"</span><span class="p">:</span> <span class="n">euclidean_distances</span><span class="p">,</span> <span class="s">"idx"</span><span class="p">:</span> <span class="n">euclidean_distances</span><span class="o">.</span><span class="n">index</span><span class="p">})</span>
<span class="n">distance_frame</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="s">"dist"</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c"># Find the most similar player to lebron (the lowest distance to lebron is lebron, the second smallest is the most similar non-lebron player)</span>
<span class="n">second_smallest</span> <span class="o">=</span> <span class="n">distance_frame</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s">"idx"</span><span class="p">]</span>
<span class="n">most_similar_to_lebron</span> <span class="o">=</span> <span class="n">nba</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">second_smallest</span><span class="p">)][</span><span class="s">"player"</span><span class="p">]</span></code></pre></div>

<h2 id="generating-training-and-testing-sets">Generating training and testing sets</h2>

<p>Now that we know how to find the nearest neighbors, we can make predictions on a test set.  We’ll try to predict how many points a player scored using the <code>5</code> closest neighbors.  We’ll find neighbors by using all the numeric columns in the dataset to generate similarity scores.</p>

<p>First, we have to generate test and train sets.  In order to do this, we’ll use random sampling.  We’ll randomly shuffle the index of the <code>nba</code> dataframe, and then pick rows using the randomly shuffled values.</p>

<p>If we didn’t do this, we’d end up predicting and training on the same data set, which would overfit.  We could do cross validation also, which would be slightly better, but slightly more complex.</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">permutation</span>

<span class="c"># Randomly shuffle the index of nba.</span>
<span class="n">random_indices</span> <span class="o">=</span> <span class="n">permutation</span><span class="p">(</span><span class="n">nba</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
<span class="c"># Set a cutoff for how many items we want in the test set (in this case 1/3 of the items)</span>
<span class="n">test_cutoff</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">nba</span><span class="p">)</span><span class="o">/</span><span class="mi">3</span><span class="p">)</span>
<span class="c"># Generate the test set by taking the first 1/3 of the randomly shuffled indices.</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">nba</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">random_indices</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">test_cutoff</span><span class="p">]]</span>
<span class="c"># Generate the train set with the rest of the data.</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">nba</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">random_indices</span><span class="p">[</span><span class="n">test_cutoff</span><span class="p">:]]</span></code></pre></div>

<h2 id="using-sklearn">Using sklearn</h2>

<p>Instead of having to do it all ourselves, we can use the k-nearest neighbors implementation in scikit-learn.  <a href="http://scikit-learn.org/stable/modules/neighbors.html">Here’s</a> the documentation.  There’s a regressor and a classifier available, but we’ll be using the regressor, as we have continuous values to predict on.</p>

<p>Sklearn performs the normalization and distance finding automatically, and lets us specify how many neighbors we want to look at.</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># The columns that we will be making predictions with.</span>
<span class="n">x_columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'age'</span><span class="p">,</span> <span class="s">'g'</span><span class="p">,</span> <span class="s">'gs'</span><span class="p">,</span> <span class="s">'mp'</span><span class="p">,</span> <span class="s">'fg'</span><span class="p">,</span> <span class="s">'fga'</span><span class="p">,</span> <span class="s">'fg.'</span><span class="p">,</span> <span class="s">'x3p'</span><span class="p">,</span> <span class="s">'x3pa'</span><span class="p">,</span> <span class="s">'x3p.'</span><span class="p">,</span> <span class="s">'x2p'</span><span class="p">,</span> <span class="s">'x2pa'</span><span class="p">,</span> <span class="s">'x2p.'</span><span class="p">,</span> <span class="s">'efg.'</span><span class="p">,</span> <span class="s">'ft'</span><span class="p">,</span> <span class="s">'fta'</span><span class="p">,</span> <span class="s">'ft.'</span><span class="p">,</span> <span class="s">'orb'</span><span class="p">,</span> <span class="s">'drb'</span><span class="p">,</span> <span class="s">'trb'</span><span class="p">,</span> <span class="s">'ast'</span><span class="p">,</span> <span class="s">'stl'</span><span class="p">,</span> <span class="s">'blk'</span><span class="p">,</span> <span class="s">'tov'</span><span class="p">,</span> <span class="s">'pf'</span><span class="p">]</span>
<span class="c"># The column that we want to predict.</span>
<span class="n">y_column</span> <span class="o">=</span> <span class="p">[</span><span class="s">"pts"</span><span class="p">]</span>

<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsRegressor</span>
<span class="c"># Create the knn model.</span>
<span class="c"># Look at the five closest neighbors.</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="c"># Fit the model on the training data.</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train</span><span class="p">[</span><span class="n">x_columns</span><span class="p">],</span> <span class="n">train</span><span class="p">[</span><span class="n">y_column</span><span class="p">])</span>
<span class="c"># Make point predictions on the test set using the fit model.</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="n">x_columns</span><span class="p">])</span></code></pre></div>

<h2 id="computing-error">Computing error</h2>

<p>Now that we know our point predictions, we can compute the error involved with our predictions.  We can compute <a href="http://en.wikipedia.org/wiki/Mean_squared_error">mean squared error</a>.  The formula is .</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># Get the actual values for the test set.</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="n">y_column</span><span class="p">]</span>

<span class="c"># Compute the mean squared error of our predictions.</span>
<span class="n">mse</span> <span class="o">=</span> <span class="p">(((</span><span class="n">predictions</span> <span class="o">-</span> <span class="n">actual</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span></code></pre></div>

        </section>

        </div></body></html>