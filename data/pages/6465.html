<html><body><div><div class="entry-content">
		<p>Often, in NLP,Â you need to answer the simple question: â€œis this a common word?â€ It turns out that this leaves the computer to answer a more vexing question: â€œWhatâ€™s a word?â€</p>
<p>Letâ€™s talk briefly about why word frequencies are important. In many cases, you want to assign more significance to uncommon words. For example,Â a product review might contain the word â€œuseâ€ and the word â€œdefectiveâ€, and the word â€œdefectiveâ€ carries way more information. If youâ€™re wondering what the deal is with John Kasich, a headline that mentions â€œKasichâ€ will be much more likely to be what youâ€™re looking for than one that merely mentions â€œJohnâ€.</p>
<p>For purposes like these, it would be nice if we could just import a Python package that could tell us whether one word was more common than another, in general, based on a wide variety of text. We looked for a while and couldnâ€™t find it. So we built it.</p>
<p><a href="https://github.com/LuminosoInsight/wordfreq">wordfreq</a> provides estimates of the frequencies of words in many languages, loading its data from efficiently-compressed data structures so it can give you word frequencies down to 1 occurrence per million without having to access an external database. ItÂ aims to avoid being limited to a particular domain or style of text, getting its data from a variety of sources: Google Books, Wikipedia, OpenSubtitles, Twitter, and the Leeds Internet Corpus.</p>
<div data-shortcode="caption" id="attachment_767" class="wp-caption aligncenter"><a href="https://luminosoinsight.files.wordpress.com/2015/08/most-common-words2.png"><img class="size-full wp-image-767" src="https://luminosoinsight.files.wordpress.com/2015/08/most-common-words2.png?w=1000" alt="The 10 most common words that wordfreq knows in 15 languages."/></a><p class="wp-caption-text">The 10 most common words that <em>wordfreq</em> knows in 15 languages. Yes, it can handle multi-character words in Chinese and Japanese; those just arenâ€™t in the top 10.Â A puzzle for Unicode geeks: guess where the start of the Arabic list is.</p></div>
<p><span id="more-659"/></p>
<h2>Partial solutions: stopwords and inverse document frequency</h2>
<p>Those who are familiar with the basics of information retrieval probably have a couple of simple suggestions in mind for dealing with word frequencies.</p>
<p>One is to come up with a list of <em>stopwords</em>, words such as â€œtheâ€ and â€œofâ€ that are too common to use for anything. Discarding stopwords can be a useful optimization, but thatâ€™s far too blunt of an operation to solve the word frequency problem in general. Thereâ€™s no place to draw the bright line between stopwords and non-stopwords, andÂ in the â€œJohn Kasichâ€ example, itâ€™s not the case that â€œJohnâ€ should be a stopword.</p>
<p>Another partial solution would be to collect all the documents youâ€™re interested in, and re-scale all the words according to their <em>inverse document frequency</em>Â or IDF. This is a quantity that decreases as the proportionÂ of documents a word appears in increases, reaching 0 for a word that appears in every document.</p>
<p>One problem with IDF is that it canâ€™t distinguish a word that appears in a lot of documents because itâ€™s unimportant, from a word that appears in a lot of documents because itâ€™sÂ <em>very</em> important to your domain.Â Another, more practical problem with IDF is that you canâ€™t calculate it until youâ€™ve seen all your documents, and it fluctuates a lot as you add documents. This is particularly an issue if your documents arrive in an endless stream.</p>
<p>We need good domain-general word frequencies, not just domain-specific word frequencies, because without the general ones, we canâ€™t determine which domain-specific word frequencies are interesting.</p>
<h2>Avoiding biases</h2>
<p>The counts of one resource alone tend to tell you more about that resource than about the language. If you ask Wikipedia alone, youâ€™ll find that â€œcensusâ€, â€œ1945â€, and â€œstubâ€ are very common words. If you ask Google Books, youâ€™ll find that â€œpropranololâ€ is supposed to be 10 times more common than â€œlolâ€ overall (and also that thereâ€™s something funny going on, so to speak, in the early 1800s).</p>
<p><a href="https://luminosoinsight.files.wordpress.com/2015/08/propranolol.png"><img class="alignnone size-large wp-image-762" src="https://luminosoinsight.files.wordpress.com/2015/08/propranolol.png?w=1000&amp;h=365" alt=""/></a></p>
<p>If you collect data fromÂ Twitter, youâ€™ll of course find out how common â€œlolâ€ is. You also might find that the ram emoji â€œğŸâ€ is supposed to be extremely common, because that guy from One Direction once tweeted â€œWe are derby super ğŸğŸğŸğŸğŸğŸğŸğŸğŸğŸğŸğŸğŸğŸğŸğŸğŸğŸğŸğŸâ€, and apparently every fan of One Direction who knows what Derby Super Rams are retweeted it.</p>
<p>Yes, wordfreq considers emoji to be words. Its Twitter frequencies would hardly be complete without them.</p>
<p>We canâ€™t entirely avoid the biases that come from where we get our data. But if we collect data from enough different sources (not just larger sources), we can at least smooth out the biasesÂ by averaging them between the different sources.</p>
<h2>Whatâ€™s a word?</h2>
<p>You have to agree with your wordlist on the matter of what constitutes a â€œwordâ€, or else youâ€™ll get weird results that arenâ€™t supported by the actual data.</p>
<p>Do you split words at all spaces and punctuation? Which of the thousands of symbols in Unicode are punctuation? Is an apostrophe punctuation? Is it punctuation when itÂ puts a word in single quotes? Is it punctuation in â€œcanâ€™tâ€, or in â€œlâ€™espritâ€? How many words is â€œU.S.â€ or â€œgoogle.comâ€?Â How many words is â€œãŠæ—©ã†ã”ã–ã„ã¾ã™â€ (â€œgood morningâ€),Â taking into accountÂ that Japanese is written without spaces? The symbol â€œ-â€ probably doesnâ€™t count as a word, but doesÂ â€œ+â€? How about â€œâ˜®â€ or â€œâ™¥â€?</p>
<p>The process of splitting text into words is called â€œtokenizationâ€, and everyoneâ€™s got their own different way to do it, which is a bit of a problem for a word frequency list.</p>
<p>We tried a few ways to make a sufficiently simple tokenization function that we could use everywhere, across many languages. We ended up withÂ our own ad-hoc rule including large sets of Unicode characters and a special case for apostrophes, and this is in fact what we used when we originally released wordfreq 1.0, which came packaged with regular expressions that look like attempts to depict the Flying Spaghetti Monster in text.</p>
<p><a href="https://luminosoinsight.files.wordpress.com/2015/08/ugly-regex.png"><img class="aligncenter wp-image-769 size-full" src="https://luminosoinsight.files.wordpress.com/2015/08/ugly-regex.png?w=1000" alt="A particularly noodly regex."/></a></p>
<p>But shortly after that,Â I realizedÂ that <a href="http://unicode.org/reports/tr29/">the Unicode Consortium had already done something similar</a>, and theyâ€™d probably thought about it for more than a few days.</p>
<div data-shortcode="caption" id="attachment_768" class="wp-caption aligncenter"><a href="https://luminosoinsight.files.wordpress.com/2015/08/boundary-fig-2.gif"><img class="size-full wp-image-768" src="https://luminosoinsight.files.wordpress.com/2015/08/boundary-fig-2.gif?w=1000" alt="Word splitting in Unicode"/></a><p class="wp-caption-text">Word splitting in Unicode. Not pictured: how to decide which of these segments count as â€œwordsâ€.</p></div>
<p>ThisÂ standard for tokenization looked like almost exactly what we wanted, and the last thing holding me back was that implementing it efficiently in Python looked like it was going to be a huge pain. Then I found that the <a href="https://pypi.python.org/pypi/regex">regex package</a> (not theÂ <em>re</em> package built into Python) contains an efficient implementation of this standard. DefiningÂ how to split text into words became a very simple regular expressionâ€¦ except in Chinese and Japanese, because a regular expression has no chance in a language where the separation between words is not written in any way.</p>
<p>So this is how wordfreq 1.1 identifies the words to count and the words to look up.Â Of course, there is going to be data that has been tokenized in a different way. When wordfreq gets something that looks like it should be multiple words, it will look them up separately and estimate their combined frequency, instead of just returning 0.</p>
<p><strong>Language support</strong></p>
<p>wordfreq supports 15 commonly-used languages, but of course some languages are better supported than others.Â English is quite polished, for example, while Chinese so far is just there to be better than nothing.</p>
<p>The reliability of each language corresponds pretty well with the number of different data sources we put together to make the wordlist. Some sources are hard to get in certain languages.Â Perhaps unsurprisingly, for example, not much of Twitter is in Chinese. Perhaps more surprisingly, not much of it is in German either.</p>
<p>The word lists that weâ€™ve built for wordfreq represent the languages where we have at least two sources. I would consider the ones with two sources a bit dubious, while all the languages that have three or more sources seem to have a reasonable ranking of words.</p>
<ul>
<li><strong>5 sources</strong>: English</li>
<li><strong>4 sources</strong>: Arabic, French, German, Italian, Portuguese, Russian, Spanish</li>
<li><strong>3 sources</strong>: Dutch, Indonesian, Japanese, Malay</li>
<li><strong>2 sources</strong>: Chinese, Greek, Korean</li>
</ul>
<h2>Compact wordlists</h2>
<p>When we were still figuring this all out, we madeÂ several 0.x versions of wordfreq that required an external SQLite database with all the word frequencies, because there are millionsÂ of possible words and we had to storeÂ a different floating-point frequency for each one. Thatâ€™s a lot of data, and it would have beenÂ infeasibleÂ to include it all inside the Python package. (GitHub and PyPI donâ€™t like huge files.) We ended up with a situation where installing wordfreq would either need to download a huge database file, or build that file from its source data, both of which would consume a lot of time and computing resourcesÂ when youâ€™re just trying to install a simple package.</p>
<p>As we tried different ways of shipping this data around to all the places that needed it, we finally tried another tactic: What if we just distributed less data?</p>
<p>Two assumptions let us greatlyÂ shrink our word lists:</p>
<ul>
<li>We donâ€™t care about the frequencies of words that occur less than once per million words. We can just assume all those wordsÂ are equally informative.</li>
<li>We donâ€™t care about, say, 2% differences in word frequency.</li>
</ul>
<p>Now instead of storing a separate frequency for each word, we group the words into 600 possible tiers of frequency. You could call these tiers â€œcentibelsâ€, a logarithmic unit similar toÂ decibels, because there are 100 of them for each factor of 10Â in the word frequency. Each of them represents a band of word frequencies that spans about a 2.3% difference.Â The data we store can then be simplified to â€œHere are all the words in tier #330â€¦ now here are all the words in tier #331â€¦â€ and converted to frequencies when you ask for them.</p>
<div data-shortcode="caption" id="attachment_784" class="wp-caption aligncenter"><a href="https://luminosoinsight.files.wordpress.com/2015/08/wordfreq-tiers.png"><img class="size-full wp-image-784" src="https://luminosoinsight.files.wordpress.com/2015/08/wordfreq-tiers.png?w=1000" alt="Some tiers of word frequencies in English."/></a><p class="wp-caption-text">Some tiers of word frequencies in English.</p></div>
<p>This let us cut down the word lists to an entirely reasonable size, so that we can put them in the repository, and just keep them in memory while youâ€™re using them. TheÂ English word list, for example, is 245 KB, or 135 KB compressed.</p>
<p>But itâ€™s important to note the trade-off here, that wordfreq only represents sufficiently common words. Itâ€™s not suited for comparing rare words to each other. A word rarer than â€œamuletâ€, â€œbunchesâ€, â€œdeactivateâ€, â€œgroupieâ€, â€œpinballâ€, or â€œslipperâ€, all of which have a frequency of about 1 per million, will not be represented in wordfreq.</p>
<h2>Getting the package</h2>
<p>wordfreq is available <a href="https://github.com/LuminosoInsight/wordfreq">on GitHub</a>, or it can be installed from the Python Package Index with the commandÂ <i>pip install wordfreq</i>. Documentation can be found inÂ <a href="https://github.com/LuminosoInsight/wordfreq">its README on GitHub</a>.</p>
<div data-shortcode="caption" id="attachment_771" class="wp-caption aligncenter"><a href="https://luminosoinsight.files.wordpress.com/2015/08/cafe-frequency.png"><img class="size-full wp-image-771" src="https://luminosoinsight.files.wordpress.com/2015/08/cafe-frequency.png?w=1000" alt="wordfreq usage example"/></a><p class="wp-caption-text">Comparing the frequency per million words of two spellings of â€œcafÃ©â€, in English and French.</p></div>
<div id="jp-post-flair" class="sharedaddy sd-like-enabled sd-sharing-enabled"><div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" id="like-post-wrapper-26373703-659-56d5a038b0f3d" data-src="//widgets.wp.com/likes/#blog_id=26373703&amp;post_id=659&amp;origin=luminosoinsight.wordpress.com&amp;obj_id=26373703-659-56d5a038b0f3d" data-name="like-post-frame-26373703-659-56d5a038b0f3d"><h3 class="sd-title">Like this:</h3><p class="likes-widget-placeholder post-likes-widget-placeholder"><span class="button"><span>Like</span></span> <span class="loading">Loading...</span></p><span class="sd-text-color"/><a class="sd-link-color"/></div>
<p id="jp-relatedposts" class="jp-relatedposts">
	<h3 class="jp-relatedposts-headline"><em>Related</em></h3>
</p></div>			</div>

	</div></body></html>