<html><body><div><article class="col-md-10 col-md-offset-1">
            
<p><a href="http://en.wikipedia.org/wiki/Random_forest" title="random forest - wikipedia" target="_blank">Random forest</a> is a highly versatile machine learning method with numerous applications ranging from marketing to healthcare and insurance. It can be used to <a href="http://epubl.ltu.se/1653-0187/2008/014/LTU-PB-EX-08014-SE.pdf" title="Response Rate Modeling - a Data Mining Based Approach for Target Selection" target="_blank">model the impact of marketing</a> on customer acquisition, retention, and churn or to <a href="http://www.biomedcentral.com/1472-6947/11/51" title="Predicting disease risks from highly imbalanced data using random forest">predict disease risk and susceptibility</a> in patients.</p>

<p>Random forest is a capable of regression and classification. It can handle a large number of features, and it's helpful for estimating which or your variables are important in the underlying data being modeled.</p>

<p>This is a post about random forests using Python.</p>

<h4>What is a Random Forest?</h4>
  <p>Random forest is solid choice for nearly any prediction problem (even non-linear ones). It's a relatively new machine learning strategy (it came out of Bell Labs in the 90s) and it can be used for just about anything. It belongs to a larger class of machine learning algorithms called ensemble methods.</p>

  <h5>Ensemble Learning</h5>
    <p><a href="http://en.wikipedia.org/wiki/Ensemble_learning" target="_blank">Ensemble learning</a> involves the combination of several models to solve a single prediction problem. It works by generating multiple classifiers/models which learn and make predictions independently. Those predictions are then combined into a single (mega) prediction that should be as good or better than the prediction made by any one classifer.</p>

    <p>Random forest is a brand of ensemble learning, as it relies on an ensemble of decision trees. More on ensemble learning in Python here: <a href="http://scikit-learn.org/dev/modules/ensemble.html" target="_blank">Scikit-Learn docs</a>.</p>

  <h5>Randomized Decision Trees</h5>
    <p>So we know that random forest is an aggregation of other models, but what types of models is it aggregating? As you might have guessed from its name, random forest aggregates <a href="http://en.wikipedia.org/wiki/Decision_tree_learning" title="decision tree learning - wikipedia" target="_blank">Classification (or Regression) Trees</a>. A decision tree is composed of a series of decisions that can be used to classify an observation in a dataset.</p>

  <h5><em>Random</em> Forest</h5>
    <p>The algorithm to induce a random forest will create a bunch of random decision trees automatically. Since the trees are generated at random, most won't be all that meaningful to learning your classification/regression problem (maybe 99.9% of trees).</p>

    <img src="../static/img/decision_tree_example.png" id="img_d1d0_0"/>
    <p>
      <sub>If an observation has a length of 45, blue eyes, and 2 legs, it's going to be classified as <span id="span_d1d0_0"><b>red</b></span>.</sub>
    </p>

  <h5>Arboreal Voting</h5>
    <p>So what good are 10000 (probably) bad models? Well it turns out that they really aren't that helpful. But <em>what is helpful</em> are the few really good decision trees that you also generated along with the bad ones.</p>

    <p>When you make a prediction, the new observation gets pushed down each decision tree and assigned a predicted value/label. Once each of the trees in the forest have reported its predicted value/label, the predictions are tallied up and the mode vote of all trees is returned as the final prediction.</p>

    <p>Simply, the 99.9% of trees that are irrelevant make predictions that are all over the map and cancel each another out. The predictions of the minority of trees that are good top that noise and yield a good prediction.</p>

    <img src="../static/img/a_random_forest.png"/>
    
<h4>Why you should I use it?</h4>
  <h5>It's Easy</h5>
    <p>Random forest is the <a href="http://www.leatherman.com/product/Super_Tool_300" title="leatherman super tool" target="_blank">Leatherman</a> of learning methods. You can throw pretty much anything at it and it'll do a serviceable job. It does a particularly good job of estimating inferred transformations, and, as a result, doesn't require much tuning like SVM (i.e. it's good for folks with tight deadlines).</p>
  <h5>An Example Transformation</h5>
    <p>Random forest is capable of learning without carefully crafted data transformations. Take the the  <code>f(x) = log(x)</code> function for example.</p>

    <p>Create some fake data and add a little noise.</p>
    <pre>import numpy as np
x = np.random.uniform(1, 100, 1000)
y = np.log(x) + np.random.normal(0, .3, 1000)</pre>
    <a href="https://gist.github.com/glamp/5716253">full gist here</a>
    <img src="../static/img/log_with_noise.png"/>
    <p>If we try and build a basic linear model to predict <code>y</code> using <code>x</code> we wind up with a straight line that sort of bisects the <code>log(x)</code> function. Whereas if we use a random forest, it does a much better job of approximating the <code>log(x)</code> curve and we get something that looks much more like the true function.
    <img src="../static/img/log_lm_vs_rf.png"/>
    <img src="../static/img/log_lm_vs_rf_fit.png"/>
    </p><p>You could argue that the random forest overfits the <code>log(x)</code> function a little bit. Either way, I think this does a nice job of illustrating how the random forest isn't bound by linear constraints.
  </p>

<h4>Uses</h4>
  <h5>Variable Selection</h5>
    <p>One of the best use cases for random forest is feature selection. One of the byproducts of trying lots of decision tree variations is that you can examine which variables are working best/worst in each tree.</p>

    <p>When a certain tree uses a one variable and another doesn't, you can compare the value lost or gained from the inclusion/exclusion of that variable. The good random forest implementations are going to do that for you, so all you need to do is know which method or variable to look at.</p>

    <p>In the following examples, we're trying to figure out which variables are most important for classifying a wine as being red or white.</p>
    <img src="../static/img/rf_wine_importance.png"/>
    
    <img src="../static/img/rf_feature_count_vs_f1.png"/>

  <h5>Classification</h5>
    <p>Random forest is also great for classification. It can be used to make predictions for categories with multiple possible values and it can be calibrated to output probabilities as well. One thing you do need to watch out for is <a href="http://en.wikipedia.org/wiki/Overfitting">overfitting</a>. Random forest can be prone to overfitting, especially when working with relatively small datasets. You should be suspicious if your model is making "too good" of predictions on our test set.</p>

    <p>One way to overfitting is to only use really relevant features in your model. While this isn't always cut and dry, using a feature selection technique (like the one mentioned previously) can make it a lot easier.</p>
    
    
    <img src="../static/img/predicting_wine_type.png"/>

    
    

  <h5>Regression</h5>
    <p>Yep. It does regression too.</p>

    <p>I've found that random forest--unlike other algorithms--does really well learning on categorical variables or a mixture of categorical and real variables. Categorical variables with high cardinality (# of possible values) can be tricky, so having something like this in your back pocket can come in quite useful.</p>
    
    


<h4>A Short Python Example</h4>
  <p>Scikit-Learn is a great way to get started with random forest. The scikit-learn API is extremely consistent across algorithms, so you horse race and switch between models very easily. A lot of times I start with something simple and then move to random forest.</p>
  <p>One of the best features of the random forest implementation in scikit-learn is the <code>n_jobs</code> parameter. This will automatically parallelize fitting your random forest based on the number of cores you want to use. <a href="http://vimeo.com/63269736" title="Vimeo - Scaling Machine Learning in Python" target="_blank">Here's a great presentation</a> by scikit-learn contributor Olivier Grisel where he talks about training a random forest on a 20 node EC2 cluster.</p>
  

  <p>Looks pretty good!</p>
  <img src="../static/img/iris_confusion_matrix.png"/>

<h4>Final Thoughts</h4>
  <p>Random forests are remarkably easy to use given how advanced they are. As with any modeling, be wary of overfitting. If you're interested in getting started with random forest in <code>R</code>, check out the <a href="http://cran.r-project.org/web/packages/randomForest/randomForest.pdf" target="_blank"><code>randomForest</code></a> package.</p>
  

  




        </article>
    </div></body></html>