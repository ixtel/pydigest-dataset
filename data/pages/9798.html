<html><body><div><div class="content html_format"><p>
      После непродолжительной, но весьма кровавой войны мне все-таки удалось откомпилировать и собрать TensorFlow для GPU с CUDA capability=3.0. Теперь можно погрузиться в него основательно, потому что машинное обучение с GPU — это быстро, легко и приятно, а без GPU — порой лишь огромная потеря времени.</p>
<p>
Попробуем запрограммировать самую простейшую логистическую регрессию.
</p><a name="habracut"/><p>
Начнем незамысловато — с импорта tensorflow:
</p><pre><code class="python">import tensorflow as tf
</code></pre><p>
Далее загружаем данные:
</p><pre><code class="python">train_X, train_Y, test_X, test_Y = load_data()
</code></pre><p>
Функцию load_data, которая загружает ваши данные, вам необходимо будет реализовать самостоятельно.
</p><p>
Структура данных предельно проста:</p><p>
 — train_X и test_X — это numpy-массивы размерностью NxF, где N — количество анализируемых инстансов (проще говоря, точек данных), а F — количество атрибутов у каждого инстанса.</p><p>
 — train_Y и test_Y — это тоже numpy-массивы, но уже размерностью NxC, где С — количество классов (в самом простом случае, 2)</p><p>
Таким образом, если train_Y[i] содержит вектор [1 0], это означает, что i-ый инстанс относится к классу 0 (поскольку в нулевом элементе вектора стоит 1).
</p><p>
Для удобства запомним важные размерности:
</p><pre><code class="python">num_features = train_X.shape[1]
num_classes = train_Y.shape[1]
</code></pre><p>
Теперь определим символьные переменные для будущих вычислений:
</p><pre><code class="python">X = tf.placeholder("float", [None, num_features])
Y = tf.placeholder("float",[None, num_classes])
</code></pre><p>
Здесь мы создаем символьную переменную X типа «float» размерностью «хоть сколько» на num_features. Количество точек данных никак не влияет на суть вычислений, поэтому можно их не фиксировать в модели. Зато потом мы сможем запускать вычисления на одной и той же модели как со 100 точками, так и с 10 миллионами.
</p><p>
C Y совершенно аналогичная ситуация.
</p><p>
Еще нужно создать переменные для хранения параметров модели:
</p><pre><code class="python">W = tf.Variable(tf.zeros([num_features,num_classes]))
B = tf.Variable(tf.zeros([num_classes]))
</code></pre><p>
А теперь описываем модель:
</p><pre><code class="python">pY = tf.nn.softmax(tf.matmul(X, W) + B)
</code></pre><p>
В данном случае мы используем наипростейшую линейную модель вида y=Wx+B, завернутую в softmax. В pY будет храниться результат расчета, то есть предсказанный Y, поэтому он также будет иметь размерность NxC.
</p><p>
Далее определяем функцию потерь:
</p><pre><code class="python">cost_fn = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pY, Y))
</code></pre><p>
Можно, конечно, функцию потерь и более явно описать:
</p><pre><code class="python">cost_fn = -tf.reduce_sum(Y * tf.log(pY))
</code></pre><p>
После чего создаем оптимизатор:
</p><pre><code class="python">opt = tf.train.AdamOptimizer(0.01).minimize(cost_fn)
</code></pre><p>
Вместо Adam желающие могут использовать и старый добрый градиентный спуск:
</p><pre><code class="python">opt = tf.train.GradientDescentOptimizer(0.01).minimize(cost_fn)
</code></pre><p>
До сих пор мы только описывали модель и ее параметры, но никакие вычисления пока не проводились. И вот, наконец-то, мы переходим к расчетам. Сначала создаем сессию и инициализируем все переменные.
</p><pre><code class="python">sess = tf.Session()
init = tf.initialize_all_variables()
sess.run(init)
</code></pre><p>
А теперь поехали:
</p><pre><code class="python">num_epochs = 40
for i in range(num_epochs):
  sess.run(opt, feed_dict={X:train_X, Y:train_Y})
</code></pre><p>
Здесь просто происходит вызов ранее описанного оптимизатора, которому передаются данные: символьная переменная X получает данные из train_X, а Y — из train_Y. Таким образом, в модели больше нет никакой неопределенности и все вычисления выполнимы.
</p><p>
В данном случае, мы проводим 40 итераций, в каждой из которых обсчитываются все имеющиеся тренировочные данные.</p><p>
Если данных много, то можно разделить их на блоки (batch'и) и учить последовательно, то есть каждая эпоха будет включать цикл по всем блокам.
</p><p>
Для оценки качества модели надо ввести критерий оценки:
</p><pre><code class="python">accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pY,1), tf.argmax(Y,1)), "float"))
</code></pre><p>
Здесь сравнивается реальный класс и предсказанный класс, после чего результат сравнения преобразуется из типа boolean в тип float (true — 1, false — 0), а затем по всем этим сравнениям берется среднее.</p><p>
Обратите внимание, что предыдущая строка только определяет критерий, а не вычисляет его. Вышеописанная формула, также как и оптимизатор, зависят от символьных переменных, которые надо наполнить данными. Поэтому для выполнения расчета снова запускаем run:
</p><pre><code class="python">accuracy_value = sess.run(accuracy, feed_dict={X:test_X, Y:test_Y})
</code></pre><p>
Однако теперь мы уже используем тестовые, а не тренировочные данные.
</p><p>
Вот и все. Это было совсем несложно.

      
      </p><p class="clear"/>
    </div>

    
  </div></body></html>