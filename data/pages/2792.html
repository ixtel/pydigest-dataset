<html><body><div><div class="content html_format"><p>
      Привет, хабр!</p>

<img src="https://habrastorage.org/files/d23/c1c/fd5/d23c1cfd5c02473f945f066df5679132.jpg"/>
<p>
В предыдущей статье (</p><a href="http://habrahabr.ru/post/247751/">«Введение в машинное обучение с помощью Python и Scikit-Learn»</a><p>) мы с вами познакомились с основными этапами решения задач машинного обучения. Сегодня подробнее поговорим о техниках, которые позволяют заметно увеличить качество разрабатываемых алгоритмов. Одна из таких техник — </p><b>Feature Engineering</b><p>. Сразу отметим, что это своего рода искусство, обучиться которому можно только прорешав огромное количество задач. Тем не менее, с опытом вырабатываются некие общие подходы, которыми хотелось бы поделиться в данной статье.
</p><a name="habracut"/><p>
Итак, как мы уже знаем, почти любая задача начинается с создания (</p><b>Engineering</b><p>) и отбора (</p><b>Selection</b><p>) признаков. Методы отбора признаков изучены достаточно хорошо и уже существует большое количество алгоритмов для этого (подробнее о них поговорим в следующий раз). А вот задача создания признаков является своего рода искусством и полностью ложится на плечи Data Scientist'а. Стоит отметить, что именно эта задача зачастую является самой сложной на практике и именно благодаря удачному отбору и созданию признаков получаются очень качественные алгоритмы. Зачастую на </p><a href="http://www.kaggle.com">kaggle.com</a><p> побеждают сами по себе простые алгоритмы с хорошо отобранными признаками (отличные примеры — </p><a href="https://kaggle2.blob.core.windows.net/wiki-files/327/e4cd1d25-eca9-49ca-9593-b254a773fe03/Market%20Makers%20-%20Milestone%201%20Description%20V2%201.pdf">Heritage Provider Network Health Prize</a><p> или </p><a href="http://pslcdatashop.org/KDDCup/workshop/papers/kdd2010ntu.pdf">Feature Engineering and Classifier Ensemble for KDD Cup 2010</a><p>)
</p><p>
Наверное, самый известный и понятный пример Feature Engineering многие из вас уже видели в </p><a href="https://www.coursera.org/course/ml">курсе Andrew Ng</a><p>. Пример заключался в следующем: с помощью линейных моделей прогнозируется цена дома в зависимости от множества признаков, среди которых есть такие, как длина дома и ширина. Линейная регрессия в данном случае прогнозирует цену дома, как линейную комбинацию ширины и длины. Но ведь любому здравому человеку понятно, что цена дома в первую очередь зависит от площади дома, которая никак не выражается через линейную комбинацию длины и ширины. Поэтому, качество алгоритма существенным образом увеличивается, если длину и ширину заметить на их произведение. Тем самым, мы получим новый признак, который наиболее сильно влияет на цену дома, а также сократим размерность признакового пространства. На мой взгляд, это самый простой и наглядный пример создания признаков. Заметим, что очень сложно придумать метод, который для любой наперед заданной задачи давал бы технику построения признаков. Именно поэтому пост и называется «Искусство Feature Engineering». Тем не менее, существует ряд простых методов и техник, которыми бы хотелось поделиться из собственного опыта:

</p><h3>Категориальные признаки</h3><p>
Допустим, у наших обьектов есть признаки, которые принимают значения на конечном наборе. Например, цвет (</p><b>color</b><p>), который может быть синим (</p><b>blue</b><p>), красным (</p><b>red</b><p>), зеленым (</p><b>green</b><p>), или его значение может быть неизвестно (</p><b>unknown</b><p>). В таком случае бывает полезным добавлять признаки вида </p><b>is_red</b><p>, </p><b>is_blue</b><p>, </p><b>is_green</b><p>, </p><b>is_red_or_blue</b><p> и другие возможные комбинации.

</p><h3>Даты и время</h3><p>
Если среди признаков есть дата или время — как правило, очень часто помогает добавлять признаки, соответствующие времени дня, количеству прошеднего времени с определенного момента, выделение сезонов, времен года, кварталов. Также помогает разделение времени на часы, минуты и секунды (если время дано в </p><b>Unix-Time</b><p> или </p><b>ISO</b><p> формате). Вариантов в данном месте масса, каждый из которых подбирается под конкретну задачу.

</p><h3>Числовые переменные</h3><p>
Если переменная вещественная, часто помогает ее округление или разделение на целую и вещественную часть (с последующей нормализацией). Или же, часто помогает приведение числового признака в категориальный. Например, если есть такой признак как масса, то можно ввести признаки вида </p><b>«масса больше X»</b><p>, </p><b>«масса от X до Y»</b><p>. 

</p><h3>Обработка строковых признаков</h3><p>
Если есть признак, значением которого есть конечное количество строк — то стоит не забывать, что в самих строках зачастую содержится информация. Наглядным примером является задача </p><a href="https://www.kaggle.com/c/titanic-gettingStarted">Titanic: Machine Learning from Disaster</a><p>, в которой имена участников плавания имели приставки </p><b>«Mr.»</b><p>, </p><b>«Mrs.»</b><p> и </p><b>«Miss.»</b><p>, по которым легко извлечь половой признак.

</p><h3>Результаты других алгоритмов</h3><p>
Часто в качестве признака также можно добавить результат работы других алгоритмов. Например, если решается задача классификации, можно сначала решить вспомогательную задачу кластеризации, и в качестве признака в первоначальной задаче взять кластер обьекта. Это обычно происходит на основе первичного анализа данных в случае, когда обьекты хорошо кластеризуются.

</p><h3>Агрегированные признаки</h3><p>
Имеет также смысл добавлять признаки, которые агрегируют признаки некоторого обьекта, тем самым также сокращая размерность признакового описания. Как правило, это полезно в задачах, в которых один обьект содержит несколько однотипных параметров. Например, человек, имеющий несколько автомобилей разной стоимости. В данном случае можно рассмотреть признаки, соответствующие максимальной/минимальной/средней стоимости автомобиля этого человека.

</p><h3>Добавление новых признаков</h3><p>
Этот пункт скорее надо отнести больше к практическим задачам из реальной жизни, нежели к соревнованиям по машинному обучению. Более подробно об этом будет отдельная статья, сейчас отметим лишь, что зачастую, чтобы эффективно решить задачу, необходимо быть экспертом в конкретной области и понимать, что влияет на конкретную целевую переменную. Возвращаясь к примеру с ценой квартиры, каждый знает, что цена зависит в первую очередь от площади, однако, в более сложной предметной области такие заключения делать достаточно сложно.
</p><p>
Итак, мы рассмотрели несколько техник создания (</p><b>Engineering</b><p>) признаков в задачах машинного обучения, которые могут помочь заметно увеличить качество существующих алгоритмов. В следующий раз мы подробнее поговорим о методах отбора (</p><b>Selection</b><p>) признаков. К счастью, там будет все проще, потому как для отбора признаков есть уже разработанные техники, в то время как создание признаков, как уже, наверное, заметил читатель, является искусством!

      
      </p><p class="clear"/>
    </div>

    
  </div></body></html>