<html><body><div><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-highly-interpretable-sklearn-compatible-classifier-based-on-decision-rules" class="anchor" href="#highly-interpretable-sklearn-compatible-classifier-based-on-decision-rules" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Highly interpretable, sklearn-compatible classifier based on decision rules</h1>

<p>This is a scikit-learn compatible wrapper for the Bayesian Rule List classifier 
developed by <a href="http://projecteuclid.org/euclid.aoas/1446488742">Letham et al., 2015</a> (see <a href="http://lethalletham.com/">Letham's original code</a>), 
extended by a minimum description length-based discretizer (<a href="http://sci2s.ugr.es/keel/pdf/algorithm/congreso/fayyad1993.pdf">Fayyad &amp;
Irani, 1993</a>) for continuous data.</p>

<p>It produces rule lists, which makes trained classifiers <strong>easily interpretable 
to human experts</strong>, and is competitive with state of the art classifiers such as 
random forests or SVMs.</p>

<p>For example, an easily understood Rule List model of the well-known Titanic dataset:</p>

<pre><code>IF male AND adult THEN survival probability: 21% (19% - 23%)
ELSE IF 3rd class THEN survival probability: 44% (38% - 51%)
ELSE IF 1st class THEN survival probability: 96% (92% - 99%)
ELSE survival probability: 88% (82% - 94%)
</code></pre>

<p>Letham et al.'s approach only works on discrete data. However, this approach can still be used
on continuous data after discretization. The RuleListClassifier class also includes a discretizer 
that can deal with continuous data (using <a href="http://sci2s.ugr.es/keel/pdf/algorithm/congreso/fayyad1993.pdf">Fayyad &amp; Irani's</a> 
minimum description length principle criterion, based on an implementation by 
<a href="https://github.com/navicto/Discretization-MDLPC">navicto</a>).</p>

<h1><a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Usage</h1>

<p>The project requires <a href="http://www.borgelt.net/pyfim.html">pyFIM</a>, <a href="http://scikit-learn.org/stable/install.html">scikit-learn</a>, 
and <a href="http://pandas.pydata.org/">pandas</a> to run.</p>

<p>The included <code>RuleListClassifier</code> works as a scikit-learn estimator, with a <code>model.fit(X,y)</code> method
which takes training data <code>X</code> (numpy array or pandas DataFrame; continuous, categorical or mixed data)
and labels <code>y</code>. Numerical data in <code>X</code> is automatically discretized. To prevent discretization (e.g. to protect
columns containing categorical data represented as integers), pass the list of protected column names
in the <code>fit</code> method, e.g. <code>model.fit(X,y,undiscretized_features=['CAT_COLUMN_NAME'])</code> (entries in 
undiscretized columns will be converted to strings and used as categorical values). </p>

<p>The learned rules of a trained model can be displayed simply by casting the object as a string, e.g. <code>print model</code>, 
or by using the <code>model.tostring(decimals=1)</code> method and optionally specifying the rounding precision.</p>

<p>Usage example:</p>

<div class="highlight highlight-source-python"><pre><span class="pl-k">from</span> RuleListClassifier <span class="pl-k">import</span> <span class="pl-k">*</span>
<span class="pl-k">from</span> sklearn.datasets.mldata <span class="pl-k">import</span> fetch_mldata
<span class="pl-k">from</span> sklearn.cross_validation <span class="pl-k">import</span> train_test_split
<span class="pl-k">from</span> sklearn.ensemble <span class="pl-k">import</span> RandomForestClassifier

feature_labels <span class="pl-k">=</span> [<span class="pl-s"><span class="pl-pds">"</span>#Pregnant<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>Glucose concentration test<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>Blood pressure(mmHg)<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>Triceps skin fold thickness(mm)<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>2-Hour serum insulin (mu U/ml)<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>Body mass index<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>Diabetes pedigree function<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>Age (years)<span class="pl-pds">"</span></span>]

data <span class="pl-k">=</span> fetch_mldata(<span class="pl-s"><span class="pl-pds">"</span>diabetes<span class="pl-pds">"</span></span>) <span class="pl-c"># get dataset</span>
y <span class="pl-k">=</span> (data.target<span class="pl-k">+</span><span class="pl-c1">1</span>)<span class="pl-k">/</span><span class="pl-c1">2</span> <span class="pl-c"># target labels (0 or 1)</span>
Xtrain, Xtest, ytrain, ytest <span class="pl-k">=</span> train_test_split(data.data, y) <span class="pl-c"># split</span>

<span class="pl-c"># train classifier (allow more iterations for better accuracy)</span>
model <span class="pl-k">=</span> RuleListClassifier(<span class="pl-v">max_iter</span><span class="pl-k">=</span><span class="pl-c1">10000</span>, <span class="pl-v">class1label</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>diabetes<span class="pl-pds">"</span></span>, <span class="pl-v">verbose</span><span class="pl-k">=</span><span class="pl-c1">False</span>)
model.fit(Xtrain, ytrain, <span class="pl-v">feature_labels</span><span class="pl-k">=</span>feature_labels)

<span class="pl-c1">print</span> <span class="pl-s"><span class="pl-pds">"</span>RuleListClassifier Accuracy:<span class="pl-pds">"</span></span>, model.score(Xtest, ytest), <span class="pl-s"><span class="pl-pds">"</span>Learned interpretable model:<span class="pl-cce">\n</span><span class="pl-pds">"</span></span>, model
<span class="pl-c1">print</span> <span class="pl-s"><span class="pl-pds">"</span>RandomForestClassifier Accuracy:<span class="pl-pds">"</span></span>, RandomForestClassifier().fit(Xtrain, ytrain).score(Xtest, ytest)
<span class="pl-s"><span class="pl-pds">"""</span></span>
<span class="pl-s">**Output:**</span>
<span class="pl-s">RuleListClassifier Accuracy: 0.776041666667 Learned interpretable model:</span>
<span class="pl-s">Trained RuleListClassifier for detecting diabetes</span>
<span class="pl-s">==================================================</span>
<span class="pl-s">IF Glucose concentration test : 157.5_to_inf THEN probability of diabetes: 81.1% (72.5%-72.5%)</span>
<span class="pl-s">ELSE IF Body mass index : -inf_to_26.3499995 THEN probability of diabetes: 5.2% (1.9%-1.9%)</span>
<span class="pl-s">ELSE IF Glucose concentration test : -inf_to_103.5 THEN probability of diabetes: 14.4% (8.8%-8.8%)</span>
<span class="pl-s">ELSE IF Age (years) : 27.5_to_inf THEN probability of diabetes: 59.6% (51.8%-51.8%)</span>
<span class="pl-s">ELSE IF Glucose concentration test : 103.5_to_127.5 THEN probability of diabetes: 15.9% (8.0%-8.0%)</span>
<span class="pl-s">ELSE probability of diabetes: 44.7% (29.5%-29.5%)</span>
<span class="pl-s">=================================================</span>
<span class="pl-s"/>
<span class="pl-s">RandomForestClassifier Accuracy: 0.729166666667</span>
<span class="pl-s"><span class="pl-pds">"""</span></span></pre></div>
</article>
  </div></body></html>