<html><body><div><div class="entry-content"><p>For a long time I thought that the most interesting problems in my field were in scalability. Some people may be more interested in scaling, and others might be more into slick interfaces and fast animations.  But for me, scalability has continued to be my passion.  For awhile though, it was a unicorn.  That unattainable thing that I <strong>wanted</strong> to work on but couldn’t find anywhere to do it at.  That is, until I started work at Future US.</p><p>Future is a media company.  Originally they started in old media focusing heavily on gaming and tech magazines.  Eventually the internet became prominent in everyday life, so more of their old media properties made the transition to the web.  The one that really matters to me though is PC Gamer.  I’ve been a huge fan of PC Gamer since I was about 7 years old.  I still have fond memories getting demo disks in the mail with my subscription.</p><p>When I was hired at Future it was to help facilitate the move of PC Gamer from its existing platform (WordPress) to Django.  Future had experienced success moving other properties to Django, so it made sense to do it with PC Gamer.  When it eventually came time to implement our caching layer, we thought about a lot of different ways that it could be done.  Varnish came up as an option, but we decided against it since nobody on the team had experience configuring it (and people elsewhere in the organization had experienced issues with it).  Eventually we settled on having Nginx serve pages directly from Memcache.  For us, this method works great because PC Gamer doesn’t have a lot of interaction (its almost completely consumption from the user end).  Anything that does require back-and-forth between the server is handled via javascript, which makes full page caching super easy to do.</p> <figure id="attachment_1812" class="wp-caption aligncenter"><a href="http://www.re-cycledair.com/wp-content/uploads/2014/10/Spark-server-architecture.png"><img src="http://www.re-cycledair.com/wp-content/uploads/2014/10/Spark-server-architecture.png" alt="The high level architecture for pc gamer." class="size-full wp-image-1812"/></a><figcaption class="wp-caption-text">The high level architecture for pc gamer.</figcaption></figure><p>So how does it all work?  The image above describes PC Gamer’s server architecture from a high level.  Its pretty basic and works quite well for us.  We end up having two types of requests: cache hits &amp; cache misses.  The flow for a cache hit is: request -&gt; load balancer -&gt; nginx -&gt; memcache -&gt; your browser.  The flow for a cache miss is: request -&gt; load balancer -&gt; nginx -&gt; application server (django) -&gt; (store page in cache) -&gt; your browser.</p><p>Since we’re basically running a static site, deciding what content to cache is easy: EVERYTHING!<br/> <figure id="attachment_1816" class="wp-caption aligncenter"><a href="http://www.re-cycledair.com/wp-content/uploads/2014/10/d053e99d38c58ae66c7cd0e61f2944605e1a4453514b0d676c90fd8067d4706d.jpg"><img src="http://www.re-cycledair.com/wp-content/uploads/2014/10/d053e99d38c58ae66c7cd0e61f2944605e1a4453514b0d676c90fd8067d4706d.jpg" alt="Cache all the things!" class="size-full wp-image-1816"/></a><figcaption class="wp-caption-text">Cache all the things!</figcaption></figure><br/> Luckily for us Django already has a nice way of doing this: The <a href="https://docs.djangoproject.com/en/1.7/topics/cache/#the-per-site-cache" title="Django Per-Site Cache" target="_blank">per-site cache</a>.  But it is not without its issues.  First of all, the cache keys it creates are insane.  We needed something a little simpler for our setup so Nginx could build the cache key of the current request on the fly.</p><h3>How It Works</h3><p>The meat and potatoes of overriding Django’s per-site cache key comes in the `_generate_cache_key` function.</p><div class="wp_syntax"><table><tr><td class="code"><pre class="python"><span>def</span> _generate_cache_key<span>(</span>request<span>,</span> method<span>,</span> headerlist<span>,</span> key_prefix<span>)</span>:
    <span>if</span> key_prefix <span>is</span> <span>None</span>:
        key_prefix <span>=</span> settings.<span>CACHE_MIDDLEWARE_KEY_PREFIX</span>
    cache_key <span>=</span> key_prefix + get_absolute_uri<span>(</span>request<span>)</span>
    <span>return</span> hashlib.<span>md5</span><span>(</span>cache_key<span>)</span>.<span>hexdigest</span><span>(</span><span>)</span></pre></td></tr></table></div><p>To make things easier for Nginx to understand we just take the url and md5 it.  Simple!</p><p>On the Nginx side of things, the setup is equally simple.</p><div class="wp_syntax"><table><tr><td class="code"><pre class="nginx">        set            $combined_string "$host$request_uri";
        set_by_lua     $memcached_key "return ngx.md5(ngx.arg[1])" $combined_string;
 
        # 404 for cache miss
        # 502 for memcached down
        error_page     404 502 504 = @fallback;
 
        memcached_pass {{ cache.private_ip }}:11211;</pre></td></tr></table></div><p>All this setup does is take the MD5 of the host + request URI and then check to see if that cache key exists in memcache.  If it does then we serve the content at that cache key, if it doesn’t we fall back to our Django application servers and they generate the page.</p><p>Thats it.  Seriously.  It’s simple, extremely fast, and works for us.  Your mileage may vary, but if you have relatively simple caching requirements I highly suggest looking into this method before looking at something like Varnish.  It could help you remove quite a bit of complexity from your setup.</p></div></div></body></html>