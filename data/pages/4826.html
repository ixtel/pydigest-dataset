<html><body><div><div class="post_body"><p>If you are new to data mining, first go through the introduction to <a href="http://dataaspirant.com/for-beginners/">Data Mining</a>.</p>

<p>Fisher's Linear Discriminator is a linear supervised classifier. Supervised classification refers to the classification being carried out where labeled training examples are available to learn the classifier.</p>

<p>First we go through the concept derivation and next we implement it in Python.</p>

<p>Fisher Linear Discriminant is used to map a d-dimentional data to one dimentional data using a projection vector W such that it maps a vector X to a scalar W<sup>T</sup>X(Note: W<sup>T</sup>X is a scalar). Classification is then carried out in one dimentional space.</p>

<p>i.e For a two class problem the classifier is</p>

<p>Decide X ∈ C-1 if W<sup>T</sup>X + w0 &gt; 0 and X ∈ C-0 if W<sup>T</sup>X + w0 &lt; 0<br/>Hence one can think of the best V as the direction along which the two classes are well separated.</p>

<p><img src="http://si.uploads.im/1SfYn.png" alt="Linear Discriminant"/></p>

<p> </p>

<p>We project the data along the direction of W. Separation between points of different classes in the projected data is a good way to rate how good is W. In the above fingure (b), the data is projected along the line bb'(W). In this direction the separation between the two classes is more. So we can easily proceed with the classification.</p>

<p>So, now our task is to find the axis W which will give the optimum separation between two class data points. After we have found the W, we project the data along that axis and we will find the separating hyperplane f(x).</p>

<p>Fisher Linear Discriminant is a formal way to find this optimum separation direction. Let M0 and M1 are the means of data from the two classes C-0 and C-1 respectively.</p>

<p>M<sub>0</sub> = (1/n0)<strong>∑</strong><sub>Xi∈C-0 </sub>X<sub>i </sub>and M<sub>1</sub> = (1/n1)<strong>∑</strong><sub>Xi∈C-1</sub> X<sub>i</sub>  <br/><br/> The corresponding means of projected Data would be</p>

<p><br/> m<sub>0</sub> = W<sup>T</sup>M<sub>0 </sub>and m<sub>1</sub> = W<sup>T</sup>M<sub>1</sub></p>

<p>The difference (m<sub>0</sub> - m<sub>1</sub>) gives us an idea of the separation between samples of the two classes after projecting the data onto the direction W.<br/>Hence we want a W such that it maximizes (m<sub>0</sub> - m<sub>1</sub>)<sup>2</sup>. We can do this by increasing magnitude of W linearly. But that is of no use. We have to make it as independent of W and relative to the variances.</p>

<p>So we can define a Fisher's maximizing criterion function as</p>

<p>J(W) = (m<sub>1</sub> - m<sub>0</sub>)<sup>2</sup> / (s<sub>0</sub><sup>2</sup> + s<sub>1</sub><sup>2</sup>)</p>

<p>Where s<sub>0</sub> and s<sub>1 </sub>are proportional to variences of class 0 and class 1 respectively</p>

<p>We now write J into more convenient form</p>

<p> (m<sub>1</sub> - m<sub>0</sub>)<sup>2</sup> = (W<sup>T</sup>M<sub>1</sub> - W<sup>T</sup>M<sub>0</sub>)<sup>2</sup></p>

<p>   = W<sup>T</sup>(M<sub>1</sub> - M<sub>0</sub>)(M<sub>1</sub> - M<sub>0</sub>)<sup>T</sup>W</p>

<p>   = W<sup>T</sup>S<sub>B</sub>W</p>

<p>where S<sub>B</sub> = (M<sub>1</sub> - M<sub>0</sub>)(M<sub>1</sub> - M<sub>0</sub>)<sup>T</sup></p>

<p>This SB is a dxd matrix of rank 1. It is called <em>"Between class scatter matrix"</em>.</p>

<p>Similarly we can write s<sub>0</sub> and s<sub>1</sub> also as quadratic forms</p>

<p>s<sub>0</sub><sup>2</sup> = W<sup>T</sup>[<strong>∑</strong><sub>Xi∈C-0</sub>(X<sub>i</sub> - M<sub>0</sub>)(X<sub>i</sub> - M<sub>0</sub>)<sup>T</sup>]W</p>

<p>s<sub>1</sub><sup>2</sup> = W<sup>T</sup>[<strong>∑</strong><sub>Xi∈C-1</sub>(X<sub>i</sub> - M<sub>0</sub>)(X<sub>i</sub> - M<sub>0</sub>)<sup>T</sup>]W</p>

<p>and we can write</p>

<p>s<sub>0</sub><sup>2</sup> + s<sub>1</sub><sup>2 </sup> = W<sup>T</sup>S<sub>W</sub>W</p>

<p>Where S<sub>W</sub> = <strong>∑</strong><sub>Xi∈C-0</sub>(X<sub>i</sub> - M<sub>0</sub>)(X<sub>i</sub> - M<sub>0</sub>)<sup>T</sup> + <strong>∑</strong><sub>Xi∈C-1</sub>(X<sub>i</sub> - M<sub>0</sub>)(X<sub>i</sub> - M<sub>0</sub>)<sup>T</sup></p>

<p>This S<sub>w</sub> also a dxd matrix and called "<em>With in class scatter matrix</em>".</p>

<p>That we can write  <strong>J(W) = (W<sup>T</sup>S<sub>B</sub>W) / (W<sup>T</sup>S<sub>W</sub>W)</strong> and is a ratio of two quadratic forms. We can find a max point by derivating it with respect to W and equating to Zero. So, that gives</p>

<p><strong>(2S<sub>B</sub>W / W<sup>T</sup>S<sub>W</sub>W) — (W<sup>T</sup>S<sub>B</sub>W / (W<sup>T</sup>S<sub>W</sub>W)<sup>2</sup>)2S<sub>W</sub>W = 0</strong></p>

<p>W<sup>T</sup>S<sub>W</sub>W, (W<sup>T</sup>S<sub>B</sub>W / (W<sup>T</sup>S<sub>W</sub>W)<sup>2</sup>) are scalars here. So we can write as follows</p>

<p>S<sub>W</sub>W = λS<sub>B</sub>W</p>

<p>Thus any maximization of the J(W) has to satisfy S<sub>W</sub>W = λS<sub>B</sub>W for some constant λ. This is a generalized eigen value problem.</p>

<p>W = λS<sub>W</sub><sup>-1</sup>S<sub>B</sub>W</p>

<p>here S<sub>B</sub>W = (M<sub>1</sub> - M<sub>0</sub>)(M<sub>1</sub> - M<sub>0</sub>)<sup>T</sup>W = k(M<sub>1</sub> - M<sub>0</sub>)              ∴ (M<sub>1</sub> - M<sub>0</sub>)<sup>T</sup>W is a scalar constant</p>

<p>Here we are interested in direction vector so, we can neglect constants.</p>

<p>⇒ <strong>W = S<sub>W</sub><sup>-1</sup>(M<sub>1</sub> - M<sub>0</sub>) </strong></p>

<p>So finally we have found the direction vector W that maximizes the separation between the projections of the two class data.</p>

<p>Click <a href="../../implementing-fishers-lda-in-python/">here</a> for implementation of Fisher's LDA in python.</p></div>
        

</div></body></html>