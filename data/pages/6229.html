<html><body><div><article class="page landing-page">
        
        <details>
          <summary>
            <h4>Peer-reviewed Evaluations</h4>
          </summary>
          <p>spaCy is committed to rigorous evaluation under standard methodology.  Two papers in 2015 confirm that:</p>
          <ol>
            <li>spaCy is the fastest syntactic parser in the world;</li>
            <li>Its accuracy is within 1% of the best available;</li>
            <li>The few systems that are more accurate are 20× slower or more.</li>
          </ol>
          <p>spaCy v0.84 was evaluated by researchers at Yahoo! Labs and Emory University, as part of a survey paper benchmarking the current state-of-the-art dependency parsers (<a href="http://aclweb.org/anthology/P/P15/P15-1038.pdf">Choi et al., 2015</a>).</p>
          <table>
            <thead>
              <tr>
                <th>System</th>
                <th>Language</th>
                <th>Accuracy</th>
                <th>Speed</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>spaCy v0.97</td>
                <td>Cython</td>
                <td>91.8</td>
                <td>13,000 (est.)</td>
              </tr>
              <tr>
                <td>ClearNLP</td>
                <td>Java</td>
                <td>91.7</td>
                <td>10,271</td>
              </tr>
              <tr>
                <td>CoreNLP</td>
                <td>Java</td>
                <td>89.6</td>
                <td>8,602</td>
              </tr>
              <tr>
                <td>MATE</td>
                <td>Java</td>
                <td>92.5</td>
                <td>550</td>
              </tr>
              <tr>
                <td>Turbo</td>
                <td>C++</td>
                <td>92.4</td>
                <td>349</td>
              </tr>
              <tr>
                <td>Yara</td>
                <td>Java</td>
                <td>92.3</td>
                <td>340</td>
              </tr>
            </tbody>
          </table>
          <p>Discussion with the authors led to accuracy improvements in spaCy, which have been accepted for publication in EMNLP, in joint work with Macquarie University (<a href="//aclweb.org/anthology/D/D15/D15-1162.pdf">Honnibal and Johnson, 2015</a>). </p>
        </details>
        <details>
          <summary>
            <h4>How does spaCy compare to NLTK?</h4>
          </summary>
          <div class="columnar">
            <div class="col">
              <h5>spaCy</h5>
              <ul>
                <li class="pro">Over 400 times faster</li>
                <li class="pro">State-of-the-art accuracy</li>
                <li class="pro">Tokenizer maintains alignment</li>
                <li class="pro">Powerful, concise API</li>
                <li class="pro">Integrated word vectors</li>
                <li class="con">English only (at present)</li>
              </ul>
            </div>
            <div class="col">
              <h5>NLTK</h5>
              <ul>
                <li class="con">Slow</li>
                <li class="con">Low accuracy</li>
                <li class="con">Tokens do not align to original string</li>
                <li class="con">Models return lists of strings</li>
                <li class="con">No word vector support</li>
                <li class="pro">Multiple languages</li>
              </ul>
            </div>
          </div>
        </details>
        <details>
          <summary>
            <h4>How does spaCy compare to CoreNLP?</h4>
          </summary>
          <div class="columnar">
            <div class="col">
              <h5>spaCy</h5>
              <ul>
                <li class="pro">50% faster</li>
                <li class="pro">More accurate parser</li>
                <li class="pro">Word vectors integration</li>
                <li class="pro">Minimalist design</li>
                <li class="pro">Great documentation</li>
                <li class="con">English only</li>
                <li class="pro">Python</li>
              </ul>
            </div>
            <div class="col">
              <h5>CoreNLP</h5>
              <ul>
                <li class="pro">More accurate NER</li>
                <li class="pro">Coreference resolution</li>
                <li class="pro">Sentiment analysis</li>
                <li class="con">Little documentation</li>
                <li class="pro">Multiple languages</li>
                <li class="neutral">Java</li>
              </ul>
            </div>
          </div>
        </details>
        <details>
          <summary>
            <h4>How does spaCy compare to ClearNLP?</h4>
          </summary>
          <div class="columnar">
            <div class="col">
              <h5>spaCy</h5>
              <ul>
                <li class="pro">30% faster</li>
                <li class="pro">Well documented</li>
                <li class="con">English only</li>
                <li class="neutral">Equivalent accuracy</li>
                <li class="pro">Python</li>
              </ul>
            </div>
            <div class="col">
              <h5>ClearNLP</h5>
              <ul>
                <li class="pro">Semantic Role Labelling</li>
                <li class="pro">Model for biology/life-science</li>
                <li class="pro">Multiple Languages</li>
                <li class="neutral">Equivalent accuracy</li>
                <li class="neutral">Java</li>
              </ul>
            </div>
          </div>
        </details>
        
        
        <p><a href="https://api.spacy.io/displacy">displaCy</a> lets you peek inside spaCy's syntactic parser, as it reads a sentence word-by-word. By repeatedly choosing from a small set of actions, it links the words together according to their syntactic structure. This type of representation powers a wide range of technologies, from translation and summarization, to sentiment analysis and algorithmic trading.  <a href="/blog/displacy">Read more.</a></p>
        
        <details>
          <summary>
            <h4>Load resources and process text</h4>
          </summary>
          <pre class="language-python"><code>from spacy.en import English
nlp = English()
doc = nlp(u'Hello, world. Here are two sentences.')
</code></pre>
        </details>
        <details>
          <summary>
            <h4>Multi-threaded generator (using OpenMP. No GIL!)</h4>
          </summary>
          <pre class="language-python"><code>texts = [u'One document.', u'...', u'Lots of documents']
# .pipe streams input, and produces streaming output
iter_texts = (texts[i % 3] for i in xrange(100000000))
for i, doc in enumerate(nlp.pipe(iter_texts, batch_size=50, n_threads=4)):
    assert doc.is_parsed
    if i == 100:
        break
</code></pre>
        </details>
        <details>
          <summary>
            <h4>Get tokens and sentences</h4>
          </summary>
          <pre class="language-python"><code>token = doc[0]
sentence = next(doc.sents)
assert token is sentence[0]
assert sentence.text == 'Hello, world.'
</code></pre>
        </details>
        <details>
          <summary>
            <h4>Use integer IDs for any string</h4>
          </summary>
          <pre class="language-python"><code>hello_id = nlp.vocab.strings['Hello']
hello_str = nlp.vocab.strings[hello_id]

assert token.orth  == hello_id  == 3125
assert token.orth_ == hello_str == 'Hello'
</code></pre>
        </details>
        <details>
          <summary>
            <h4>Get and set string views and flags</h4>
          </summary>
          <pre class="language-python"><code>assert token.shape_ == 'Xxxxx'
for lexeme in nlp.vocab:
    if lexeme.is_alpha:
        lexeme.shape_ = 'W'
    elif lexeme.is_digit:
        lexeme.shape_ = 'D'
    elif lexeme.is_punct:
        lexeme.shape_ = 'P'
    else:
        lexeme.shape_ = 'M'
assert token.shape_ == 'W'
</code></pre>
        </details>
        <details>
          <summary>
            <h4>Export to numpy arrays</h4>
          </summary>
          <pre class="language-python"><code>from spacy.attrs import ORTH, LIKE_URL, IS_OOV

attr_ids = [ORTH, LIKE_URL, IS_OOV]
doc_array = doc.to_array(attr_ids)
assert doc_array.shape == (len(doc), len(attr_ids))
assert doc[0].orth == doc_array[0, 0]
assert doc[1].orth == doc_array[1, 0]
assert doc[0].like_url == doc_array[0, 1]
assert list(doc_array[:, 1]) == [t.like_url for t in doc]
</code></pre>
        </details>
        <details>
          <summary>
            <h4>Word vectors</h4>
          </summary>
          <pre class="language-python"><code>doc = nlp("Apples and oranges are similar. Boots and hippos aren't.")

apples = doc[0]
oranges = doc[2]
boots = doc[6]
hippos = doc[8]

assert apples.similarity(oranges) &gt; boots.similarity(hippos)
</code></pre>
        </details>
        <details>
          <summary>
            <h4>Part-of-speech tags</h4>
          </summary>
          <pre class="language-python"><code>from spacy.parts_of_speech import ADV

def is_adverb(token):
    return token.pos == spacy.parts_of_speech.ADV

# These are data-specific, so no constants are provided. You have to look
# up the IDs from the StringStore.
NNS = nlp.vocab.strings['NNS']
NNPS = nlp.vocab.strings['NNPS']
def is_plural_noun(token):
    return token.tag == NNS or token.tag == NNPS

def print_coarse_pos(token):
    print(token.pos_)

def print_fine_pos(token):
    print(token.tag_)
</code></pre>
        </details>
        <details>
          <summary>
            <h4>Syntactic dependencies</h4>
          </summary>
          <pre class="language-python"><code>def dependency_labels_to_root(token):
    '''Walk up the syntactic tree, collecting the arc labels.'''
    dep_labels = []
    while token.head is not token:
        dep_labels.append(token.dep)
        token = token.head
    return dep_labels
</code></pre>
        </details>
        <details>
          <summary>
            <h4>Named entities</h4>
          </summary>
          <pre class="language-python"><code>def iter_products(docs):
    for doc in docs:
        for ent in doc.ents:
            if ent.label_ == 'PRODUCT':
                yield ent

def word_is_in_entity(word):
    return word.ent_type != 0

def count_parent_verb_by_person(docs):
    counts = defaultdict(defaultdict(int))
    for doc in docs:
        for ent in doc.ents:
            if ent.label_ == 'PERSON' and ent.root.head.pos == VERB:
                counts[ent.orth_][ent.root.head.lemma_] += 1
    return counts
</code></pre>
          
          
          
          
        </details>
        <details>
          <summary>
            <h4>Calculate inline mark-up on original string</h4>
          </summary>
          <pre class="language-python"><code>def put_spans_around_tokens(doc, get_classes):
    '''Given some function to compute class names, put each token in a
    span element, with the appropriate classes computed.

    All whitespace is preserved, outside of the spans. (Yes, I know HTML
    won't display it. But the point is no information is lost, so you can
    calculate what you need, e.g. &lt;br /&gt; tags, &lt;p&gt; tags, etc.)
    '''
    output = []
    template = '&lt;span classes="{classes}"&gt;{word}&lt;/span&gt;{space}'
    for token in doc:
        if token.is_space:
            output.append(token.orth_)
        else:
            output.append(
              template.format(
                classes=' '.join(get_classes(token)),
                word=token.orth_,
                space=token.whitespace_))
    string = ''.join(output)
    string = string.replace('\n', '')
    string = string.replace('\t', '    ')
    return string
</code></pre>
        </details>
        <details>
          <summary>
            <h4>Efficient binary serialization</h4>
          </summary>
          <pre class="language-python"><code>from spacy.tokens.doc import Doc

byte_string = doc.to_bytes()
open('moby_dick.bin', 'wb').write(byte_string)

nlp = spacy.en.English()
for byte_string in Doc.read_bytes(open('moby_dick.bin', 'rb')):
   doc = Doc(nlp.vocab)
   doc.from_bytes(byte_string)
</code></pre>
        </details>
        <details>
          <summary>
            <h4>Full documentation</h4>
          </summary>
          
        </details>
        
        <details>
          <summary>
            <h4>Updating your installation</h4>
          </summary>To update your installation:
          <pre class="language-bash"><code>$ pip install --upgrade spacy
$ python -m spacy.en.download</code></pre>
          <p>Most updates ship a new model, so you will usually have to redownload the data.</p>
        </details>
        <details open="open">
          <summary>
            <h4>conda</h4>
          </summary>
          <pre class="language-bash"><code>$ conda config --add channels spacy
$ conda install spacy
$ python -m spacy.en.download</code></pre>Sometimes conda is not up to date with the latest release. If you can't get the latest version on conda, you can always fall back to the pip install.
        </details>
        <details open="open">
          <summary>
            <h4>pip and virtualenv</h4>
          </summary>
          <p>With Python 2.7 or Python 3, using Linux or OSX, ensure that you have the following packages installed:</p>
          <pre class="language-bash"><code>build-essential python-dev</code></pre>
          <p>Then run:</p>
          <pre class="language-bash"><code>$ pip install spacy
$ python -m spacy.en.download</code></pre>
        </details>
        <p>
          The download command fetches and installs about 500mb of data, for
          the parser model and word vectors, which it installs within the spacy
          package directory.
        </p>
        <p>Usually you'll want to install spaCy within a <a href="https://virtualenv.readthedocs.org/en/latest/">virtualenv</a>, to avoid modifying system state:
          </p><pre class="language-bash"><code>virtualenv my_env_dir
source my_env_dir/bin/activate</code></pre>
        
        <details open="open">
          <summary>
            <h4>Windows (64 bit)</h4>
          </summary>We've been working on Windows support. Our tests now succeed on 64 bit builds of Windows. Installation from pip should work if you have a C++ compiler installed. Please see the <a href="https://github.com/honnibal/spaCy/README-MSVC.txt">README-MSVC.txt</a> file for instructions on compiling from source.
        </details>
        <details>
          <summary>
            <h4>Workaround for obsolete system Python</h4>
          </summary>
          <p>
            If you're stuck using a server with an old version of Python, and you
            don't have root access, I've prepared a bootstrap script to help you
            compile a local Python install.  Run:
          </p>
          <pre class="language-bash"><code>$ curl https://raw.githubusercontent.com/honnibal/spaCy/master/bootstrap_python_env.sh | bash &amp;&amp; source .env/bin/activate</code></pre>
        </details>
        <details>
          <summary>
            <h4>Compile from source</h4>
          </summary>
          <p>
            The other way to install the package is to clone the github repository,
            and build it from source.  This installs an additional dependency,
            Cython.  If you're using Python 2, I also recommend installing fabric
            and fabtools – this is how I build the project.
             
            Ensure that you have the following packages installed:
          </p>
          <pre class="language-bash"><code>build-essential python-dev git python-virtualenv</code></pre>
          <pre class="language-bash"><code>$ git clone https://github.com/honnibal/spaCy.git
$ cd spaCy
$ virtualenv .env &amp;&amp; source .env/bin/activate
$ export PYTHONPATH=`pwd`
$ pip install -r requirements.txt
$ python setup.py build_ext --inplace
$ python -m spacy.en.download
$ pip install pytest
$ py.test spacy/tests/</code></pre>
          <p>
            Python packaging is awkward at the best of times, and it's particularly tricky
            with C extensions, built via Cython, requiring large data files.  So,
            please report issues as you encounter them.
          </p>
        </details>
        <details>
          <summary>
            <h4>pypy (Unsupported)</h4>
          </summary>
          If PyPy support is a priority for you, please get in touch.  We could likely
          fix the remaining issues, if necessary.  However, the library is likely to
          be much slower on PyPy, as it's written in Cython, which produces code tuned
          for the performance of CPython.
        </details>
        <h4>What's New?</h4>
        <details>
          <summary>
            <h4>2016-01-19 v0.100: Smoother installation and model downloads, bug fixes</h4>
          </summary>
          <ul>
            <li>Redo setup.py, and remove ugly headers_workaround hack. Should result in fewer install problems.</li>
            <li>Update data downloading and installation functionality, by migrating to the Sputnik data-package manager. This will allow us to offer finer grained control of data installation in future. * Fix bug when using custom entity types in Matcher. This should work by default when using the <code>English.__call__</code> method of running the pipeline. If invoking <code>Parser.__call__</code> directly to do NER, you should call the <code>Parser.add_label()</code> method to register your entity type.</li>
            <li>Fix head-finding rules in Span.</li>
            <li>Fix problem that caused doc.merge() to sometimes hang</li>
            <li>Fix problems in handling of whitespace</li>
          </ul>
          <summary>
            <h4>2015-11-08 v0.99: Improve span merging, internal refactoring</h4>
          </summary>
          <ul>
            <li>Merging multi-word tokens into one, via the doc.merge() and span.merge() methods, no longer invalidates existing Span objects. This makes it much easier to merge multiple spans, e.g. to merge all named entities, or all base noun phrases. Thanks to @andreasgrv for help on this patch.</li>
            <li>Lots of internal refactoring, especially around the machine learning module, thinc. The thinc API has now been improved, and the spacy._ml wrapper module is no longer necessary.</li>
            <li>The lemmatizer now lower-cases non-noun, noun-verb and non-adjective words.</li>
            <li>A new attribute, .rank, is added to Token and Lexeme objects, giving the frequency rank of the word.</li>
          </ul>
          <summary>
            <h4>2015-11-03 v0.98: Smaller package, bug fixes</h4>
          </summary>
          <ul>
            <li>Remove binary data from PyPi package.</li>
            <li>Delete archive after downloading data</li>
            <li>Use updated cymem, preshed and thinc packages</li>
            <li>Fix information loss in deserialize</li>
            <li>Fix __str__ methods for Python2</li>
          </ul>
          <summary>
            <h4>2015-10-24 v0.97: Reduce load time, bug fixes</h4>
          </summary>
          <ul>
            <li>Load the StringStore from a json list, instead of a text file. Accept a file-like object in the API instead of a path, for better flexibility.</li>
            <li>Load from file, rather than path, in StringStore</li>
            <li>Fix bugs in download.py</li>
            <li>Require <code>--force</code> to over-write the data directory in download.py</li>
            <li>Fix bugs in <code>Matcher</code> and <code>doc.merge()</code></li>
          </ul>
        </details>
        <details>
          <summary>
            <h4>2015-09-21 v0.93: Bug fixes to word vectors. Rename .repvec to .vector. Rename .string attribute.</h4>
          </summary>
          <ul>
            <li>Bug fixes for word vectors.</li>
            <li>The attribute to access word vectors was formerly named <code>token.repvec</code>. This has been renamed <code>token.vector</code>. The <code>.repvec</code> name is now deprecated. It will remain available until the next version.</li>
            <li>Add <code>.vector</code> attributes to <code>Doc</code> and <code>Span</code> objects, which gives the average of their word vectors. </li>
            <li>Add a <code>.similarity</code> method to <code>Token</code>, <code>Doc</code>, <code>Span</code> and <code>Lexeme</code> objects, that performs cosine similarity over word vectors.</li>
            <li>The attribute <code>.string</code>, which gave a whitespace-padded string representation, is now renamed <code>.text_with_ws</code>. A new <code>.text</code> attribute has been added. The <code>.string</code> attribute is now deprecated. It will remain available until the next version.</li>
          </ul>
        </details>
        <details>
          <summary>
            <h4>2015-09-15 v0.90: Refactor to allow multi-lingual, better customization.</h4>
          </summary>
          <ul>
            <li>Move code out of <code>spacy.en</code> module, to allow new languages to be added more easily.</li>
            <li>New <code>Matcher</code> class, to support token-based expressions, for custom named entity rules.</li>
            <li>Users can now write to <code>Lexeme</code> objects, to set their own flags and string attributes. Values set on the vocabulary are inherited by tokens. This is especially effective in combination with the rule logic.</li>
          </ul>
        </details>
        <details>
          <summary>
            <h4>2015-07-29 v0.89: Fix Spans, efficiency</h4>
          </summary>
          <ul> 
            <li>Fix regression in parse times on very long texts. Recent versions were calculating parse features in a way that was polynomial in input length. </li>
            <li>Add tag SP (coarse tag SPACE) for whitespace tokens. Ensure entity recogniser does not assign entities to whitespace.</li>
            <li>Rename <code>Span.head</code> to <code>Span.root</code>, fix its documentation, and make it more efficient.</li>
          </ul>
        </details>
        <details>
          <summary>
            <h4> 2015-07-08 v0.88: Refactoring release.</h4>
          </summary>
          <ul>
            <li>If you have the data for v0.87, you don't need to redownload the data for this release.</li>
            <li>You can now set <code>tag=False</code>, <code>parse=False</code> or <code>entity=False</code> when creating the pipleine, to disable some of the models. See the documentation for details.</li>
            <li>Models no longer lazy-loaded.</li>
            <li>Warning emitted when parse=True or entity=True but model not loaded.</li>
            <li>Rename the tokens.Tokens class to tokens.Doc. An alias has been made to assist backwards compatibility, but you should update your code to refer to the new class name.</li>
            <li>Various bits of internal refactoring</li>
          </ul>
        </details>
        <details>
          <summary>
            <h4>2015-07-01 v0.87: Memory use</h4>
          </summary>
          <ul>
            <li>Changed weights data structure. Memory use should be reduced 30-40%. Fixed speed regressions introduced in the last few versions.</li>
            <li>Models should now be slightly more robust to noise in the input text, as I'm now training on data with a small amount of noise added, e.g. I randomly corrupt capitalization, swap spaces for newlines, etc. This is bringing a small benefit on out-of-domain data. I think this strategy could yield better results with a better noise-generation function. If you think you have a good way to make clean text resemble the kind of noisy input you're seeing in your domain, get in touch.  </li>
          </ul>
        </details>
        <details>
          <summary>
            <h4>2015-06-24 v0.86: Parser accuracy</h4>
          </summary>
          <p>Parser now more accurate, using novel non-monotonic transition system that's currently under review.</p>
        </details>
        <details>
          <summary>
            <h4>2015-05-12 v0.85: More diverse training data</h4>
          </summary>
          <ul>
            <li>Parser produces richer dependency labels following the `ClearNLP scheme`_</li>
            <li>Training data now includes text from a variety of genres.</li>
            <li>Parser now uses more memory and the data is slightly larger, due to the additional labels. Impact on efficiency is minimal: entire process still takes 
          </li></ul>
          <p>Most users should see a substantial increase in accuracy from the new model.</p>
        </details>
        <details>
          <summary>
            <h4>2015-05-12 v0.84: Bug fixes</h4>
          </summary>
          <ul>
            <li>Bug fixes for parsing</li>
            <li>Bug fixes for named entity recognition</li>
          </ul>
        </details>
        <details>
          <summary>
            <h4>2015-04-13 v0.80</h4>
          </summary>
          <p>Preliminary support for named-entity recognition. Its accuracy is substantially behind the state-of-the-art. I'm working on improvements. </p>
          <ul>
            <li>Better sentence boundary detection, drawn from the syntactic structure.</li>
            <li>Lots of bug fixes.</li>
          </ul>
        </details>
        <details>
          <summary>
            <h4>2015-03-05: v0.70</h4>
          </summary>
          <ul>
            <li>Improved parse navigation API</li>
            <li>Bug fixes to labelled parsing</li>
          </ul>
        </details>
        <details>
          <summary>
            <h4>2015-01-30: v0.4</h4>
          </summary>
          <ul>
            <li>Train statistical models on running text running text</li>
          </ul>
        </details>
        <details>
          <summary>
            <h4>2015-01-25: v0.33</h4>
          </summary>
          
        </details>
      </article>
    </div></body></html>