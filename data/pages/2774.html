<html><body><div><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-neuraltalk" class="anchor" href="#neuraltalk" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>NeuralTalk</h1>

<p><strong>Warning: Deprecated.</strong>
Hi there, this code is now quite old and inefficient, and now deprecated. I am leaving it on Github for educational purposes, but if you would like to run or train image captioning I warmly recommend my new code release <a href="https://github.com/karpathy/neuraltalk2">NeuralTalk2</a>. NeuralTalk2 is written in <a href="http://torch.ch/">Torch</a> and is SIGNIFICANTLY (I mean, ~100x+) faster because it is batched and runs on the GPU. It also supports CNN finetuning, which helps a lot with performance.</p>

<p>This project contains <em>Python+numpy</em> source code for learning <strong>Multimodal Recurrent Neural Networks</strong> that describe images with sentences.</p>

<p>This line of work was recently featured in a <a href="http://www.nytimes.com/2014/11/18/science/researchers-announce-breakthrough-in-content-recognition-software.html">New York Times article</a> and has been the subject of multiple academic papers from the research community over the last few months. This code currently implements the models proposed by <a href="http://arxiv.org/abs/1411.4555">Vinyals et al. from Google (CNN + LSTM)</a> and by <a href="http://cs.stanford.edu/people/karpathy/deepimagesent/">Karpathy and Fei-Fei from Stanford (CNN + RNN)</a>. Both models take an image and predict its sentence description with a Recurrent Neural Network (either an LSTM or an RNN).</p>

<h2><a id="user-content-overview" class="anchor" href="#overview" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Overview</h2>

<p>The pipeline for the project looks as follows:</p>

<ul>
<li>The <strong>input</strong> is a dataset of images and 5 sentence descriptions that were collected with Amazon Mechanical Turk. In particular, this code base is set up for <a href="http://nlp.cs.illinois.edu/HockenmaierGroup/Framing_Image_Description/KCCA.html">Flickr8K</a>, <a href="http://shannon.cs.illinois.edu/DenotationGraph/">Flickr30K</a>, and <a href="http://mscoco.org/">MSCOCO</a> datasets. </li>
<li>In the <strong>training stage</strong>, the images are fed as input to RNN and the RNN is asked to predict the words of the sentence, conditioned on the current word and previous context as mediated by the hidden layers of the neural network. In this stage, the parameters of the networks are trained with backpropagation.</li>
<li>In the <strong>prediction stage</strong>, a witheld set of images is passed to RNN and the RNN generates the sentence one word at a time. The results are evaluated with <strong>BLEU score</strong>. The code also includes utilities for visualizing the results in HTML.</li>
</ul>

<h2><a id="user-content-dependencies" class="anchor" href="#dependencies" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Dependencies</h2>

<p><strong>Python 2.7</strong>, modern version of <strong>numpy/scipy</strong>, <strong>perl</strong> (if you want to do BLEU score evaluation), <strong>argparse</strong> module. Most of these are okay to install with <strong>pip</strong>. To install all dependencies at once, run the command <code>pip install -r requirements.txt</code></p>

<p>I only tested this code with Ubuntu 12.04, but I tried to make it as generic as possible (e.g. use of <strong>os</strong> module for file system interactions etc. So it might work on Windows and Mac relatively easily.)</p>

<p><em>Protip</em>: you really want to link your numpy to use a BLAS implementation for its matrix operations. I use <strong>virtualenv</strong> and link numpy against a system installation of <strong>OpenBLAS</strong>. Doing this will make this code almost an order of time faster because it relies very heavily on large matrix multiplies.</p>

<h2><a id="user-content-getting-started" class="anchor" href="#getting-started" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Getting started</h2>

<ol>
<li><strong>Get the code.</strong> <code>$ git clone</code> the repo and install the Python dependencies</li>
<li><strong>Get the data.</strong> I don't distribute the data in the Git repo, instead download the <code>data/</code> folder from <a href="http://cs.stanford.edu/people/karpathy/deepimagesent/">here</a>. Also, this download does not include the raw image files, so if you want to visualize the annotations on raw images, you have to obtain the images from Flickr8K / Flickr30K / COCO directly and dump them into the appropriate data folder.</li>
<li><strong>Train the model.</strong> Run the training <code>$ python driver.py</code> (see many additional argument settings inside the file) and wait. You'll see that the learning code writes checkpoints into <code>cv/</code> and periodically reports its status in <code>status/</code> folder. </li>
<li><strong>Monitor the training.</strong> The status can be inspected manually by reading the JSON and printing whatever you wish in a second process. In practice I run cross-validations on a cluster, so my <code>cv/</code> folder fills up with a lot of checkpoints that I further filter and inspect with other scripts. I am including my cluster training status visualization utility as well if you like. Run a local webserver (e.g. <code>$ python -m SimpleHTTPServer 8123</code>) and then open <code>monitorcv.html</code> in your browser on <code>http://localhost:8123/monitorcv.html</code>, or whatever the web server tells you the path is. You will have to edit the file to setup the paths properly and point it at the right json files.</li>
<li><strong>Evaluate model checkpoints.</strong> To evaluate a checkpoint from <code>cv/</code>, run the <code>evaluate_sentence_predctions.py</code> script and pass it the path to a checkpoint.</li>
<li><strong>Visualize the predictions.</strong> Use the included html file <code>visualize_result_struct.html</code> to visualize the JSON struct produced by the evaluation code. This will visualize the images and their predictions. Note that you'll have to download the raw images from the individual dataset pages and place them into the corresponding <code>data/</code> folder.</li>
</ol>

<p>Lastly, note that this is currently research code, so a lot of the documentation is inside individual Python files. If you wish to work with this code, you'll have to get familiar with it and be comfortable reading Python code.</p>

<h2><a id="user-content-pretrained-model" class="anchor" href="#pretrained-model" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Pretrained model</h2>

<p>Some pretrained models can be found in the <a href="http://cs.stanford.edu/people/karpathy/neuraltalk/">NeuralTalk Model Zoo</a>. The slightly hairy part is that if you wish to apply these models to some arbitrary new image (one not from Flickr8k/30k/COCO) you have to first extract the CNN features. I use the 16-layer <a href="http://www.robots.ox.ac.uk/%7Evgg/research/very_deep/">VGG network</a> from Simonyan and Zisserman, because the model is beautiful, powerful and available with <a href="http://caffe.berkeleyvision.org/">Caffe</a>. There is opportunity for putting the preprocessing and inference into a single nice function that uses the Python wrapper to get the features and then runs the pretrained sentence model. I might add this in the future.</p>

<h2><a id="user-content-using-the-model-to-predict-on-new-images" class="anchor" href="#using-the-model-to-predict-on-new-images" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Using the model to predict on new images</h2>

<p>The code allows you to easily predict and visualize results of running the model on COCO/Flickr8K/Flick30K images. If you want to run the code on arbitrary image (e.g. on your file system), things get a little more complicated because we need to first need to pipe your image through the VGG CNN to get the 4096-D activations on top. </p>

<p>Have a look inside the folder <code>example_images</code> for instructions on how to do this. Currently, the code for extracting the raw features from each image is in Matlab, so you will need it installed on your system. Caffe also has a wrapper for Python, but I wasn't yet able to use the Python wrapper to exactly reproduce the features I get from Matlab. The <code>example_images</code> will walk you through the process, and you will eventually use <code>predict_on_images.py</code> to run the prediction.</p>

<h2><a id="user-content-using-your-own-data" class="anchor" href="#using-your-own-data" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Using your own data</h2>

<p>The input to the system is the <strong>data</strong> folder, which contains the Flickr8K, Flickr30K and MSCOCO datasets. In particular, each folder (e.g. <code>data/flickr8k</code>) contains a <code>dataset.json</code> file that stores the image paths and sentences in the dataset (all images, sentences, raw preprocessed tokens, splits, and the mappings between images and sentences). Each folder additionally contains <code>vgg_feats.mat</code> , which is a <code>.mat</code> file that stores the CNN features from all images, one per row, using the VGG Net from ILSVRC 2014. Finally, there is the <code>imgs/</code> folder that holds the raw images. I also provide the Matlab script that I used to extract the features, which you may find helpful if you wish to use a different dataset. This is inside the <code>matlab_features_reference/</code> folder, and see the Readme file in that folder for more information.</p>

<h2><a id="user-content-license" class="anchor" href="#license" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>License</h2>

<p>BSD license.</p>
</article>
  </div></body></html>