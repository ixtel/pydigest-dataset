<html><body><div><div class="post-text" itemprop="text">
<p>The reason for that is that you have many more 1's in your observations than 768's. So even if -1 is not exactly 1, it gets a high predicted value, because the histogram has a much larger larger value at 1 than at 768.</p>

<p>Up to a multiplicative constant, the formula for prediction is:</p>

<p><a href="http://i.stack.imgur.com/EMAGM.gif"><img src="http://i.stack.imgur.com/EMAGM.gif" alt="enter image description here"/></a></p>

<p>where K is your kernel, D your observations and h your bandwitdh. Looking at <a href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html">the doc for <code>gaussian_kde</code></a>, we see that if no value is provided for <code>bw_method</code>, it is estimated in some way, which here doesn't suit you.</p>

<p>So you can try some different values: the larger the bandwidth, the more points far from your new data are taken into account, the limit case being an almost constant predicted function.</p>

<p>On the other hand, a very small bandwidth only takes really close points into account, which is what I thing you want.</p>

<p>Some graphs to illustrate the influence of the bandwidth:
<a href="http://i.stack.imgur.com/bwOsM.png"><img src="http://i.stack.imgur.com/bwOsM.png" alt="enter image description here"/></a></p>

<p>Code used:</p>

<pre><code>import matplotlib.pyplot as plt
f, axarr = plt.subplots(2, 2, figsize=(10, 10))
for i, h in enumerate([0.01, 0.1, 1, 5]):
    my_pdf = gaussian_kde(osservazioni, h)
    axarr[i//2, i%2].plot(x, my_pdf(x), 'r') # distribution function
    axarr[i//2, i%2].set_title("Bandwidth: {0}".format(h))
    axarr[i//2, i%2].hist(osservazioni, normed=1, alpha=.3) # histogram
</code></pre>

<p>With your current code, for x=-1, the value of K((x-x_i)/h) for all x_i's who are equal to 1 is smaller than 1, but you add up a lot of these values (there are 921 1s in your observations, and also 357 2s)</p>

<p>On the other hand for x = 768, the value of the kernel is 1 for all x_i's which are 768, but there are not many such points (39 to be precise). So here a lot of "small" terms make a larger sum than a small number of larger terms.</p>

<p>If you don't want this behavior, you can decrease the size of your gaussian kernel : this way the penalty (K(-2)) paid because of the distance between -1 and 1 will be higher. But I think that this would be overfitting your observations.</p>

<p>A formula to determine whether a new sample is acceptable (compared to your empirical distribution) or not is more of a statistical problem, you can have a look at <code>stats.stackexchange.com</code></p>

<p>You can always try to use a low value for the bandwidth, which will give you a peaked predicted function. Then you can normalize this function, dividing it by its maximal value.</p>

<p>After that, all predicted values will be between 0 and 1:</p>

<pre><code>maxDensityValue = np.max(my_pdf(x))
for e in new_values:
    print("{0} {1}".format(e, my_pdf(e)/maxDensityValue))
</code></pre>
    </div>
    </div></body></html>