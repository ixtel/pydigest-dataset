<html><body><div><div class="main">
		<p><a href="http://www.flickr.com/photos/adactio/7753864240/sizes/s/in/photolist-cPbzGu-a9GzWC-bD2DKY-9kbih-95WzW3-59QZLB-59QXMa-59QYQT-59R1Kg-bUd8L-36Nvr5-anLpJ5-anHsXT-anLcHu-9zUCZG-bnMKJd-9u1X3j-7iop9o-9LNukK-7vRx4t-9zBw2y-9oSXB3-DjadM-NRZZj-9u1Xwo-9ED1JW-9pkQbZ-9poUYq-avLhMr-9qk2zx-9p9Xvo-9oSY2C-9wrpaG-9gGKCf-7iosuU-4J52vA-bk8EDK-9zRF9r-4aoCYD-cdc8t7-9ez6i6-68ro8m-9j5xWM-9gGLS5-ccfJ9L-ccfG9h-amZF8-4zMfjs-4XisBX-9Q2vv5-eiWMNE/"><img class="alignright size-full wp-image-649" alt="devices" src="http://tetamap.files.wordpress.com/2013/11/devices.jpg?w=700"/></a>How to best distribute tests against multiple devices or resources with pytest? This interesting question came up during my training in Lviv (Ukraine) at an embedded systems company. Distributing tests to processes can serve two purposes:</p>
<ul>
<li>running the full test suite against each device to verify they all work according to the test specification</li>
<li>distributing the test load to several devices of the same type in order to minimize overall test execution time.</li>
</ul>
<p>The solution to both problems is easy if you use two <a href="http://pytest.org">pytest</a> facilities:</p>
<ul>
<li>the <a href="http://pytest.org/latest/fixture.html">general fixture mechanism</a>: we write a fixture function which provides a <tt>device</tt> object which is pre-configured for use in tests.</li>
<li>the <a href="https://pypi.python.org/pypi/pytest-xdist">pytest-xdist plugin</a>: we use it to run subprocesses and communicate configuration data for the <tt>device</tt> fixture from the master process to the subprocesses.</li>
</ul>
<p>To begin with, let’s configure three devices that are each reachable by a separate IP address. We create a list of ip addresses in a file:</p>
<pre># content of devices.json
["192.168.0.1", "192.168.0.2", "192.168.0.3"]</pre>
<p>We now create a local pytest plugin which reads the configuration data, implements a per-process <tt>device</tt> fixture and the master-to-slave communication to configure each subprocess according to our device list:</p>
<pre># content of conftest.py

import pytest

def read_device_list():
    import json
    with open("devices.json") as f:
        return json.load(f)

def pytest_configure(config):
     # read device list if we are on the master
     if not hasattr(config, "slaveinput"):
        config.iplist = read_device_list()

def pytest_configure_node(node):
    # the master for each node fills slaveinput dictionary
    # which pytest-xdist will transfer to the subprocess
    node.slaveinput["ipadr"] = node.config.iplist.pop()

@pytest.fixture(scope="session")
def device(request):
    slaveinput = getattr(request.config, "slaveinput", None)
    if slaveinput is None: # single-process execution
        ipadr = read_device_list()[0]
    else: # running in a subprocess here
        ipadr = slaveinput["ipadr"]
    return Device(ipadr)

class Device:
    def __init__(self, ipadr):
        self.ipadr = ipadr

    def __repr__(self):
        return "&lt;Device ip=%s&gt;" % (self.ipadr)</pre>
<p>We can now write tests that simply make use of the <tt>device</tt> fixture by using its name as an argument to a test function:</p>
<pre># content of test_device.py
import time

def test_device1(device):
    time.sleep(2)  # simulate long test time
    assert 0, device

def test_device2(device):
    time.sleep(2)  # simulate long test time
    assert 0, device

def test_device3(device):
    time.sleep(2)  # simulate long test time
    assert 0, device</pre>
<p>Let’s first run the tests in a single-process, only using a single device (also using some reporting option to shorten output):</p>
<pre>$ py.test test_device.py -q --tb=line
FFF
================================= FAILURES =================================
/tmp/doc-exec-9/test_device.py:5: AssertionError: &lt;Device ip=192.168.0.1&gt;
/tmp/doc-exec-9/test_device.py:9: AssertionError: &lt;Device ip=192.168.0.1&gt;
/tmp/doc-exec-9/test_device.py:13: AssertionError: &lt;Device ip=192.168.0.1&gt;
3 failed in 6.02 seconds</pre>
<p>As to be expected, we get six seconds execution time (3 tests times 2 seconds each).</p>
<p>Now let’s run the same tests in three subprocesses, each using a different device:</p>
<pre>$ py.test --tx 3*popen --dist=each test_device.py -q --tb=line
gw0 I / gw1 I / gw2 I
gw0 [3] / gw1 [3] / gw2 [3]

scheduling tests via EachScheduling
FFFFFFFFF
================================= FAILURES =================================
E   AssertionError: &lt;Device ip=192.168.0.1&gt;
E   AssertionError: &lt;Device ip=192.168.0.3&gt;
E   AssertionError: &lt;Device ip=192.168.0.2&gt;
E   AssertionError: &lt;Device ip=192.168.0.1&gt;
E   AssertionError: &lt;Device ip=192.168.0.3&gt;
E   AssertionError: &lt;Device ip=192.168.0.2&gt;
E   AssertionError: &lt;Device ip=192.168.0.3&gt;
E   AssertionError: &lt;Device ip=192.168.0.1&gt;
E   AssertionError: &lt;Device ip=192.168.0.2&gt;
9 failed in 6.52 seconds</pre>
<p>We just created three subprocesses each running three tests. Instead of 18 seconds execution time (9 tests times 2 seconds per test) we roughly got 6 seconds, a 3-times speedup. Each subprocess ran in parallel three tests against “its” device.</p>
<p>Let’s also run with load-balancing, i.e. distributing the tests against three different devices so that each device executes one test:</p>
<pre>$ py.test --tx 3*popen --dist=load test_device.py -q --tb=line
gw0 I / gw1 I / gw2 I
gw0 [3] / gw1 [3] / gw2 [3]

scheduling tests via LoadScheduling
FFF
================================= FAILURES =================================
E   AssertionError: &lt;Device ip=192.168.0.3&gt;
E   AssertionError: &lt;Device ip=192.168.0.2&gt;
E   AssertionError: &lt;Device ip=192.168.0.1&gt;
3 failed in 2.50 seconds</pre>
<p>Here each test runs in a separate process against its device, overall more than halfing the test time compared to what it would take in a single-process (3*2=6 seconds). If we had many more tests than subproceses than load-scheduling would distribute tests in real-time to the process which has finished executing other tests.</p>
<p>Note that the tests themselves do not need to be aware of the distribution mode. All configuration and setup is contained in the <tt>conftest.py</tt> file.</p>
<p>To summarize the behaviour of the hooks and fixtures in <tt>conftest.py</tt>:</p>
<ul>
<li><tt>pytest_configure(config)</tt> is called both on the master and each subprocess. We can distinguish where we are by checking for presence of <tt>config.slaveinput</tt>.</li>
<li><tt>pytest_configure_node(node)</tt> is called for each subprocess. We can fill the <tt>slaveinput</tt> dictionary which the subprocess slave can then read via its <tt>config.slaveinput</tt> dictionary.</li>
<li>the <tt>device</tt> fixture only is called when a test needs it. In distributed mode, tests are only collected and executed in a subprocess. In non-distributed mode, tests are run single-process. The <tt>Device</tt> class is just a stub — it will need to grow methods for actual device communication. The tests can then simply use those <tt>device</tt> methods.</li>
</ul>
<p>I’d like to thank Anton and the participants of my three day testing training in Lviv (Ukraine) for bringing up this and many other interesting questions.</p>

<div id="jp-post-flair" class="sharedaddy sd-like-enabled"><div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" id="like-post-wrapper-6679328-644-56d59b353228f" data-src="//widgets.wp.com/likes/#blog_id=6679328&amp;post_id=644&amp;origin=tetamap.wordpress.com&amp;obj_id=6679328-644-56d59b353228f" data-name="like-post-frame-6679328-644-56d59b353228f"><h3 class="sd-title">Like this:</h3><p class="likes-widget-placeholder post-likes-widget-placeholder"><span class="button"><span>Like</span></span> <span class="loading">Loading...</span></p><span class="sd-text-color"/><a class="sd-link-color"/></div>
<p id="jp-relatedposts" class="jp-relatedposts">
	<h3 class="jp-relatedposts-headline"><em>Related</em></h3>
</p></div>			</div>

	</div></body></html>