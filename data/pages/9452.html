<html><body><div><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-lda2vec-tools-for-interpreting-natural-language" class="anchor" href="#lda2vec-tools-for-interpreting-natural-language" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>lda2vec: Tools for interpreting natural language</h1>
<a href="https://github.com/cemoody/lda2vec/blob/master/LICENSE"><img alt="http://img.shields.io/badge/license-MIT-blue.svg?style=flat" src="https://camo.githubusercontent.com/4ad0bc4de8816451a4a76b886b76142b99d10ffb/687474703a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d626c75652e7376673f7374796c653d666c6174" data-canonical-src="http://img.shields.io/badge/license-MIT-blue.svg?style=flat"/></a>
<a href="http://lda2vec.readthedocs.org/en/latest/?badge=latest"><img alt="https://readthedocs.org/projects/lda2vec/badge/?version=latest" src="https://camo.githubusercontent.com/642dcfee18c3401713c99bf56da8568dc1268c7f/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f6c6461327665632f62616467652f3f76657273696f6e3d6c6174657374" data-canonical-src="https://readthedocs.org/projects/lda2vec/badge/?version=latest"/></a>
<a href="https://travis-ci.org/cemoody/lda2vec"><img alt="https://travis-ci.org/cemoody/lda2vec.svg?branch=master" src="https://camo.githubusercontent.com/dcd61ca43f8280c4bf7284ceb288f1d3a2a1e5ef/68747470733a2f2f7472617669732d63692e6f72672f63656d6f6f64792f6c6461327665632e7376673f6272616e63683d6d6173746572" data-canonical-src="https://travis-ci.org/cemoody/lda2vec.svg?branch=master"/></a>
<a href="https://travis-ci.org/cemoody/lda2vec"><img src="https://camo.githubusercontent.com/6fbb2484f8b8c3d52eb4c87bfa21981d8a9959b9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f7665726167652d39332532352d677265656e2e737667" data-canonical-src="https://img.shields.io/badge/coverage-93%25-green.svg"/>
</a>
<a href="https://twitter.com/intent/follow?screen_name=chrisemoody"><img alt="https://img.shields.io/twitter/follow/chrisemoody.svg?style=social" src="https://camo.githubusercontent.com/de6b021cd2a6ce9c124df482a693a7c383110da1/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f6368726973656d6f6f64792e7376673f7374796c653d736f6369616c" data-canonical-src="https://img.shields.io/twitter/follow/chrisemoody.svg?style=social"/></a>
<p>The lda2vec model tries to mix the best parts of word2vec and LDA
into a single framework. word2vec captures powerful relationships
between words, but the resulting vectors are largely interpretable
and don't represent documents. LDA on the other hand is quite
interpretable by humans, but doesn't model local word relationships
like word2vec. We build a model that builds both word and document
topics, makes them interpreable,  makes topics over clients, times,
and documents, and makes them supervised topics.</p>
<a name="user-content-resources"/>
<h2><a id="user-content-resources" class="anchor" href="#resources" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Resources</h2>
<p>See this <a href="http://nbviewer.jupyter.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda.ipynb">Jupyter Notebook</a>
for an example of an end-to-end demonstration.</p>
<p>See this <a href="http://www.slideshare.net/ChristopherMoody3/word2vec-lda-and-introducing-a-new-hybrid-algorithm-lda2vec-57135994">presentation</a>
for a presentation focused on the benefits of word2vec, LDA, and lda2vec.</p>
<p>See the <a href="https://lda2vec.readthedocs.org/en/latest/">API reference docs</a></p>
<a name="user-content-about"/>
<h2><a id="user-content-about" class="anchor" href="#about" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>About</h2>
<a href="/cemoody/lda2vec/blob/master/images/img00_word2vec.png" target="_blank"><img alt="images/img00_word2vec.png" src="/cemoody/lda2vec/raw/master/images/img00_word2vec.png"/></a>
<p>Word2vec tries to model word-to-word relationships.</p>
<a href="/cemoody/lda2vec/blob/master/images/img01_lda.png" target="_blank"><img alt="images/img01_lda.png" src="/cemoody/lda2vec/raw/master/images/img01_lda.png"/></a>
<p>LDA models document-to-word relationships.</p>
<a href="/cemoody/lda2vec/blob/master/images/img02_lda_topics.png" target="_blank"><img alt="images/img02_lda_topics.png" src="/cemoody/lda2vec/raw/master/images/img02_lda_topics.png"/></a>
<p>LDA yields topics over each document.</p>
<a href="/cemoody/lda2vec/blob/master/images/img03_lda2vec_topics01.png" target="_blank"><img alt="images/img03_lda2vec_topics01.png" src="/cemoody/lda2vec/raw/master/images/img03_lda2vec_topics01.png"/></a>
<p>lda2vec yields topics not over just documents, but also regions.</p>
<a href="/cemoody/lda2vec/blob/master/images/img04_lda2vec_topics02.png" target="_blank"><img alt="images/img04_lda2vec_topics02.png" src="/cemoody/lda2vec/raw/master/images/img04_lda2vec_topics02.png"/></a>
<p>lda2vec also yields topics over clients.</p>
<a href="/cemoody/lda2vec/blob/master/images/img05_lda2vec_topics03_supervised.png" target="_blank"><img alt="images/img05_lda2vec_topics03_supervised.png" src="/cemoody/lda2vec/raw/master/images/img05_lda2vec_topics03_supervised.png"/></a>
<p>lda2vec the topics can be 'supervised' and forced to predict another target.</p>
<p>lda2vec also includes more contexts and features than LDA. LDA dictates that
words are generated by a document vector; but we might have all kinds of
'side-information' that should influence our topics. For example, a single
client comment is about a particular item ID, written at a particular time
and in a particular region. In this case, lda2vec gives you topics over all
items (separating jeans from shirts, for example) times (winter versus summer)
regions (desert versus coastal) and clients (sporty vs professional attire).</p>
<p>Ultimately, the topics are interpreted using the excellent pyLDAvis library:</p>
<a href="/cemoody/lda2vec/blob/master/images/img06_pyldavis.gif" target="_blank"><img alt="images/img06_pyldavis.gif" src="/cemoody/lda2vec/raw/master/images/img06_pyldavis.gif"/></a>
<a name="user-content-requirements"/>
<h2><a id="user-content-requirements" class="anchor" href="#requirements" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Requirements</h2>
<p>Minimum requirements:</p>
<ul>
<li>Python 2.7+</li>
<li>NumPy 1.10+</li>
<li>Chainer 1.5.1+</li>
<li>spaCy 0.99+</li>
</ul>
<p>Requirements for some features:</p>
<ul>
<li>CUDA support</li>
<li>Testing utilities: py.test</li>
</ul>

</article>
  </div></body></html>