<html><body><div><div class="section-inner layoutSingleColumn"><p name="f48e" id="f48e" class="graf--p graf--first">The deployment mechanism I’ve used thus far is a Fabric script plus configuration templates for supervisor and nginx. Each time I run “fab deploy” from my workstation, Fabric script does the following on the remote host:</p><ul class="postList"><li name="58c8" id="58c8" class="graf--li graf-after--p">sets up a new directory for the new deployment. Let’s refer to this directory as $TARGET.</li><li name="e843" id="e843" class="graf--li graf-after--li">sets up a python3 virtualenv in $TARGET/venv</li><li name="115c" id="115c" class="graf--li graf-after--li">fetches the latest snapshot of code from GitHub into $TARGET. It is convenient to use GitHub’s Subversion interface for this and run a “svn export” command. It produces just the source files without any version control metadata–exactly what’s needed.</li><li name="e436" id="e436" class="graf--li graf-after--li">installs dependencies listed in requirements file. These get installed into the new virtualenv and don’t affect the live application. Downloading and building the dependencies take up to a minute.</li><li name="e476" id="e476" class="graf--li graf-after--li">runs Django management commands to collect static files, run database migrations etc.</li><li name="53ea" id="53ea" class="graf--li graf-after--li">rewrites the supervisor configuration file to run gunicorn from the new virtual environment</li><li name="33a7" id="33a7" class="graf--li graf-after--li">updates nginx configuration, in case I’ve changed anything in the nginx configuration template</li><li name="cbc8" id="cbc8" class="graf--li graf-after--li">runs “supervisorctl reload” and “/etc/init.d/nginx restart”. At this point the web application becomes unavailable and remains unavailable until supervisor starts back up, launches gunicorn process, and the Django code initializes. This usually takes 5 to 10 seconds, and nginx would typically return “502 Bad Gateway” responses during this time.</li><li name="d9ea" id="d9ea" class="graf--li graf-after--li">All done!</li></ul><p name="3b60" id="3b60" class="graf--p graf-after--li">Here’s how the relevant part of Fabric script looks. The virtualenv context manager seen below is from the excellent <a href="https://github.com/ronnix/fabtools" data-href="https://github.com/ronnix/fabtools" class="markup--anchor markup--p-anchor" rel="nofollow">fabtools</a> library.</p><figure name="9427" id="9427" class="graf--figure graf--iframe graf-after--p"><p class="iframeContainer"/></figure><p name="b8c2" id="b8c2" class="graf--p graf-after--figure">Now, how to eliminate the downtime during the last steps of each deploy?Let’s set some constraints: no load balancer (for now anyway). Everything runs off a single box, and even a single non-200 response is undesirable. And, baby steps: I will consider the simple (and common) case when there are no database migrations to be applied or they are backwards-compatible: the old version of the app keeps working acceptably after the migrations are applied.</p><p name="752d" id="752d" class="graf--p graf-after--p">The first idea I looked into was based on the observation that availability is more important for some parts of the app than others. Specifically, the API part of the app listens for pings from the monitored client systems, and the frontend part serves pages to normal website visitors. While it would be embarrassing to show error pages to human visitors, not missing any pings is actually more important. A missed ping can lead to a false alert being sent sometime later. That’s even more embarrassing!</p><p name="5a48" id="5a48" class="graf--p graf-after--p">I considered and prototyped listening to pings using Amazon API Gateway. It would put ping messages in Amazon SQS queue, which the Django app could consume at its leisure. This would be a relatively simple way to improve availability and scalability by quite a lot at the cost of somewhat increased complexity and a new external dependency. I might look into this again in future.</p><p name="b6f4" id="b6f4" class="graf--p graf-after--p">Next idea: separate the “listen to pings” functionality from the rest of the Django app. The ping listener logic is very simple and, ultimately, amounts to two SQL operations: one update and one insert. It could be easy enough to rewrite this part, perhaps using one of the python microframeworks, or maybe using a language other than Python, or maybe even handle it from nginx itself, using <a href="https://github.com/FRiCKLE/ngx_postgres" data-href="https://github.com/FRiCKLE/ngx_postgres" class="markup--anchor markup--p-anchor" rel="nofollow">ngx_postgres</a> module. For a little amusement, here’s the nginx configuration fragment which, basically, works as-is (please forgive the funny looking regular expression):</p><figure name="9164" id="9164" class="graf--figure graf--iframe graf-after--p"><p class="iframeContainer"/></figure><p name="a3c6" id="a3c6" class="graf--p graf-after--figure">Here’s what’s going on: when the client requests and the URL of a certain format, the server runs a PostgreSQL query and returns either HTTP code 200 or HTTP code 400. This is also a performance win, because the request doesn’t have to travel through the hoops of gunicorn, Django and psycopg2. As long as the database is available, nginx can handle the ping requests, even if the Django application is not running for any reason.</p><p name="a672" id="a672" class="graf--p graf-after--p">The not so great thing with this approach is that it’s “tricky” and adds to the number of things that the developer and systems administrator need to know. For example, when the database schema changes, the SQL query above might need to be updated and tested as well. Getting the ngx_postgres extension set up isn’t a simple matter of “apt-get install” either.</p><p name="de09" id="de09" class="graf--p graf-after--p">Thinking more about it, the main goal of zero downtime can also be achieved by just carefully orchestrating process restarts and reloads.</p><p name="5217" id="5217" class="graf--p graf-after--p">My deployment script was using “/etc/init.d/nginx restart” because I didn’t know any better. As I learned, it can be replaced it with “/etc/init.d/nginx reload” which handles things gracefully:</p><blockquote name="868a" id="868a" class="graf--blockquote graf-after--p">Run service nginx reload or /etc/init.d/nginx reload</blockquote><blockquote name="a1e1" id="a1e1" class="graf--blockquote graf-after--blockquote">It will do a hot reload of the configuration without downtime. If you have pending requests, then there will be lingering nginx processes that will handle those connections before it dies, so it’s an extremely graceful way to reload configs. – <a href="http://serverfault.com/questions/378581/nginx-config-reload-without-downtime" data-href="http://serverfault.com/questions/378581/nginx-config-reload-without-downtime" class="markup--anchor markup--blockquote-anchor" rel="nofollow">“Nginx config reload without downtime” on ServerFault</a></blockquote><p name="39b1" id="39b1" class="graf--p graf-after--blockquote">Similarly, my deployment script was using “supervisorctl reload” which stops all managed services, re-reads configuration, and starts all services. Instead “supervisorctl update” can be used to start, stop and restart the changed tasks as necessary.</p><p name="22c2" id="22c2" class="graf--p graf-after--p">Now, here’s what “fab deploy” can do:</p><ul class="postList"><li name="ce67" id="ce67" class="graf--li graf-after--p">set up a new virtual environment as before</li><li name="d8df" id="d8df" class="graf--li graf-after--li">create a supervisor task with unique name (“hc_<em class="markup--em markup--li-em">timestamp</em>”)</li><li name="dcaa" id="dcaa" class="graf--li graf-after--li">start the new gunicorn process alongside the running one. nginx talks to gunicorn processes using UNIX sockets, and each process uses a separate, again timestamped, socket file</li><li name="f77d" id="f77d" class="graf--li graf-after--li">wait a little–then verify that the new gunicorn process has started up and is serving responses</li><li name="e98e" id="e98e" class="graf--li graf-after--li">update nginx configuration to point to the new socket file and reload nginx</li><li name="28b2" id="28b2" class="graf--li graf-after--li">stop the old gunicorn process</li></ul><p name="e3d1" id="e3d1" class="graf--p graf-after--li">Here’s the improved part of Fabric script which juggles supervisor jobs:</p><figure name="a071" id="a071" class="graf--figure graf--iframe graf-after--p"><p class="iframeContainer"/></figure><p name="e3ea" id="e3ea" class="graf--p graf-after--figure">With this, nginx is always serving requests, and is talking to a live gunicorn process at all times. To verify this in practice, I wrote a quick script that requests a particular URL again and again in an infinite loop. As soon as it hits a non-200 response, it would print out a hard-to-miss error message. With this banging against my test VM, I did a couple deploys and saw no missed requests. Success!</p><h3 name="ec11" id="ec11" class="graf--h3 graf-after--p">Summary</h3><p name="3086" id="3086" class="graf--p graf-after--h3">There are many ways to achieve zero downtime during code deploys, and each has its own trade-offs. For example, a reasonable strategy is to extract the critical parts out of the bigger application. Each part can then be updated independently. Later, the parts can also be scaled independently. The downside to this is more code and configuration to maintain.</p><p name="7818" id="7818" class="graf--p graf-after--p">What I ultimately ended up doing:</p><ul class="postList"><li name="f1ef" id="f1ef" class="graf--li graf-after--p">hot-reload supervisor and nginx configurations instead of just restarting them. Obvious thing to do in retrospect.</li><li name="6cd3" id="6cd3" class="graf--li graf-after--li">make sure the new gunicorn process is alive and being used by nginx before stopping the old gunicorn process.</li><li name="3b4f" id="3b4f" class="graf--li graf-after--li graf--last">and keep the whole setup relatively simple. As the project gets more usage, I will need to look at performance hotspots and figure out how to scale horizontally, but this should do for now!</li></ul></div></div></body></html>