<html><body><div><div class="content html_format"><p>
      Привет, хабр!</p>

<img src="https://habrastorage.org/getpro/habr/post_images/42f/e81/f34/42fe81f3404ede93b648612bdc45c465.png" alt="image"/>
<p>
В </p><a href="http://habrahabr.ru/post/250811/">прошлый раз</a><p> мы познакомились с инструментом </p><b>Apache Spark</b><p>, который в последнее время становится чуть ли не самым популярным средством для обработки больших данных и в частности, </p><b>Large Scale Machine Learning</b><p>. Сегодня мы рассмотрим подробнее библиотеку </p><b>MlLib</b><p>, а именно — покажем, как решать задачи машинного обучения — классификации, регресии, кластеризации, а также коллаборативной фильтрации. Кроме этого покажем, как можно исследовать признаки с целью отбора и выделения новых (т.н. </p><b>Feature Engineering</b><p>, о котором мы </p><a href="http://habrahabr.ru/post/248129/">говорили ранее</a><p>, причем </p><a href="http://habrahabr.ru/post/249759/">не один раз</a><p>).
</p><a name="habracut"/>
<h3>План</h3><p>
Прежде всего посмотрим, как хранить объекты нашей обучающей выборки, как считать базовые статистики признаков, после чего — алгоритмы машинного обучения (классификация, регрессия, кластеризация) и наконец, рассмотрим пример построения рекомендательной системы — т.н. методы коллаборативной фильтрации, а если быть точнее — один из самых распространенных алгоритмов ALS. 

</p><h3>Вектора</h3><p>
Для простых </p><b>«плотных»</b><p> векторов есть специальный класс </p><b>Vectors.dense</b><p>:

</p><pre><code class="python">from pyspark.mllib.linalg import Vectors
my_vec = Vectors.dence ([1.12, 4.10, 1.5, -2.7, 3.5, 10.7, 0.7])
</code></pre><p>
Для </p><b>«разреженных»</b><p> векторов используется класс </p><b>Vectors.sparse</b><p>:

</p><pre><code class="python">from pyspark.mllib.linalg import Vectors
my_vec = Vectors.sparse(10, [0,2,4,9], [-1.2, 3.05, -4.08, 0.46])
</code></pre><p>
Здесь первым аргументом является количество признаков (длина вектора), далее идут списком — номера ненулевых признаков, и после — сами значения признаков.

</p><h3>Размеченные вектора</h3><p>
Для размеченных точек в Spark'е есть специальный класс </p><b>LabeledPoint</b><p>:

</p><pre><code class="python">from pyspark.mllib.regression import LabeledPoint
my_point = LabeledPoint(1.0, my_vec)
</code></pre><p>
Где в классе </p><b>LabeledPoint</b><p> мы имеем </p><b>LabeledPoint.features</b><p> — любой из описанных выше векторов, а </p><b>LabeledPoint.label</b><p> — это, соответственно, метка, которая может принимать любое действительное значение в случае задачи регрессии и значения </p><b>[0.0,1.0,2.0,...]</b><p> — для задач классификации

</p><h3>Работа с признаками</h3><p>
Не секрет, что зачастую, чтобы построить хороший алгоритм машинного обучения, достаточно просто посмотреть на признаки, отобрать из наиболее релевантные или придумать новые. Для этой цели в спарке класс </p><b>Statistics</b><p>, с помощью которого можно делать все эти вещи, например:

</p><pre><code class="python">from pyspark.mllib.stat import Statistics
summary = Statistics.colStats(features)

# meas of features
summary.mean
# non zeros features
summary.numNonzeros
# variance
summary.variance
# correlations of features
Statistics.corr(features)
</code></pre><p>
Помимо этого, в Spark'е есть огромное количество дополнительных возможностей вроде сэмплирования, генерации стандартных признаков (вроде TF-IDF для текстов), а также такая важная вещь, как масштабирование признаков (читателю предлагается после прочтения данной статьи посмотреть это в документации). Для последнего есть специальный класс </p><b>Scaler</b><p>:

</p><pre><code class="python">from pyspark.mllib.feature import StandardScaler
scaler = StandardScaler(withMean=True, withStd=True).fit(features)
scaler.transform (features.map(lambda x:x.toArray()))
</code></pre><p>
Единственное, что важно помнить — в случае разреженных векторов это не работает и стратегию масштабирования надо продумывать под конкретную задачу. Теперь перейдем непосредственно к задачам машинного обучения.

</p><h3>Классификация и регрессия</h3>
<h4>Линейные методы</h4><p>
Самыми распространенными методами </p><a href="http://habrahabr.ru/post/248779/">как всегда</a><p> являются линейные классификаторы. Обучение линейного классификатора сводится к задаче выпуклой минимизации функционала от вектора весов. Различие заключается в выборе функции потерь, функции регуляризации, количества итераций и множества других параметров. Для примера, рассмотрим ниже логистическую функцию потерь (и, соответственно, т.н. метод логистической регрессии), 500 итераций и L2 — регуляризацию.

</p><pre><code class="python">import pyspark.mllib.classification as cls
model = cls.LogisticRegressionWithSGD.train(train, iterations=500, regType="l2")
</code></pre><p>
Аналогично делается и линейная регрессия:

</p><pre><code class="python">import pyspark.mllib.regression as regr
model = regr.RidgeRegressionWithSGD.train(train)
</code></pre>
<h4>Наивный Байес</h4><p>
В этом случае, алгоритм обучения принимает на вход всего 2 параметра — саму обучающую выборку и параметр сглаживания:

</p><pre><code class="python">from pyspark.mllib.classification import NaiveBayes
model = NaiveBayes.train(train, 8.5)
model.predict(test.features)
</code></pre>
<h4>Решающие деревья</h4><p>
В спарке, как и во многих других пакетах, реализованы деревья регрессии и классификации. Алгоритм обучения принимает на вход множество параметров, такие, как множество классов, максимальная глубина дерева. Также алгоритму необходимо указать, какие категории имеют категориальные признаки, а также множество других параметров. Однако одним из самых важных из них при обучении деревьев является так называемый </p><b>impurity</b><p> — критерий вычисления так называемой </p><b>information gain</b><p>, который обычно может принимать следующие значения: </p><b>entropy</b><p> и </p><b>gini</b><p> — для задач классификации, </p><b>variance</b><p> — для задач регрессии. Для примера рассмотрим бинарную классификацию с параметрами, определенными ниже:

</p><pre><code class="python">from pyspark.mllib.tree import DecisionTree
model = DecisionTree.trainClassifier(train, numClasses=2, impurity='gini', maxDepth=5)
model.predict(test.map(lambda x: x.features))
</code></pre>
<h4>Random Forest</h4><p>
Случайные леса, как известно, является одними из универсальных алгоритмов и следовало ожидать, что в этом инструменте они будут реализованы. Используют они деревья, описанные выше. Здесь точно также есть методы </p><b>trainClassifier</b><p> и </p><b>trainRegression</b><p> — для обучения классификатора и функции регрессии соответственно. Одними из самых важных параметров являются — количество деревьев в лесу, уже известный нам </p><b>impurity</b><p>, а также </p><b>featureSubsetStrategy</b><p> — количество признаков, которые рассматриваются при разбиении на очередном узле дерева (подробнее о значениях — см. документацию). Соответственно, ниже пример бинарной классификации с помощью 50 деревьев:

</p><pre><code class="python">from pyspark.mllib.tree import RandomForest
model = RandomForest.trainClassifier(train, numClasses=2, numTrees=50, featureSubsetStrategy="auto", impurity='gini', maxDepth=20, seed=12)
model.predict(test.map(lambda x:x.features))
</code></pre>
<h3>Кластеризация</h3><p>
Как и везде, в спарке реализован всем известный алгоритм </p><b>KMeans</b><p>, обучение которого принимает на вход непосредственно датасет, число кластеров, число итераций, а также стратегию выбора начальных центров кластеров (параметр </p><b>initializationMode</b><p>, который по умолчанию имеет значение </p><b>k-means</b><p>, а также может принимать значение </p><b>random</b><p>):

</p><pre><code class="python">from pyspark.mllib.clustering import KMeans
clusters = KMeans.train(features, 3, maxIterations=100, runs=5, initializationMode="random")
clusters.predict(x.features))
</code></pre>
<h3>Коллаборативная фильтрация</h3><p>
Учитывая, что самый известный пример применения Больших Данных — это рекомендательная система, было бы странным, если бы самые простейшие алгоритмы не были реализованы во многих пакетах. Это касается и Spark'а. В нем реализован алгоритм </p><b>ALS (Alternative Least Square)</b><p> — пожалуй, один из самых известных алгоритмов коллаборативной фильтрации. Описание самого алгоритма заслуживает отдельной статьи. Здесь только скажем в двух словах, что алгоритм фактически занимается разложением матрицы отзывов (строки которой — это пользователи, а столбцы — продукты) — на матрицы </p><b>продукт — топик</b><p> и </p><b>топик-пользователь</b><p>, где топики — это некоторые скрытые переменные, смысл которых зачастую не понятен (вся прелесть алгоритма </p><b>ALS</b><p> как раз в том, чтобы сами топики и их значения найти). Суть этих топиков в том, что каждый пользователь и каждый фильм теперь характеризуются набором признаков, а скалярное произведение этих векторов — это и есть оценка фильма конкретного пользователя. Обучающая выборка для этого алгоритма задается в виде таблицы </p><b>userID -&gt; productID -&gt; rating</b><p>. После чего делается обучение модели с помощью ALS (который, также как и другие алгоритмы, принимает на вход множество параметров, прочитать о которых предлагается читателю самостоятельно):

</p><pre><code class="python">from pyspark.mllib.recommendation import ALS
model = ALS.train (ratings, 20, 60)
predictions = model.predictAll(ratings.map (lambda x: (x[0],x[1])))
</code></pre>
<h3>Заключение</h3><p>
Итак, мы кратко рассмотрели библиотеку </p><b>MlLib</b><p> из фреймворка Apache Spark, который разрабатывался для распределенной обработки больших данных. Напомним, что основным преимуществом данного инструмента, </p><a href="http://habrahabr.ru/post/250811/">как обсуждалось ранее</a><p>, является то, что данные можно кэшировать в оперативной памяти, что позволяет существенно ускорять вычисления в случае итеративных алгоритмов, какими и являются большинство алгоритмов машинного обучения.

      
      </p><p class="clear"/>
    </div>

    
  </div></body></html>