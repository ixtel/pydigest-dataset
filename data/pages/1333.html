<html><body><div><div class="post-body entry-content" id="post-body-7990324759990656542" itemprop="description articleBody">
<div dir="ltr">
<span>Imagine you want to extract content from the Web that isn't all in only one page: you need a way to </span><span>navigate</span><span> through the site to get to the pages that contain the useful information. For example, maybe you want to get the latest </span><a href="http://mentalfloss.com/big-questions"><span>"big questions" articles</span></a><span> of the </span><a href="http://mentalfloss.com/"><span>Mental Floss</span></a><span> website, but only those in the </span><span>Origins</span><span> and </span><span>Fact Check</span><span> categories.</span></div>
<b><br/></b>


<b><br/></b>

<div dir="ltr">
<span>If you have an interest in Python and web scraping, you may have already played with the nice </span><a href="http://docs.python-requests.org/"><span>requests library</span></a><span> to get content of pages from the Web. Maybe you have toyed around using </span><a href="http://www.crummy.com/software/BeautifulSoup/"><span>BeautifulSoup</span></a><span> or </span><a href="http://lxml.de/"><span>lxml</span></a><span> to make the content extraction easier. Well, now we are going to show you how to use the </span><a href="http://scrapy.org/"><span>Scrapy framework</span></a><span>, which has all these functionalities and many more, so that solving the sort of problem we introduced above is a walk in the park.</span></div>
<b><br/></b>

<div dir="ltr">
<span>It is worth noting that Scrapy tries not only to solve the content extraction (called </span><a href="http://en.wikipedia.org/wiki/Web_scraping"><span>scraping</span></a><span>), but also the </span><span>navigation</span><span> to the relevant pages for the extraction (called </span><a href="http://en.wikipedia.org/wiki/Web_crawler"><span>crawling</span></a><span>). To achieve that, a core concept in the framework is the </span><span>Spider</span><span> -- in practice, a Python object with a few special features, for which you write the code and the framework is responsible for triggering it.</span></div>
<b><br/></b>

<p dir="ltr">
<span>Just so that you have an idea of what it looks like, come on take a peek at the code of a little program below that uses Scrapy to extract some information (link, title and number of views) from a YouTube channel. Don't worry about understanding this code yet, we're just showing it here so that you have a feeling of a code using Scrapy. By the end of this tutorial, you'll be able to understand and write programs like this one. =)</span></p>
<b><br/></b>

<div dir="ltr">
<table><colgroup><col/></colgroup><tbody>
<tr><td><p dir="ltr">
<span>import</span><span> scrapy</span></p>
<p dir="ltr">
<span>from</span><span> scrapy</span><span>.</span><span>contrib</span><span>.</span><span>loader </span><span>import</span><span> </span><span>ItemLoader</span></p>
<br/>
<p dir="ltr">
<span>class</span><span> </span><span>YoutubeVideo</span><span>(</span><span>scrapy</span><span>.</span><span>Item</span><span>):</span></p>
<p dir="ltr">
<span>    link </span><span>=</span><span> scrapy</span><span>.</span><span>Field</span><span>()</span></p>
<p dir="ltr">
<span>    title </span><span>=</span><span> scrapy</span><span>.</span><span>Field</span><span>()</span></p>
<p dir="ltr">
<span>    views </span><span>=</span><span> scrapy</span><span>.</span><span>Field</span><span>()</span></p>
<br/>
<p dir="ltr">
<span>class</span><span> </span><span>YoutubeChannelLister</span><span>(</span><span>scrapy</span><span>.</span><span>Spider</span><span>):</span></p>
<p dir="ltr">
<span>    name </span><span>=</span><span> </span><span>'youtube-channel-lister'</span></p>
<p dir="ltr">
<span>    youtube_channel </span><span>=</span><span> </span><span>'LongboardUK'</span></p>
<p dir="ltr">
<span>    start_urls </span><span>=</span><span> </span><span>[</span><span>'https://www.youtube.com/user/%s/videos'</span><span> </span><span>%</span><span> youtube_channel]</span></p>
<br/>
<p dir="ltr">
<span>    </span><span>def</span><span> parse</span><span>(</span><span>self</span><span>,</span><span> response</span><span>):</span></p>
<p dir="ltr">
<span>        </span><span>for</span><span> sel </span><span>in</span><span> response</span><span>.</span><span>css</span><span>(</span><span>"ul#channels-browse-content-grid &gt; li"</span><span>):</span></p>
<p dir="ltr">
<span>            loader </span><span>=</span><span> </span><span>ItemLoader</span><span>(</span><span>YoutubeVideo</span><span>(),</span><span> selector</span><span>=</span><span>sel)</span></p>
<br/>
<p dir="ltr">
<span>            loader</span><span>.</span><span>add_xpath</span><span>(</span><span>'link'</span><span>,</span><span> </span><span>'.//h3/a/@href')</span></p>
<p dir="ltr">
<span>            loader</span><span>.</span><span>add_xpath</span><span>(</span><span>'title'</span><span>,</span><span> </span><span>'.//h3/a/text()')</span></p>
<p dir="ltr">
<span>            loader</span><span>.</span><span>add_xpath</span><span>(</span><span>'views'</span><span>,</span><span> </span><span>".//ul/li[1]/text()")</span></p>
<br/>
<p dir="ltr">
<span>            </span><span>yield</span><span> loader</span><span>.</span><span>load_item</span><span>()</span></p>
</td></tr>
</tbody></table>
</div>
<b><br/></b>

<p dir="ltr">
<span>Before we talk more about Scrapy, make sure you have the latest version installed using the command (depending on your environment, you may need to use </span><span>sudo</span><span> or the </span><span>--user</span><span> option for pip install):</span></p>
<b><br/></b>

<p dir="ltr">
<span>pip install --upgrade scrapy</span></p>
<b><br/></b>

<blockquote class="tr_bq">
<span>Note:</span><span> depending on your Python environment, the installation may be a bit tricky because of the dependency on </span><a href="https://twistedmatrix.com/"><span>Twisted</span></a><span>. If you use Windows, check out </span><a href="http://scrapy.readthedocs.org/en/latest/intro/install.html#platform-specific-installation-notes"><span>the specific instructions in the official installation guide</span></a><span>. If you use a Debian-based Linux distro, you may want to use the </span><a href="http://scrapy.readthedocs.org/en/latest/topics/ubuntu.html#topics-ubuntu"><span>official Scrapy APT repository</span></a><span>.</span></blockquote>
<b><br/></b>

<p dir="ltr">
<span>To be able to follow this tutorial, you'll need Scrapy version 0.24 or above. You can check your installed Scrapy version using the command:</span></p>
<b><br/></b>

<p dir="ltr">
<span>python -c 'import scrapy; print("%s.%s.%s" % scrapy.version_info)'</span></p>
<b><br/></b>

<p dir="ltr">
<span>The output of this command in the environment we used for this tutorial is like this:</span></p>
<b><br/></b>

<p dir="ltr">
<span>$ python -c 'import scrapy; print("%s.%s.%s" % scrapy.version_info)'</span></p>
<p dir="ltr">
<span>0.24.2</span></p>
<b><br/></b>

<p dir="ltr">
<span>The anatomy of a spider</span></p>

<b><br/></b>

<p dir="ltr">
<span>A </span><span>Scrapy spider</span><span> is responsible for defining how to follow the links "navigating" through a website (that's the so-called </span><span>crawling</span><span> part) and how to extract the information from the pages into Python data structures.</span></p>
<b><br/></b>

<div dir="ltr">
<span>To define a minimal spider, create a class extending </span><a href="http://doc.scrapy.org/en/latest/topics/spiders.html#scrapy.spider.Spider"><span>scrapy.Spider</span></a><span> and give it a name using the </span><span>name</span><span> attribute:</span></div>
<b><br/></b>

<div dir="ltr">
<table><colgroup><col/></colgroup><tbody>
<tr><td><p dir="ltr">
<span>import</span><span> scrapy</span></p>
<br/>
<br/>
<p dir="ltr">
<span>class</span><span> </span><span>MinimalSpider</span><span>(</span><span>scrapy</span><span>.</span><span>Spider</span><span>):</span></p>
<p dir="ltr">
<span>    </span><span>"""The smallest Scrapy-Spider in the world!"""</span></p>
<p dir="ltr">
<span>    name </span><span>=</span><span> </span><span>'minimal'</span></p>
</td></tr>
</tbody></table>
</div>
<b><br/></b>

<p dir="ltr">
<span>Put this in a file with the name </span><span>minimal.py</span><span> and run your spider to check if everything is okay, using the command:</span></p>
<b><br/></b>

<p dir="ltr">
<span>scrapy runspider minimal.py</span></p>
<b><br/></b>

<p dir="ltr">
<span>If everything is fine, you'll see in the screen some messages from the log marked as INFO and DEBUG. If there is any message marked as ERROR, it means that there is something wrong and you need to check for errors in your spider code.</span></p>
<b><br/></b>

<div dir="ltr">
<span>The life of a spider starts with the generation of HTTP requests (</span><a href="http://doc.scrapy.org/en/latest/topics/request-response.html"><span>Request objects</span></a><span>) to put in motion the framework engine. The part of the spider responsible for this is the </span><span>start_requests()</span><span> method, that returns an </span><a href="https://docs.python.org/2/glossary.html#term-iterable"><span>iterable</span></a><span> with the first requests to be done for the spider.</span></div>
<b><br/></b>

<p dir="ltr">
<span>Adding this element to our minimal spider, we have:</span></p>
<b><br/></b>

<div dir="ltr">
<table><colgroup><col/></colgroup><tbody>
<tr><td><p dir="ltr">
<span>import</span><span> scrapy</span></p>
<br/>
<br/>
<p dir="ltr">
<span>class</span><span> </span><span>MinimalSpider</span><span>(</span><span>scrapy</span><span>.</span><span>Spider</span><span>):</span></p>
<p dir="ltr">
<span>    </span><span>"""The smallest Scrapy-Spider of the world, maybe"""</span></p>
<p dir="ltr">
<span>    name </span><span>=</span><span> </span><span>'minimal'</span></p>
<br/>
<p dir="ltr">
<span>    </span><span>def</span><span> start_requests</span><span>(</span><span>self</span><span>):</span></p>
<p dir="ltr">
<span>        </span><span>return</span><span> </span><span>[</span><span>scrapy</span><span>.</span><span>Request</span><span>(</span><span>url)</span></p>
<p dir="ltr">
<span>                </span><span>for</span><span> url </span><span>in</span><span> </span><span>[</span><span>'http://www.google.com'</span><span>,</span><span> </span><span>http://www.yahoo.com'</span><span>]]</span></p>
</td></tr>
</tbody></table>
</div>
<b><br/></b>

<div dir="ltr">
<span>The </span><span>start_requests()</span><span> method must return an </span><a href="https://docs.python.org/2/glossary.html#term-iterable"><span>iterable</span></a><span> of </span><a href="http://doc.scrapy.org/en/latest/topics/request-response.html"><span>scrapy.Request</span></a><span> objects, which represent an HTTP request to be made by the framework (these contain data like URL, parameters, cookies, etc) and define a function to be called when the request is complete -- a </span><span>callback</span><span>.</span></div>
<b><br/></b>

<blockquote class="tr_bq">
<span>Note:</span><span> if you are familiar with implementing AJAX in JavaScript, this way of work dispatching requests and registering </span><span>callbacks</span><span> may sound familiar.</span></blockquote>
<b><br/></b>

<div dir="ltr">
<span>In our example, we return a simple list of requests to Google and Yahoo websites, but the </span><span>start_requests()</span><span> method could also be implemented as a </span><a href="https://wiki.python.org/moin/Generators"><span>Python generator</span></a><span>.</span></div>
<p dir="ltr">
<span>If you have tried to execute the example like it is now, you may noticed that there is something still missing, because Scrapy will show two messages marked as ERROR, complaining that a method was not implemented:</span></p>
<b><br/></b>

<p dir="ltr">
<span>....</span></p>
<p dir="ltr">
<span>  File "/home/elias/.virtualenvs/scrapy/local/lib/python2.7/site-packages/scrapy/spider.py", line 56, in parse</span></p>
<p dir="ltr">
<span>    raise NotImplementedError</span></p>
<p dir="ltr">
<span>exceptions.NotImplementedError: </span></p>
<b><br/></b>

<p dir="ltr">
<span>This happens because, as we didn't register a callback for the Request objects, Scrapy tried to call the default callback, which is the </span><span>parse()</span><span> method of the Spider object. Let's add this method to our minimal spider, so that we can execute it:</span></p>
<b><br/></b>

<div dir="ltr">
<table><colgroup><col/></colgroup><tbody>
<tr><td><p dir="ltr">
<span>import</span><span> scrapy</span></p>
<br/>
<br/>
<p dir="ltr">
<span>class</span><span> </span><span>MinimalSpider</span><span>(</span><span>scrapy</span><span>.</span><span>Spider</span><span>):</span></p>
<p dir="ltr">
<span>    </span><span>"""The 2nd smallest Scrapy-Spider of the world!"""</span></p>
<p dir="ltr">
<span>    name </span><span>=</span><span> </span><span>'minimal'</span></p>
<br/>
<p dir="ltr">
<span>    </span><span>def</span><span> start_requests</span><span>(</span><span>self</span><span>):</span></p>
<p dir="ltr">
<span>        </span><span>return</span><span> </span><span>(</span><span>scrapy</span><span>.</span><span>Request</span><span>(</span><span>url)</span></p>
<p dir="ltr">
<span>                </span><span>for</span><span> url </span><span>in</span><span> </span><span>[</span><span>'http://www.google.com'</span><span>,</span><span> </span><span>http://www.yahoo.com'</span><span>])</span></p>
<br/>
<p dir="ltr">
<span>    </span><span>def</span><span> parse</span><span>(</span><span>self</span><span>,</span><span> response</span><span>):</span></p>
<p dir="ltr">
<span>        </span><span>self</span><span>.</span><span>log</span><span>(</span><span>'GETTING URL: %s'</span><span> </span><span>%</span><span> response</span><span>.</span><span>url)</span></p>
</td></tr>
</tbody></table>
</div>
<b><br/></b>

<p dir="ltr">
<span>Now, when you execute it using the command: </span><span>scrapy runspider minimal.py</span><span> you should see something like this in the output:</span></p>
<b><br/></b>

<p dir="ltr">
<span>2014-07-26 15:39:56-0300 [minimal] DEBUG: Crawled (200) &lt;GET http://www.google.com.br/?gfe_rd=cr&amp;ei=_PXTU8f6N4mc8Aas1YDABA&gt; (referer: None)</span></p>
<p dir="ltr">
<span>2014-07-26 15:39:56-0300 [minimal] DEBUG: GETTING URL: http://www.google.com.br/?gfe_rd=cr&amp;ei=_PXTU8f6N4mc8Aas1YDABA</span></p>
<p dir="ltr">
<span>2014-07-26 15:39:57-0300 [minimal] DEBUG: Redirecting (302) to &lt;GET https://br.yahoo.com/?p=us&gt; from &lt;GET https://www.yahoo.com/&gt;</span></p>
<p dir="ltr">
<span>2014-07-26 15:39:58-0300 [minimal] DEBUG: Crawled (200) &lt;GET https://br.yahoo.com/?p=us&gt; (referer: None)</span></p>
<p dir="ltr">
<span>2014-07-26 15:39:58-0300 [minimal] DEBUG: GETTING URL: https://br.yahoo.com/?p=us</span></p>
<b><br/></b>

<p dir="ltr">
<span>To make our code even cleaner, we can take advantage of the default implementation of </span><span>start_requests()</span><span>: if you don't define it, Scrapy will create requests for a list of URLs in the attribute named </span><span>start_urls</span><span> -- the same kind of thing we're doing above. So, we'll keep the same functionality and reduce the code, using:</span></p>
<b><br/></b>

<div dir="ltr">
<table><colgroup><col/></colgroup><tbody>
<tr><td><p dir="ltr">
<span>import</span><span> scrapy</span></p>
<br/>
<br/>
<p dir="ltr">
<span>class</span><span> </span><span>MinimalSpider</span><span>(</span><span>scrapy</span><span>.</span><span>Spider</span><span>):</span></p>
<p dir="ltr">
<span>    </span><span>"""A menor Scrapy-Aranha do mundo!"""</span></p>
<p dir="ltr">
<span>    name </span><span>=</span><span> </span><span>'minimal'</span></p>
<p dir="ltr">
<span>    </span><span>start_urls </span><span>=</span><span> [</span></p>
<p dir="ltr">
<span>        </span><span>'http://www.google.com',</span></p>
<p dir="ltr">
<span>        </span><span>'http://www.yahoo.com',</span></p>
<p dir="ltr">
<span>    ]</span></p>
<br/>
<p dir="ltr">
<span>    </span><span>def</span><span> parse</span><span>(</span><span>self</span><span>,</span><span> response</span><span>):</span></p>
<p dir="ltr">
<span>        </span><span>self</span><span>.</span><span>log</span><span>(</span><span>'GETTING URL: %s'</span><span> </span><span>%</span><span> response</span><span>.</span><span>url)</span></p>
</td></tr>
</tbody></table>
</div>
<b><br/></b>

<div dir="ltr">
<span>Like in the </span><span>parse()</span><span> method shown above, every callback gets the content of the HTTP response as an argument (in a </span><a href="http://scrapy.readthedocs.org/en/latest/topics/request-response.html#scrapy.http.Response"><span>Response</span></a><span> object). So, inside this callback, where we already have the content of the page, that's where we'll do the information extraction, i.e., the </span><span>data scraping</span><span> itself.</span></div>
<b><br/></b>


<p dir="ltr">
<span><br/></span></p>
<p dir="ltr">
<span>Callbacks, Requests &amp; Items</span></p>
<p dir="ltr">
<span>Functions registered as callbacks for the requests can return an iterable of objects, in which every object can be:</span></p>
<b><br/></b>

<ul>
<li dir="ltr"><div dir="ltr">
<span>an instance of a subclass of </span><a href="http://doc.scrapy.org/en/latest/topics/items.html"><span>scrapy.Item</span></a><span>, which you define to contain the data to be collected from the page</span></div>
</li>
<li dir="ltr"><div dir="ltr">
<span>an object of type </span><a href="http://doc.scrapy.org/en/latest/topics/request-response.html"><span>scrapy.Request</span></a><span> representing yet another request to be made (possibly registering another </span><span>callback</span><span>)</span></div>
</li>
</ul>
<b><br/></b>

<p dir="ltr">
<span>With this mechanism of requests and callbacks that may generate new requests (with new callbacks), you can program the navigation through a site generating requests for the links to be followed, until getting to the pages that contain the items you're interested. For example, for a spider that needs to extract products from the website of an online store navigating through categories, you could use a structure like the following:</span></p>
<b><br/></b>

<div dir="ltr">
<table><colgroup><col/></colgroup><tbody>
<tr><td><p dir="ltr">
<span>import</span><span> scrapy</span></p>
<br/>
<p dir="ltr">
<span>class</span><span> </span><span>SkeletonSpider</span><span>(</span><span>scrapy</span><span>.</span><span>Spider</span><span>):</span></p>
<p dir="ltr">
<span>    name </span><span>=</span><span> </span><span>'spider-mummy'</span></p>
<p dir="ltr">
<span>    start_urls </span><span>=</span><span> </span><span>[</span><span>'http://www.some-online-webstore.com']</span></p>
<br/>
<p dir="ltr">
<span>    </span><span>def</span><span> </span><span>parse</span><span>(</span><span>self</span><span>,</span><span> response</span><span>):</span></p>
<p dir="ltr">
<span>        </span><span>for</span><span> c </span><span>in</span><span> </span><span>[...]:</span></p>
<p dir="ltr">
<span>            url_category </span><span>=</span><span> </span><span>...</span></p>
<p dir="ltr">
<span>            </span><span>yield</span><span> scrapy</span><span>.</span><span>Request</span><span>(</span><span>url_category</span><span>,</span><span> </span><span>self</span><span>.</span><span>parse_category_page</span><span>)</span></p>
<br/>
<p dir="ltr">
<span>    </span><span>def</span><span> </span><span>parse_category_page</span><span>(</span><span>self</span><span>,</span><span> response</span><span>):</span></p>
<p dir="ltr">
<span>        </span><span>for</span><span> p </span><span>in</span><span> </span><span>[...]:</span></p>
<p dir="ltr">
<span>            url_product </span><span>=</span><span> </span><span>...</span></p>
<p dir="ltr">
<span>            </span><span>yield</span><span> scrapy</span><span>.</span><span>Request</span><span>(</span><span>url_product</span><span>,</span><span> </span><span>self</span><span>.</span><span>parse_product</span><span>)</span></p>
<br/>
<p dir="ltr">
<span>    </span><span>def</span><span> </span><span>parse_product</span><span>(</span><span>self</span><span>,</span><span> response</span><span>):</span></p>
<p dir="ltr">
<span>        </span><span>...</span></p>
</td></tr>
</tbody></table>
</div>
<b><br/></b>

<p dir="ltr">
<span>In the above structure, the default callback -- </span><span>parse()</span><span> method -- handles the response of the first request to the online store website and generates new requests for the pages of the categories, registering another callback to handle them -- the </span><span>parse_category_page()</span><span> method. This last method does something similar, generating the requests for the product pages, this time registering a callback that extracts the item objects with the product data.</span></p>
<b><br/></b>

<h2 dir="ltr">
<span>Why do I need to define classes for the items?</span></h2>
<b><br/></b>

<p dir="ltr">
<span>Scrapy proposes that you create a few classes that represent the items you intend to extract from the pages. For example, if you want to extract the prices and details of products from an online store, you could use a class like the following:</span></p>
<b><br/></b>

<div dir="ltr">
<table><colgroup><col/></colgroup><tbody>
<tr><td><p dir="ltr">
<span>import</span><span> scrapy</span></p>
<br/>
<p dir="ltr">
<span>class</span><span> </span><span>Product</span><span>(</span><span>scrapy</span><span>.</span><span>Item)</span></p>
<p dir="ltr">
<span>    description </span><span>=</span><span> scrapy</span><span>.</span><span>Field</span><span>()</span></p>
<p dir="ltr">
<span>    price </span><span>=</span><span> scrapy</span><span>.</span><span>Field</span><span>()</span></p>
<p dir="ltr">
<span>    brand </span><span>=</span><span> scrapy</span><span>.</span><span>Field</span><span>()</span></p>
<p dir="ltr">
<span>    category </span><span>=</span><span> scrapy</span><span>.</span><span>Field</span><span>()</span></p>
</td></tr>
</tbody></table>
</div>
<b><br/></b>

<div dir="ltr">
<span>As you can see, the item classes are just subclasses from </span><a href="http://doc.scrapy.org/en/latest/topics/items.html"><span>scrapy.Item</span></a><span> in which you add the desired fields (instances of the class </span><a href="http://doc.scrapy.org/en/latest/topics/items.html#scrapy.item.Field"><span>scrapy.Field</span></a><span>). You can then use an instance of this class like if it were a Python dictionary:</span></div>
<b><br/></b>

<div dir="ltr">
<table><colgroup><col/></colgroup><tbody>
<tr><td><p dir="ltr">
<span>&gt;&gt;&gt;</span><span> p </span><span>=</span><span> </span><span>Product</span><span>()</span></p>
<p dir="ltr">
<span>&gt;&gt;&gt;</span><span> p</span><span>[</span><span>'price'</span><span>]</span><span> </span><span>=</span><span> </span><span>13</span></p>
<p dir="ltr">
<span>&gt;&gt;&gt;</span><span> </span><span>print</span><span> p</span></p>
<p dir="ltr">
<span>{</span><span>'price'</span><span>:</span><span> </span><span>13}</span></p>
</td></tr>
</tbody></table>
</div>
<b><br/></b>

<p dir="ltr">
<span>The biggest difference from a traditional dictionary is that an Item by default does not allow you to assign a value to a key that was not declared as a field:</span></p>
<b><br/></b>

<div dir="ltr">
<table><colgroup><col/></colgroup><tbody>
<tr><td><p dir="ltr">
<span>&gt;&gt;&gt;</span><span> p</span><span>[</span><span>'silly_walk'</span><span>]</span><span> </span><span>=</span><span> </span><span>54</span></p>
<p dir="ltr">
<span>...</span></p>
<p dir="ltr">
<span>KeyError</span><span>:</span><span> </span><span>'Product does not support field: silly_walk'</span></p>
</td></tr>
</tbody></table>
</div>
<b><br/></b>

<div dir="ltr">
<span>The advantage of defining classes for items is that it allows you to take advantage of other features of the framework that works for these classes. For example, </span><a href="http://scrapy.readthedocs.org/en/latest/topics/feed-exports.html"><span>you can use the feed exports mechanism</span></a><span> to export the collected items to </span><a href="http://en.wikipedia.org/wiki/JSON"><span>JSON</span></a><span>, </span><a href="http://en.wikipedia.org/wiki/Comma-separated_values"><span>CSV</span></a><span>, </span><a href="http://en.wikipedia.org/wiki/XML"><span>XML</span></a><span>, etc. You can also exploit the </span><a href="http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html"><span>item pipeline</span></a><span> features, that allows you to plug-in other processing on top of the collected items (things like validating the extracted data, removing duplicated items, storing in a database, etc).</span></div>
<b><br/></b>

<p dir="ltr">
<span>Now, let's do some scraping!</span></p>
<div dir="ltr">
<span>To do the scraping itself, i.e., extracting the data from the page, it's nice if you know </span><a href="http://en.wikipedia.org/wiki/XPath"><span>XPath</span></a><span>, a language created for doing queries in XML content which is core to the </span><a href="http://doc.scrapy.org/en/latest/topics/selectors.html"><span>selectors mechanism of the framework</span></a><span>. If you don't know XPath, you can use </span><a href="http://en.wikipedia.org/wiki/Cascading_Style_Sheets#Selector"><span>CSS selectors</span></a><span> in Scrapy just as well. We encourage you to learn some XPath nevertheless, because it allows for expressions much more powerful than just CSS (in fact, the CSS functionality in Scrapy works by converting your CSS expressions to XPath expressions). We'll put some links to useful resources about these at the end of the article.</span></div>
<b><br/></b>

<div dir="ltr">
<span>So, you can test the result of XPath or CSS expressions for a page using the </span><a href="http://doc.scrapy.org/en/latest/topics/shell.html"><span>Scrapy shell</span></a><span>. Run the command:</span></div>
<b><br/></b>

<p dir="ltr">
<span>scrapy shell http://stackoverflow.com</span></p>
<b><br/></b>

<p dir="ltr">
<span>This command makes a request to the informed URL and opens a Python shell (or IPython, if you have it installed) while making available some objects for you to explore. The most important object is the variable </span><span>response</span><span>, which contains the response of the HTTP request and corresponds to the </span><span>response</span><span> argument received by the callbacks.</span></p>
<b><br/></b>


<b><br/></b>

<div dir="ltr">
<table><colgroup><col/></colgroup><tbody>
<tr><td><p dir="ltr">
<span>&gt;&gt;&gt;</span><span> response</span><span>.</span><span>url</span></p>
<p dir="ltr">
<span>'http://stackoverflow.com'</span></p>
<p dir="ltr">
<span>&gt;&gt;&gt;</span><span> response</span><span>.</span><span>headers</span></p>
<p dir="ltr">
<span>{</span><span>'Cache-Control'</span><span>:</span><span> </span><span>'public, no-cache="Set-Cookie", max-age=49',</span></p>
<p dir="ltr">
<span> </span><span>'Content-Type'</span><span>:</span><span> </span><span>'text/html; charset=utf-8',</span></p>
<p dir="ltr">
<span> </span><span>'Date'</span><span>:</span><span> </span><span>'Sat, 09 Aug 2014 03:47:31 GMT',</span></p>
<p dir="ltr">
<span> </span><span>'Expires'</span><span>:</span><span> </span><span>'Sat, 09 Aug 2014 03:48:20 GMT',</span></p>
<p dir="ltr">
<span> </span><span>'Last-Modified'</span><span>:</span><span> </span><span>'Sat, 09 Aug 2014 03:47:20 GMT',</span></p>
<p dir="ltr">
<span> </span><span>'Set-Cookie'</span><span>:</span><span> </span><span>'prov=5a8741f7-7ee3-4993-b723-72142d48696c; domain=.stackoverflow.com; expires=Fri, 01-Jan-2055 00:00:00 GMT; path=/; HttpOnly',</span></p>
<p dir="ltr">
<span> </span><span>'Vary'</span><span>:</span><span> </span><span>'*',</span></p>
<p dir="ltr">
<span> </span><span>'X-Frame-Options'</span><span>:</span><span> </span><span>'SAMEORIGIN'}</span></p>
</td></tr>
</tbody></table>
</div>
<b><br/></b>

<p dir="ltr">
<span>You can use the </span><span>xpath()</span><span> and </span><span>css()</span><span> methods of the </span><span>response</span><span> object to query the HTML content in the response:</span></p>
<b><br/></b>

<div dir="ltr">
<table><colgroup><col/></colgroup><tbody>
<tr><td><p dir="ltr">
<span>&gt;&gt;&gt;</span><span> response</span><span>.</span><span>xpath</span><span>(</span><span>'//title'</span><span>)</span><span> </span><span># gets the title via XPath</span></p>
<p dir="ltr">
<span>[&lt;</span><span>Selector</span><span> xpath</span><span>=</span><span>'//title'</span><span> data</span><span>=</span><span>u</span><span>'&lt;title&gt;Stack Overflow&lt;/title&gt;'</span><span>&gt;]</span></p>
<p dir="ltr">
<span>&gt;&gt;&gt;</span><span> response</span><span>.</span><span>css</span><span>(</span><span>'title'</span><span>)</span><span> </span><span># gets the title via CSS</span></p>
<p dir="ltr">
<span>[&lt;</span><span>Selector</span><span> xpath</span><span>=</span><span>u</span><span>'descendant-or-self::title'</span><span> data</span><span>=</span><span>u</span><span>'&lt;title&gt;Stack Overflow&lt;/title&gt;'</span><span>&gt;]</span></p>
<p dir="ltr">
<span>&gt;&gt;&gt;</span><span> len</span><span>(</span><span>response</span><span>.</span><span>css</span><span>(</span><span>'div'</span><span>))</span><span> </span><span># counts the number of div elements</span></p>
<p dir="ltr">
<span>1345</span></p>
</td></tr>
</tbody></table>
</div>
<b><br/></b>

<p dir="ltr">
<span>The result of calling one of these methods is a list object containing selector objects resulting from the query. This list object has an </span><span>extract()</span><span> method which extracts the HTML content from all the selectors together. The selectors, on the other hand, besides having their own </span><span>extract()</span><span> method to extract their content, also have </span><span>xpath()</span><span> and </span><span>css()</span><span> methods that you can use to do new queries in the scope of each selector.</span></p>
<b><br/></b>

<p dir="ltr">
<span>Take a look at the examples below in the same Scrapy shell, that will help clearing up things a little bit.</span></p>
<b><br/></b>

<p dir="ltr">
<span>Extracts HTML content from &lt;title&gt; element, calling the </span><span>extract()</span><span> method from the selector list (note that the result is a Python list):</span></p>
<b><br/></b>

<div dir="ltr">
<table><colgroup><col/></colgroup><tbody>
<tr><td><p dir="ltr">
<span>&gt;&gt;&gt;</span><span> response</span><span>.</span><span>xpath</span><span>(</span><span>'//title'</span><span>).</span><span>extract</span><span>()</span></p>
<p dir="ltr">
<span>[</span><span>u</span><span>'&lt;title&gt;Stack Overflow&lt;/title&gt;']</span></p>
</td></tr>
</tbody></table>
</div>
<b><br/></b>

<p dir="ltr">
<span>Stores the first selector of the result in a variable and calls the </span><span>extract()</span><span> method on the selector (see how the result now is just a string):</span></p>
<b><br/></b>

<div dir="ltr">
<table><colgroup><col/></colgroup><tbody>
<tr><td><p dir="ltr">
<span>&gt;&gt;&gt;</span><span> title_sel </span><span>=</span><span> response</span><span>.</span><span>xpath</span><span>(</span><span>'//title'</span><span>)[</span><span>0]</span></p>
<p dir="ltr">
<span>&gt;&gt;&gt;</span><span> title_sel</span><span>.</span><span>extract</span><span>()</span></p>
<p dir="ltr">
<span>u</span><span>'&lt;title&gt;Stack Overflow&lt;/title&gt;'</span></p>
</td></tr>
</tbody></table>
</div>
<b><br/></b>

<p dir="ltr">
<span>Applies the XPath expression </span><span>text()</span><span> to get the text content of the selector, and calls the extract() method from the resulting list:</span></p>
<b><br/></b>

<div dir="ltr">
<table><colgroup><col/></colgroup><tbody>
<tr><td><p dir="ltr">
<span>&gt;&gt;&gt;</span><span> title_sel</span><span>.</span><span>xpath</span><span>(</span><span>'text()'</span><span>).</span><span>extract</span><span>()</span></p>
<p dir="ltr">
<span>[</span><span>u</span><span>'Stack Overflow']</span></p>
</td></tr>
</tbody></table>
</div>
<b><br/></b>

<p dir="ltr">
<span>Prints the extraction of the first selector resulting of the XPath expression </span><span>text()</span><span> applied to selector in variable title_sel:</span></p>
<b><br/></b>

<div dir="ltr">
<table><colgroup><col/></colgroup><tbody>
<tr><td><p dir="ltr">
<span>&gt;&gt;&gt;</span><span> </span><span>print</span><span> title_sel</span><span>.</span><span>xpath</span><span>(</span><span>'text()'</span><span>)[</span><span>0</span><span>].</span><span>extract</span><span>()</span></p>
<p dir="ltr">
<span>Stack</span><span> </span><span>Overflow</span></p>
</td></tr>
</tbody></table>
</div>
<b><br/></b>

<p dir="ltr">
<span>Well, when you have a good grip on this way to work with selectors, the simple way to extract an item is just to create an instance of the desired Item class and fill the values obtained using this selectors API.</span></p>
<b><br/></b>

<div dir="ltr">
<span>Here, take a look at the code of a spider using this technique to get the most frequently asked questions of </span><a href="http://stackoverflow.com/"><span>StackOverflow</span></a><span>:</span></div>
<b><br/></b>

<div dir="ltr">
<table><colgroup><col/></colgroup><tbody>
<tr><td><p dir="ltr">
<span>import</span><span> scrapy</span></p>
<p dir="ltr">
<span>import</span><span> urlparse</span></p>
<br/>
<br/>
<p dir="ltr">
<span>class</span><span> </span><span>Question</span><span>(</span><span>scrapy</span><span>.</span><span>Item</span><span>):</span></p>
<p dir="ltr">
<span>    link </span><span>=</span><span> scrapy</span><span>.</span><span>Field</span><span>()</span></p>
<p dir="ltr">
<span>    title </span><span>=</span><span> scrapy</span><span>.</span><span>Field</span><span>()</span></p>
<p dir="ltr">
<span>    excerpt </span><span>=</span><span> scrapy</span><span>.</span><span>Field</span><span>()</span></p>
<p dir="ltr">
<span>    tags </span><span>=</span><span> scrapy</span><span>.</span><span>Field</span><span>()</span></p>
<br/>
<p dir="ltr">
<span>class</span><span> </span><span>StackoverflowTopQuestionsSpider</span><span>(</span><span>scrapy</span><span>.</span><span>Spider</span><span>):</span></p>
<p dir="ltr">
<span>    name </span><span>=</span><span> </span><span>'so-top-questions'</span></p>
<br/>
<p dir="ltr">
<span>    </span><span>def</span><span> __init__</span><span>(</span><span>self</span><span>,</span><span> tag</span><span>=</span><span>None</span><span>):</span></p>
<p dir="ltr">
<span>        questions_url </span><span>=</span><span> </span><span>'http://stackoverflow.com/questions'</span></p>
<p dir="ltr">
<span>        </span><span>if</span><span> tag:</span></p>
<p dir="ltr">
<span>            questions_url </span><span>+=</span><span> </span><span>'/tagged/%s'</span><span> </span><span>%</span><span> tag</span></p>
<br/>
<p dir="ltr">
<span>        </span><span>self</span><span>.</span><span>start_urls </span><span>=</span><span> </span><span>[</span><span>questions_url </span><span>+</span><span> </span><span>'?sort=frequent']</span></p>
<br/>
<p dir="ltr">
<span>    </span><span>def</span><span> parse</span><span>(</span><span>self</span><span>,</span><span> response</span><span>):</span></p>
<p dir="ltr">
<span>        build_full_url </span><span>=</span><span> </span><span>lambda</span><span> link</span><span>:</span><span> urlparse</span><span>.</span><span>urljoin</span><span>(</span><span>response</span><span>.</span><span>url</span><span>,</span><span> link)</span></p>
<br/>
<p dir="ltr">
<span>        </span><span>for</span><span> qsel </span><span>in</span><span> response</span><span>.</span><span>css</span><span>(</span><span>"#questions &gt; div"</span><span>):</span></p>
<p dir="ltr">
<span>            it </span><span>=</span><span> </span><span>Question</span><span>()</span></p>
<br/>
<p dir="ltr">
<span>            it</span><span>[</span><span>'link'</span><span>]</span><span> </span><span>=</span><span> build_full_url(</span></p>
<p dir="ltr">
<span>                qsel</span><span>.</span><span>css</span><span>(</span><span>'.summary h3 &gt; a'</span><span>).</span><span>xpath</span><span>(</span><span>'@href'</span><span>)[</span><span>0</span><span>].</span><span>extract</span><span>())</span></p>
<p dir="ltr">
<span>            it</span><span>[</span><span>'title'</span><span>]</span><span> </span><span>=</span><span> qsel</span><span>.</span><span>css</span><span>(</span><span>'.summary h3 &gt; a::text'</span><span>)[</span><span>0</span><span>].</span><span>extract</span><span>()</span></p>
<p dir="ltr">
<span>            it</span><span>[</span><span>'tags'</span><span>]</span><span> </span><span>=</span><span> qsel</span><span>.</span><span>css</span><span>(</span><span>'a.post-tag::text'</span><span>).</span><span>extract</span><span>()</span></p>
<p dir="ltr">
<span>            it</span><span>[</span><span>'excerpt'</span><span>]</span><span> </span><span>=</span><span> qsel</span><span>.</span><span>css</span><span>(</span><span>'div.excerpt::text'</span><span>)[</span><span>0</span><span>].</span><span>extract</span><span>()</span></p>
<br/>
<p dir="ltr">
<span>            </span><span>yield</span><span> it</span></p>
</td></tr>
</tbody></table>
</div>
<b><br/></b>

<p dir="ltr">
<span>As you can see, the spider defines an Item class named </span><span>Question</span><span>, and uses the Selectors API to iterate through the HTML elements of the questions (obtained with the CSS selector </span><span>#questions &gt; div</span><span>) and generating a Question object for each one of these elements, filling all the fields (link, title, tags and question excerpt).</span></p>
<b><br/></b>

<div dir="ltr">
<span>There are two interesting things worth noticing in the extraction done in the </span><span>parse()</span><span> callback: the first one is that we use a pseudo-selector </span><span>::text</span><span> to get the text content of the elements, avoiding the HTML tags. The second is how we use the function </span><a href="https://docs.python.org/2/library/urlparse.html"><span>urlparse.urljoin()</span></a><span> to combine the URL of the request with the content of the </span><span>href</span><span> attribute, making sure that the result of this will be a correct absolute URL.</span></div>
<b><br/></b>

<p dir="ltr">
<span>Put this code in a file named top_asked_so_questions.py and run it using the command:</span></p>
<b><br/></b>

<p dir="ltr">
<span>scrapy runspider top_asked_so_questions.py -o questions.json</span><br/>
<span><br/></span></p>
<p dir="ltr">
<span>If everything went well, Scrapy will show in the screen the scraped items and also write a file named </span><span>questions.json</span><span> containing them. At the end of the output, you should see some stats, including the item scraped count:</span></p>
<b><br/></b>

<p dir="ltr">
<span>2014-08-02 14:27:37-0300 [so-top-questions] INFO: Dumping Scrapy stats:</span></p>
<p dir="ltr">
<span><span class="Apple-tab-span"> </span></span><span>{'downloader/request_bytes': 242,</span></p>
<p dir="ltr">
<span><span class="Apple-tab-span"> </span></span><span> 'downloader/request_count': 1,</span></p>
<p dir="ltr">
<span><span class="Apple-tab-span"> </span></span><span> ...</span></p>
<p dir="ltr">
<span><span class="Apple-tab-span"> </span></span><span> </span><span>'item_scraped_count': 50,</span></p>
<p dir="ltr">
<span><span class="Apple-tab-span"> </span></span><span> 'log_count/DEBUG': 53,</span></p>
<p dir="ltr">
<span><span class="Apple-tab-span"> </span></span><span> 'log_count/INFO': 8,</span></p>
<p dir="ltr">
<span><span class="Apple-tab-span"> </span></span><span> ...</span></p>
<p dir="ltr">
<span><span class="Apple-tab-span"> </span></span><span> 'start_time': datetime.datetime(2014, 8, 2, 17, 27, 36, 912002)}</span></p>
<p dir="ltr">
<span>2014-08-02 14:27:37-0300 [so-top-questions] INFO: Spider closed (finished)</span></p>
<blockquote class="tr_bq">
<span><b>Note:</b></span><span> if you run this twice in a row, you need to remove the output file questions.json file before each run. This is because</span><span> </span><span>Scrapy </span><span>by default</span><span> appends to a file instead of overwriting it, which ends up making the JSON file unusable. This is done</span><span> for historical reasons, </span><span>it made sense for spiders which used the JSON Lines format (the previous default), and may change in the future.</span></blockquote>

<h2 dir="ltr">
<span>Arachnoid arguments</span></h2>
<p dir="ltr">
<span>You may have noticed that the class for this spider has a constructor accepting an optional argument called </span><span>tag</span><span>. We can specify a value for this argument for the spider to get the frequently asked questions with the python tag, using the </span><span>-a</span><span> option:</span></p>
<b><br/></b>

<p dir="ltr">
<span>scrapy runspider top_asked_so_questions.py -o python-questions.json </span><span><b>-a tag=python</b></span></p>
<b><br/></b>

<p dir="ltr">
<span>Using this little trick you can write generic spiders, so that you just pass some parameters and get a different result. For example, you may write one spider for several sites that have the same HTML structure, making the URL of the site a parameter. Or, a spider for a blog in which the parameters define a time range of the posts and comments to extract.</span></p>
<b><br/></b>

<p dir="ltr">
<span>Putting it all together</span></p>
<div dir="ltr">
<span>In the previous sections, you saw how to do web crawling with Scrapy, navigating through the pages of a site using the mechanism of requests and callbacks. You also saw how to use the </span><a href="http://doc.scrapy.org/en/latest/topics/selectors.html"><span>Selector API</span></a><span> to extract the content of a page into items and execute the spider using the command </span><span>scrapy runspider</span><span>.</span></div>
<b><br/></b>

<div dir="ltr">
<span>Now, we shall put it all together in a spider that solves the problem we presented in the introduction: let's scrape the latest "big questions" articles from </span><a href="http://mentalfloss.com/"><span>mentalfloss.com</span></a><span>, offering an option to inform the category (</span><a href="http://mentalfloss.com/big-questions/origins"><span>Origins</span></a><span>, </span><a href="http://mentalfloss.com/big-questions/the-body"><span>The Body</span></a><span>, </span><a href="http://mentalfloss.com/big-questions/fact-check"><span>Fact Check</span></a><span>, etc). This way, if you just run the spider, it should scrape all the articles in the blog; if you pass in a category, it should scrape only the articles of that subject.</span></div>
<b><br/></b>

<blockquote class="tr_bq">
<span>Note:</span><span> Before writing a spider, it's useful to explore a little bit the pages of the site using the browser's inspection capabilities and the </span><span>scrapy shell</span><span>, so that you can see how the site is structured and you can also try a few CSS or XPath expressions in the shell. There are also some browser extensions that allow you to test XPath expressions directly in a page: </span><a href="https://chrome.google.com/webstore/detail/xpath-helper/hgimnogjllphhhkhlmebbmlgjoejdpjl?hl=en"><span>XPath Helper</span></a><span> for Chrome and </span><a href="https://addons.mozilla.org/en-US/firefox/addon/xpath-checker/"><span>XPath Checker</span></a><span> for Firefox. Discovering the best way to extract the content of a site using XPath or CSS is more of an art than a science, therefore we won't try to explain much here, but it's worthy telling you that you learn a lot after a little experience.</span></blockquote>
<b><br/></b>

<p dir="ltr">
<span>Have a look at the final code of the spider:</span></p>
<b><br/></b>

<div dir="ltr">
<table><colgroup><col/></colgroup><tbody>
<tr><td><p dir="ltr">
<span>import</span><span> scrapy</span></p>
<p dir="ltr">
<span>import</span><span> urlparse</span></p>
<br/>
<br/>
<p dir="ltr">
<span>class</span><span> </span><span>Article</span><span>(</span><span>scrapy</span><span>.</span><span>Item</span><span>):</span></p>
<p dir="ltr">
<span>    title </span><span>=</span><span> scrapy</span><span>.</span><span>Field</span><span>()</span></p>
<p dir="ltr">
<span>    content </span><span>=</span><span> scrapy</span><span>.</span><span>Field</span><span>()</span></p>
<p dir="ltr">
<span>    link </span><span>=</span><span> scrapy</span><span>.</span><span>Field</span><span>()</span></p>
<p dir="ltr">
<span>    author </span><span>=</span><span> scrapy</span><span>.</span><span>Field</span><span>()</span></p>
<p dir="ltr">
<span>    date </span><span>=</span><span> scrapy</span><span>.</span><span>Field</span><span>()</span></p>
<br/>
<br/>
<p dir="ltr">
<span>class</span><span> </span><span>MentalFlossArticles</span><span>(</span><span>scrapy</span><span>.</span><span>Spider</span><span>):</span></p>
<p dir="ltr">
<span>    name </span><span>=</span><span> </span><span>'mentalfloss-articles'</span></p>
<br/>
<p dir="ltr">
<span>    </span><span>def</span><span> __init__</span><span>(</span><span>self</span><span>,</span><span> category</span><span>=</span><span>None</span><span>):</span></p>
<p dir="ltr">
<span>        articles_url </span><span>=</span><span> </span><span>'http://mentalfloss.com/big-questions'</span></p>
<br/>
<p dir="ltr">
<span>        </span><span>if</span><span> category:</span></p>
<p dir="ltr">
<span>            articles_url </span><span>+=</span><span> </span><span>'/'</span><span> </span><span>+</span><span> category</span></p>
<br/>
<p dir="ltr">
<span>        </span><span>self</span><span>.</span><span>start_urls </span><span>=</span><span> </span><span>[</span><span>articles_url]</span></p>
<br/>
<p dir="ltr">
<span>    </span><span>def</span><span> parse</span><span>(</span><span>self</span><span>,</span><span> response</span><span>):</span></p>
<p dir="ltr">
<span>        </span><span>"""Gets the page with the article list,</span><br/>
<span>        find the article links and generates</span><br/>
<span>        requests for each article page</span></p>
<p dir="ltr">
<span>        </span><span>"""</span></p>
<p dir="ltr">
<span>        article_links </span><span>=</span><span> response</span><span>.</span><span>xpath(</span></p>
<p dir="ltr">
<span>            </span><span>"//header/hgroup/h1/a/@href"</span></p>
<p dir="ltr">
<span>        </span><span>).</span><span>extract</span><span>()</span></p>
<br/>
<p dir="ltr">
<span>        </span><span>for</span><span> link </span><span>in</span><span> article_links:</span></p>
<p dir="ltr">
<span>            article_url </span><span>=</span><span> urlparse</span><span>.</span><span>urljoin(</span></p>
<p dir="ltr">
<span>                response</span><span>.</span><span>url</span><span>,</span><span> link)</span></p>
<p dir="ltr">
<span>            </span><span>yield</span><span> scrapy</span><span>.</span><span>Request</span><span>(</span><span>article_url,</span></p>
<p dir="ltr">
<span>                                 </span><span>self</span><span>.</span><span>extract_article)</span></p>
<br/>
<p dir="ltr">
<span>    </span><span>def</span><span> extract_article</span><span>(</span><span>self</span><span>,</span><span> response</span><span>):</span></p>
<p dir="ltr">
<span>        </span><span>"""Gets the article page and extract</span><br/>
<span>        an item with the article data</span></p>
<p dir="ltr">
<span>        </span><span>"""</span></p>
<p dir="ltr">
<span>        article </span><span>=</span><span> </span><span>Article</span><span>()</span></p>
<p dir="ltr">
<span>        css </span><span>=</span><span> </span><span>lambda</span><span> s</span><span>:</span><span> response</span><span>.</span><span>css</span><span>(</span><span>s</span><span>).</span><span>extract</span><span>()</span></p>
<br/>
<p dir="ltr">
<span>        article</span><span>[</span><span>'link'</span><span>]</span><span> </span><span>=</span><span> response</span><span>.</span><span>url</span></p>
<p dir="ltr">
<span>        article</span><span>[</span><span>'title'</span><span>]</span><span> </span><span>=</span><span> css</span><span>(</span><span>"h1.title &gt; span::text"</span><span>)[</span><span>0]</span></p>
<p dir="ltr">
<span>        article</span><span>[</span><span>'date'</span><span>]</span><span> </span><span>=</span><span> css</span><span>(</span><span>'.date-display-single::text'</span><span>)[</span><span>0]</span></p>
<br/>
<p dir="ltr">
<span>        article</span><span>[</span><span>'content'</span><span>]</span><span> </span><span>=</span><span> </span><span>" "</span><span>.</span><span>join(</span></p>
<p dir="ltr">
<span>            css</span><span>(</span><span>'#content-content p::text'</span><span>))</span></p>
<br/>
<p dir="ltr">
<span>        article</span><span>[</span><span>'author'</span><span>]</span><span> </span><span>=</span><span> css(</span></p>
<p dir="ltr">
<span>            </span><span>"div.field-name-field-enhanced-authors"</span></p>
<p dir="ltr">
<span>            </span><span>" a::text"</span><span>)[</span><span>0]</span></p>
<br/>
<p dir="ltr">
<span>        </span><span>yield</span><span> article</span></p>
</td></tr>
</tbody></table>
</div>
<b><br/></b>

<p dir="ltr">
<span>Just like before, you can run the spider with:</span></p>
<b><br/></b>

<p dir="ltr">
<span>scrapy runspider mentalfloss.py -o articles-all.json</span></p>
<b><br/></b>

<p dir="ltr">
<span>And to get the articles from each section, you can use commands like:</span></p>
<b><br/></b>

<p dir="ltr">
<span>scrapy runspider mentalfloss.py -o articles-origins.json -a category=origins</span><span><br class="kix-line-break"/></span><br/>
<span><br/></span></p>
<p dir="ltr">
<span>scrapy runspider mentalfloss.py -o articles-fact-check.json -a category=fact-check</span><span><br class="kix-line-break"/></span></p>
<b><br/></b>

<p dir="ltr">
<span>The code for this spider has a very similar structure to the previous one, with its argument handling and everything.</span></p>
<b><br/></b>

<p dir="ltr">
<span>The main difference is that in this one, the first callback (the </span><span>parse()</span><span> method) generates other requests for the article pages, which are handled by the second callback: the </span><span>extract_article()</span><span> method, which scrapes the article data.</span></p>
<b><br/></b>

<div dir="ltr">
<span>The content extraction also does a little bit more work. We created a </span><span>css()</span><span> helper function to abbreviate calling </span><span>response.css(&lt;selector&gt;).extract()</span><span> and used that to get the result of our selectors to fill the Article item. Note also how we take advantage of </span><a href="https://docs.python.org/2/reference/lexical_analysis.html#string-literal-concatenation"><span>Python's feature of concatenating literal strings</span></a><span> on the CSS selector for the author field, to break it in two lines.</span></div>
<b><br/></b>

<p dir="ltr">
<span>Final words</span></p>
<p dir="ltr">
<span>If you made until here, congratulations! Here is a trophy for you:</span></p>
<b><br/></b>


<b><br/></b>

<p dir="ltr">
<span>Now that you have learned to write Scrapy spiders and therefore are enabled to download the whole Internet to your home PC, try not to get banned by the website hosts laying around! :)</span></p>
<b><br/></b>


<b><br/></b>

<p dir="ltr">
<span>Useful resources:</span></p>


<p/>
</div>
</div></body></html>