<html><body><div><div class="content">
      
        <h1 class="content-title">Naive Bayes Classifier using Python and Kyoto Cabinet</h1>
      
      
      
  
  
  
    <p><a href="http://media.charlesleifer.com/blog/photos/p1422977174.11.png" title="photos/p1422977174.11.png"><img alt="photos/p1422977174.11.png" class="img-responsive" src="http://m.charlesleifer.com/t/800x-/blog/photos/p1422977174.11.png?key=mCmEboIjkBlzEgPuHGdkAA"/></a></p>
<p>In this post I will describe how to build a simple naive bayes classifier with Python and the <a href="http://fallabs.com/kyotocabinet/">Kyoto Cabinet</a> key/value database. I'll begin with a short description of how a probabilistic classifier works, then we will implement a simple classifier and put it to use by writing a spam detector. The training and test data will come from the Enron spam/ham corpora, which contains several thousand emails that have been pre-categorized as spam or ham.</p>
<p>The classifier structure will be based in part on the classifier presented in Toby Segaran's excellent book <a href="http://shop.oreilly.com/product/9780596529321.do">Programming Collective Intelligence</a>. I recommend picking up a copy of this book! It is packed with useful information and practical examples of machine learning algorithms.</p>
<h3>Overview of Probabilistic Classification</h3>
<p>Our classifier will measure how often certain <em>features</em> correspond with one or more <em>categories</em>. Our goal is to be able to take a set of uncategorized features and, based on the observed likelihood that a given feature belongs to a category, predict an overall best category for the feature-set.</p>
<p>Because the classifier relies on historical observations, we need a way to <em>train</em> it. So how will we train the classifier? Or, to put it another way, <em>what clues do we have that a message is spam?</em> The answer is actually pretty simple: we will just use the individual words that make up the email message. These words and their association with either spam or ham messages will form the basis of our classifier.</p>
<p>Once we have associated the various <em>features</em> (words) with our two <em>categories</em> (spam and ham), we can calculate the probability that a given feature belongs to one category or another. For instance, the probability that the word <em>money</em> appears in a spam message is much higher than the probability it appears in a legitimate email. So how do you calculate this probability?</p>
<p><a href="http://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering#Mathematical_foundation">The calculation</a> is actually pretty straightforward. Say we have trained our classifier using 200 email messages, 80 are spam and 120 are ham. Now, suppose that the word <em>money</em> appears in 25 spam emails, and only 5 of the ham emails. If we assume that an email is equally likely to be spam or ham, the probability that the word <em>money</em> indicates a spam document is calculated:</p>
<div class="highlight"><pre>"money" is spam = P(spam has "money") / (P(spam has "money") + P(ham has "money"))
                = (25 / 80) / ((25 / 80) + (5 / 120))
                = .88, or 88%
</pre></div>
<p>The classifier will evaluate the probabilities for all the words in a document to come up with an overall probability of the likelihood a document is either spam or ham.</p>
<p>If you are interested in reading more about our classifier, perhaps the best introduction to Bayes' theorem is the Wikipedia introductory example – which is worth checking it out. For a more thorough introduction I recommend reading the excellent post <em>An intuitive and short explanation of Bayes' Theorem</em>.</p>

<h3>What data do we need?</h3>
<p>In order to calculate these probabilities, we are going to be storing <em>counts</em> of things, because it is these counts that allow us to calculate percentages, which can be combined to give an overall probability.</p>
<p>Recollecting the example above, we have:</p>
<div class="highlight"><pre>let P(M|S) = probability that "money" appears in spam email (25 / 80)
let P(M|H) = probability that "money" appears in ham email (5 / 120)
P(S|M) is the probability that a message is spam, if it contains "money".

P(S|M) = P(M|S) / P(M|S) + P(M|H)
       = (25 / 80) / ((25 / 80) + (5 / 120))
</pre></div>
<p>So we will need to store the following counts of things:</p>
<ul>
<li>How many documents are in each category.</li>
<li>How often a word is associated with each category.</li>
</ul>
<p>We'll use <a href="http://fallabs.com/kyotocabinet/">Kyoto Cabinet</a>, a super fast key/value store, to persist our trained features and categories. Kyoto Cabinet has a couple different database types, but since we are going to be iterating over ranges of keys, we will use the B-Tree.</p>
<h3>Installing Kyoto Cabinet</h3>
<p><a href="http://media.charlesleifer.com/blog/photos/p1422943877.01.jpg" title="photos/p1422943877.01.jpg"><img alt="photos/p1422943877.01.jpg" class="img-responsive" src="http://m.charlesleifer.com/t/800x-/blog/photos/p1422943877.01.jpg?key=5-qi_gjP8L1V_3MD96wNwg"/></a></p>
<p>Before we can actually get started, we'll need to install <a href="http://fallabs.com/kyotocabinet/">Kyoto Cabinet</a> and the <a href="http://fallabs.com/kyotocabinet/pythonlegacypkg/">Python bindings</a>. At the time of writing, Kyoto Cabinet is at version 1.2.76 and the Python package is 1.18.</p>
<p>First we'll compile and install Kyoto Cabinet:</p>
<div class="highlight"><pre><span class="gp">$</span> <span class="nb">export</span> <span class="nv">KCVER</span><span class="o">=</span><span class="s2">"1.2.76"</span>
<span class="gp">$</span> wget  http://fallabs.com/kyotocabinet/pkg/kyotocabinet-$KCVER.tar.gz
<span class="gp">$</span> tar xzf kyotocabinet-$KCVER.tar.gz
<span class="gp">$</span> <span class="nb">cd</span> kyotocabinet-$KCVER
<span class="gp">$</span> ./configure
<span class="gp">$</span> make
<span class="gp">$</span> sudo make install
</pre></div>
<p>Now we'll install the python package. You can do this in a <code>virtualenv</code> if you want, or install system-wide.</p>
<div class="highlight"><pre><span class="gp">$</span> <span class="nb">export</span> <span class="nv">PYKCVER</span><span class="o">=</span><span class="s2">"1.18"</span>
<span class="gp">$</span> wget http://fallabs.com/kyotocabinet/pythonlegacypkg/kyotocabinet-python-legacy-$PYKCVER.tar.gz
<span class="gp">$</span> tar xzf kyotocabinet-python-legacy-$PYKCVER.tar.gz
<span class="gp">$</span> <span class="nb">cd</span> kyotocabinet-python-legacy-$PYKCVER
<span class="gp">$</span> python setup.py build
<span class="gp">$</span> python setup.py install  <span class="c1"># Note: you may need to use "sudo".</span>
</pre></div>
<p>You can test your installation by running:</p>
<div class="highlight"><pre><span class="gp">$</span> python -c <span class="s1">'import kyotocabinet; print kyotocabinet.VERSION'</span>
<span class="go">1.2.76</span>
</pre></div>
<h3>Coding up the persistence layer</h3>
<p>We'll begin by extending the <code>kyotocabinet.DB</code> class to add methods for storing and retrieving counts for the data described above.</p>
<p>We will have three types of keys:</p>
<ul>
<li>How many times a category is observed, e.g. <code>category.spam</code> and <code>category.ham</code>.</li>
<li>How often a feature is associated with a given category, e.g. <code>feature2category.money.spam</code>.</li>
<li>The total number of documents, <code>total-categories</code>.</li>
</ul>
<p>The following class provides convenience methods for incrementing and retrieving counts of features and categories.</p>
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">operator</span>
<span class="kn">import</span> <span class="nn">struct</span>
<span class="kn">import</span> <span class="nn">kyotocabinet</span> <span class="kn">as</span> <span class="nn">kc</span>


<span class="k">class</span> <span class="nc">ClassifierDB</span><span class="p">(</span><span class="n">kc</span><span class="o">.</span><span class="n">DB</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Wrapper for `kyotocabinet.DB` that provides utilities for working with</span>
<span class="sd">    features and categories.</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ClassifierDB</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_category_tmpl</span> <span class="o">=</span> <span class="s1">'category.</span><span class="si">%s</span><span class="s1">'</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_feature_to_category_tmpl</span> <span class="o">=</span> <span class="s1">'feature2category.</span><span class="si">%s</span><span class="s1">.</span><span class="si">%s</span><span class="s1">'</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_total_count</span> <span class="o">=</span> <span class="s1">'total-count'</span>

    <span class="k">def</span> <span class="nf">get_int</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
        <span class="c1"># Kyoto serializes ints big-endian 8-bytes long, so we need to unpack</span>
        <span class="c1"># them using the `struct` module.</span>
        <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">value</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">struct</span><span class="o">.</span><span class="n">unpack</span><span class="p">(</span><span class="s1">'&gt;Q'</span><span class="p">,</span> <span class="n">value</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">incr_feature_category</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature</span><span class="p">,</span> <span class="n">category</span><span class="p">):</span>
        <span class="sd">"""Increment the count for the feature in the given category."""</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">increment</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_feature_to_category_tmpl</span> <span class="o">%</span> <span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">category</span><span class="p">),</span>
            <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">incr_category</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">category</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">        Increment the count for the given category, increasing the total</span>
<span class="sd">        count as well.</span>
<span class="sd">        """</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">increment</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_total_count</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">increment</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_category_tmpl</span> <span class="o">%</span> <span class="n">category</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">category_count</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">category</span><span class="p">):</span>
        <span class="sd">"""Return the number of documents in the given category."""</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_category_tmpl</span> <span class="o">%</span> <span class="n">category</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">total_count</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">"""Return the total number of documents overall."""</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_total_count</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_feature_category_count</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature</span><span class="p">,</span> <span class="n">category</span><span class="p">):</span>
        <span class="sd">"""Get the count of the feature in the given category."""</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_int</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_feature_to_category_tmpl</span> <span class="o">%</span> <span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">category</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">get_feature_counts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature</span><span class="p">):</span>
        <span class="sd">"""Get the total count for the feature across all categories."""</span>
        <span class="n">prefix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_feature_to_category_tmpl</span> <span class="o">%</span> <span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="s1">''</span><span class="p">)</span>
        <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">match_prefix</span><span class="p">(</span><span class="n">prefix</span><span class="p">):</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_int</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">total</span>

    <span class="k">def</span> <span class="nf">iter_categories</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">        Return an iterable that successively yields all the categories</span>
<span class="sd">        that have been observed.</span>
<span class="sd">        """</span>
        <span class="n">category_prefix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_category_tmpl</span> <span class="o">%</span> <span class="s1">''</span>
        <span class="n">prefix_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">category_prefix</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">category_key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">match_prefix</span><span class="p">(</span><span class="n">category_prefix</span><span class="p">):</span>
            <span class="k">yield</span> <span class="n">category_key</span><span class="p">[</span><span class="n">prefix_len</span><span class="p">:]</span>
</pre></div>
<p>Now that we have a way of storing data, let's see how to train and classify documents. Also, just a note, but since we've encapsulated the logic for storing this data in its own class, it should be pretty easy to swap out different storage engines.</p>
<h3>Building the classifier</h3>
<p>The classifier exists to answer the following question:</p>
<blockquote>
<p>Given a set of features, what is the probability they belong to a given category?</p>
</blockquote>
<p>Because this is a supervised algorithm, we will need to train the classifier by populating it with counts for features and categories from a set of pre-categorized documents. Once trained, we will plug in a list of features and receive a list of possible categories and probabilities.</p>
<p>Let's define the skeleton of our classifier object, with code for connecting to the persistence layer:</p>
<div class="highlight"><pre><span class="k">class</span> <span class="nc">NBC</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Simple naive bayes classifier.</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="n">read_only</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">        Initialize the classifier by pointing it at a database file. If you</span>
<span class="sd">        intend to only use the classifier for classifying documents, specify</span>
<span class="sd">        `read_only=True`.</span>
<span class="sd">        """</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">filename</span> <span class="o">=</span> <span class="n">filename</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">filename</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">'.kct'</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">'Database filename must have "kct" extension.'</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">db</span> <span class="o">=</span> <span class="n">ClassifierDB</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="n">read_only</span><span class="o">=</span><span class="n">read_only</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">connect</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">read_only</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">        Open the database. Since Kyoto Cabinet only allows a single writer</span>
<span class="sd">        at a time, the `connect()` method accepts a parameter allowing the</span>
<span class="sd">        database to be opened in read-only mode (supporting multiple readers).</span>

<span class="sd">        If you plan on training the classifier, specify `read_only=False`.</span>

<span class="sd">        If you plan only on classifying documents, it is safe to specify</span>
<span class="sd">        `read_only=True`.</span>
<span class="sd">        """</span>
        <span class="k">if</span> <span class="n">read_only</span><span class="p">:</span>
            <span class="n">flags</span> <span class="o">=</span> <span class="n">kc</span><span class="o">.</span><span class="n">DB</span><span class="o">.</span><span class="n">OREADER</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">flags</span> <span class="o">=</span> <span class="n">kc</span><span class="o">.</span><span class="n">DB</span><span class="o">.</span><span class="n">OWRITER</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">db</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">filename</span><span class="p">,</span> <span class="n">flags</span> <span class="o">|</span> <span class="n">kc</span><span class="o">.</span><span class="n">DB</span><span class="o">.</span><span class="n">OCREATE</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">close</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">"""Close the database."""</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">db</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="o">*</span><span class="n">categories</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">        Train the classifier, storing the association of the given feature</span>
<span class="sd">        set with the given categories.</span>
<span class="sd">        """</span>

    <span class="k">def</span> <span class="nf">feature_probability</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature</span><span class="p">,</span> <span class="n">category</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">        Calculate the probability that a particular feature is associated</span>
<span class="sd">        with the given category.</span>
<span class="sd">        """</span>

    <span class="k">def</span> <span class="nf">weighted_probability</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature</span><span class="p">,</span> <span class="n">category</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">        Determine the probability a feature corresponds to the given category.</span>
<span class="sd">        The probability is weighted by the importance of the feature, which</span>
<span class="sd">        is determined by looking at the feature across all categories in</span>
<span class="sd">        which it appears.</span>
<span class="sd">        """</span>

    <span class="k">def</span> <span class="nf">document_probability</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">category</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">        Calculate the probability that a set of features match the given</span>
<span class="sd">        category.</span>
<span class="sd">        """</span>

    <span class="k">def</span> <span class="nf">weighted_document_probability</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">category</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">        Calculate the probability that a set of features match the given</span>
<span class="sd">        category, and weight that score by the importance of the category.</span>
<span class="sd">        """</span>

    <span class="k">def</span> <span class="nf">classify</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">        Classify the features by finding the categories that match the</span>
<span class="sd">        features with the highest probability.</span>
<span class="sd">        """</span>
</pre></div>
<p>Let's start with the training method. This method will simply iterate through the features and categories provided, incrementing counts in the database.</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="o">*</span><span class="n">categories</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Increment the counts for the features in the given categories.</span>
<span class="sd">    """</span>
    <span class="k">for</span> <span class="n">category</span> <span class="ow">in</span> <span class="n">categories</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">db</span><span class="o">.</span><span class="n">incr_feature_category</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">category</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">db</span><span class="o">.</span><span class="n">incr_category</span><span class="p">(</span><span class="n">category</span><span class="p">)</span>
</pre></div>
<p>Believe it or not, the above is all the code we need to start training our classifier! Of course, we're not done yet — we need to write the code to classify new documents. Let's start plugging the training data into some methods we can use to classify documents.</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">feature_probability</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature</span><span class="p">,</span> <span class="n">category</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Calculate the probability that a particular feature is associated</span>
<span class="sd">    with the given category.</span>
<span class="sd">    """</span>
    <span class="n">fcc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">db</span><span class="o">.</span><span class="n">get_feature_category_count</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">category</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">fcc</span><span class="p">:</span>
        <span class="n">category_count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">db</span><span class="o">.</span><span class="n">category_count</span><span class="p">(</span><span class="n">category</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">fcc</span><span class="p">)</span> <span class="o">/</span> <span class="n">category_count</span>
    <span class="k">return</span> <span class="mi">0</span>

<span class="k">def</span> <span class="nf">weighted_probability</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature</span><span class="p">,</span> <span class="n">category</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Determine the probability a feature corresponds to the given category.</span>
<span class="sd">    The probability is weighted by the importance of the feature, which</span>
<span class="sd">    is determined by looking at the feature across all categories in</span>
<span class="sd">    which it appears.</span>
<span class="sd">    """</span>
    <span class="c1"># Calculate the "initial" probability that the given feature will</span>
    <span class="c1"># appear in the category.</span>
    <span class="n">initial_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_probability</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">category</span><span class="p">)</span>

    <span class="c1"># Sum the counts of this feature across all categories -- e.g.,</span>
    <span class="c1"># how many times overall does the word "money" appear?</span>
    <span class="n">totals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">db</span><span class="o">.</span><span class="n">get_feature_counts</span><span class="p">(</span><span class="n">feature</span><span class="p">)</span>

    <span class="c1"># Calculate the weighted average. This is slightly different than what</span>
    <span class="c1"># we did in the above example, and will help give us a more evenly weighted</span>
    <span class="c1"># result and prevents us returning 0.</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">weight</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">totals</span> <span class="o">*</span> <span class="n">initial_prob</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">weight</span> <span class="o">+</span> <span class="n">totals</span><span class="p">)</span>
</pre></div>
<p>The above <em>weighted_probability</em> function allows us to calculate the probability that a feature is associated with a given category. Now it will get more interesting as we will be calculating the probability that a <strong>set</strong> of features matches a category. To calculate this, we'll simply multiply together all the probabilities of the individual features:</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">document_probability</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">category</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Calculate the probability that a set of features match the given</span>
<span class="sd">    category.</span>
<span class="sd">    """</span>
    <span class="n">feature_probabilities</span> <span class="o">=</span> <span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weighted_probability</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">category</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">features</span><span class="p">]</span>
    <span class="k">return</span> <span class="nb">reduce</span><span class="p">(</span><span class="n">operator</span><span class="o">.</span><span class="n">mul</span><span class="p">,</span> <span class="n">feature_probabilities</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
<p>Like we did with the features in <em>weighted_probability</em>, we will also weight the document probabilities.</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">weighted_document_probability</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">category</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Calculate the probability that a set of features match the given</span>
<span class="sd">    category, and weight that score by the importance of the category.</span>
<span class="sd">    """</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">db</span><span class="o">.</span><span class="n">total_count</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># Avoid divison by zero.</span>
        <span class="k">return</span> <span class="mi">0</span>

    <span class="c1"># Calculate the probability that a document will have the given category.</span>
    <span class="c1"># In our example this is (80 / 200) for spam, (Spam docs / Total docs).</span>
    <span class="n">cat_prob</span> <span class="o">=</span> <span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">db</span><span class="o">.</span><span class="n">category_count</span><span class="p">(</span><span class="n">category</span><span class="p">))</span> <span class="o">/</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">db</span><span class="o">.</span><span class="n">total_count</span><span class="p">())</span>

    <span class="c1"># Get the probabilities of each feature for the given category.</span>
    <span class="n">doc_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">document_probability</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">category</span><span class="p">)</span>

    <span class="c1"># Weight the document probability by the category probability.</span>
    <span class="k">return</span> <span class="n">doc_prob</span> <span class="o">*</span> <span class="n">cat_prob</span>
</pre></div>
<p><strong>Finally</strong> we come to the heart of the classifier, the method that classifies a set of features. This will calculate the probability for each category (i.e., the probability for spam and ham) and then return the calculated probabilities sorted so the best match is first:</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">classify</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Classify the features by finding the categories that match the</span>
<span class="sd">    features with the highest probability.</span>
<span class="sd">    """</span>
    <span class="n">probabilities</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">category</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">db</span><span class="o">.</span><span class="n">iter_categories</span><span class="p">():</span>
        <span class="n">probabilities</span><span class="p">[</span><span class="n">category</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weighted_document_probability</span><span class="p">(</span>
            <span class="n">features</span><span class="p">,</span>
            <span class="n">category</span><span class="p">)</span>

    <span class="k">return</span> <span class="nb">sorted</span><span class="p">(</span>
        <span class="n">probabilities</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span>
        <span class="n">key</span><span class="o">=</span><span class="n">operator</span><span class="o">.</span><span class="n">itemgetter</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)[:</span><span class="n">limit</span><span class="p">]</span>
</pre></div>
<p>That's all there is to it! In the next section we will use this classifier to process data from Enron's spam corpus.</p>
<h3>Processing data from the Enron spam corpus</h3>
<p><a href="http://media.charlesleifer.com/blog/photos/p1422944038.53.jpg" title="photos/p1422944038.53.jpg"><img alt="photos/p1422944038.53.jpg" class="img-responsive" src="http://m.charlesleifer.com/t/800x-/blog/photos/p1422944038.53.jpg?key=Awx_H-gdf5e2X8q-zsdcUw"/></a></p>
<p>To follow along, you'll need to <a href="http://media.charlesleifer.com/blog/downloads/misc/corpuses.tar.gz">download the Enron spam corpora</a>. The <code>corpuses.tar.gz</code> file contains 3 different collections of spam / ham emails from Enron and will be used to train and test the classifier.</p>
<p>Let's create a new script called <em>enron.py</em> that we'll use to read the emails from the Enron corpora and train our classifier. The first function we write will read all the files in a given corpus and train the classifier.</p>
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># Import our classifier (whatever you named the file), assumed to be</span>
<span class="c1"># in same directory.</span>
<span class="kn">from</span> <span class="nn">classifier</span> <span class="kn">import</span> <span class="n">NBC</span>


<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">corpus</span><span class="o">=</span><span class="s1">'corpus'</span><span class="p">):</span>
    <span class="n">classifier</span> <span class="o">=</span> <span class="n">NBC</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s1">'enron.kct'</span><span class="p">)</span>
    <span class="n">curdir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">__file__</span><span class="p">)</span>

    <span class="c1"># Paths to spam and ham documents.</span>
    <span class="n">spam_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">curdir</span><span class="p">,</span> <span class="n">corpus</span><span class="p">,</span> <span class="s1">'spam'</span><span class="p">)</span>
    <span class="n">ham_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">curdir</span><span class="p">,</span> <span class="n">corpus</span><span class="p">,</span> <span class="s1">'ham'</span><span class="p">)</span>

    <span class="c1"># Train the classifier with the spam documents.</span>
    <span class="n">train_category</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span> <span class="n">spam_dir</span><span class="p">,</span> <span class="s1">'spam'</span><span class="p">)</span>

    <span class="c1"># Train the classifier with the ham documents.</span>
    <span class="n">train_category</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span> <span class="n">ham_dir</span><span class="p">,</span> <span class="s1">'ham'</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">classifier</span>


<span class="k">def</span> <span class="nf">train_category</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">category</span><span class="p">):</span>
    <span class="n">files</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="k">print</span> <span class="s1">'Preparing to train </span><span class="si">%s</span><span class="s1"> </span><span class="si">%s</span><span class="s1"> files'</span> <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">files</span><span class="p">),</span> <span class="n">category</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">files</span><span class="p">:</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">filename</span><span class="p">))</span> <span class="k">as</span> <span class="n">fh</span><span class="p">:</span>
            <span class="n">contents</span> <span class="o">=</span> <span class="n">fh</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

        <span class="c1"># extract the words from the document</span>
        <span class="n">features</span> <span class="o">=</span> <span class="n">extract_features</span><span class="p">(</span><span class="n">contents</span><span class="p">)</span>

        <span class="c1"># train the classifier to associate the features with the category</span>
        <span class="n">classifier</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">category</span><span class="p">)</span>
</pre></div>
<p>As you can see in the above code, we are calling a function <em>extract_features</em> to extract the words from the file contents. Our spam detector will simply use the words from the email message as the features.</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">extract_features</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">min_len</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Extract all the words in the string `s` that have a length within</span>
<span class="sd">    the specified bounds.</span>
<span class="sd">    """</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">s</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">():</span>
        <span class="n">wlen</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">wlen</span> <span class="o">&gt;</span> <span class="n">min_len</span> <span class="ow">and</span> <span class="n">wlen</span> <span class="o">&lt;</span> <span class="n">max_len</span><span class="p">:</span>
            <span class="n">words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">words</span>
</pre></div>
<p>After training the classifier, let's write a function to test it on a different corpus. The following function will classify all the spam and ham documents, recording whether the classifier guessed correctly or not.</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span> <span class="n">corpus</span><span class="o">=</span><span class="s1">'corpus2'</span><span class="p">):</span>
    <span class="n">curdir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">__file__</span><span class="p">)</span>

    <span class="c1"># Paths to spam and ham documents.</span>
    <span class="n">spam_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">curdir</span><span class="p">,</span> <span class="n">corpus</span><span class="p">,</span> <span class="s1">'spam'</span><span class="p">)</span>
    <span class="n">ham_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">curdir</span><span class="p">,</span> <span class="n">corpus</span><span class="p">,</span> <span class="s1">'ham'</span><span class="p">)</span>

    <span class="n">correct</span> <span class="o">=</span> <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">path</span><span class="p">,</span> <span class="n">category</span> <span class="ow">in</span> <span class="p">((</span><span class="n">spam_dir</span><span class="p">,</span> <span class="s1">'spam'</span><span class="p">),</span> <span class="p">(</span><span class="n">ham_dir</span><span class="p">,</span> <span class="s1">'ham'</span><span class="p">)):</span>
        <span class="n">filenames</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
        <span class="k">print</span> <span class="s1">'Preparing to test </span><span class="si">%s</span><span class="s1"> </span><span class="si">%s</span><span class="s1"> files from </span><span class="si">%s</span><span class="s1">.'</span> <span class="o">%</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">filenames</span><span class="p">),</span>
            <span class="n">category</span><span class="p">,</span>
            <span class="n">corpus</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">filename</span><span class="p">))</span> <span class="k">as</span> <span class="n">fh</span><span class="p">:</span>
                <span class="n">contents</span> <span class="o">=</span> <span class="n">fh</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

            <span class="c1"># Extract the words from the document.</span>
            <span class="n">features</span> <span class="o">=</span> <span class="n">extract_features</span><span class="p">(</span><span class="n">contents</span><span class="p">)</span>
            <span class="n">results</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">classify</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">category</span><span class="p">:</span>
                <span class="n">correct</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">pct</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">correct</span><span class="p">)</span> <span class="o">/</span> <span class="n">total</span><span class="p">)</span>
    <span class="k">print</span> <span class="s1">'[</span><span class="si">%s</span><span class="s1">]: </span><span class="si">%s</span><span class="s1"> documents, </span><span class="si">%02f%%</span><span class="s1"> accurate!'</span> <span class="o">%</span> <span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">total</span><span class="p">,</span> <span class="n">pct</span><span class="p">)</span>
</pre></div>
<p>Let's make it so that when we run our script from the command line it will train itself using <em>corpus</em> and will then test itself against the other 2 corpora:</p>
<div class="highlight"><pre><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s1">'__main__'</span><span class="p">:</span>
    <span class="n">classifier</span> <span class="o">=</span> <span class="n">train</span><span class="p">()</span>
    <span class="n">test</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span> <span class="s1">'corpus2'</span><span class="p">)</span>
    <span class="n">test</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span> <span class="s1">'corpus3'</span><span class="p">)</span>
    <span class="n">classifier</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="n">os</span><span class="o">.</span><span class="n">unlink</span><span class="p">(</span><span class="s1">'enron.kct'</span><span class="p">)</span>
</pre></div>
<p>Here is the output I get from running the script (took about 90s to run):</p>
<div class="highlight"><pre><span class="gp">$</span> python enron.py
<span class="go">Preparing to train 1500 spam files</span>
<span class="go">Trained 1500 files</span>
<span class="go">Preparing to train 3672 ham files</span>
<span class="go">Trained 3672 files</span>
<span class="go">Preparing to test 3675 spam files from corpus2.</span>
<span class="go">Preparing to test 1500 ham files from corpus2.</span>
<span class="go">[corpus2]: processed 5175 documents, 90.318841% accurate</span>
<span class="go">Preparing to test 4500 spam files from corpus3.</span>
<span class="go">Preparing to test 1500 ham files from corpus3.</span>
<span class="go">[corpus3]: processed 6000 documents, 85.533333% accurate</span>
</pre></div>
<p>90% and 85%! Not too bad.</p>
<h3>Improving Accuracy</h3>
<p>While the accuracy is significantly better than a random guess, it could definitely be improved. How can we improve the accuracy of the classifier? Reflecting on how the classifier works, <strong>the absolute most important thing to do is to ensure we are extracting high quality features</strong>.</p>
<p>Here are a couple ideas for improving the features:</p>
<ul>
<li>Filter out noise while extracting words, things like common stop words.</li>
<li>Treat the words in the email subject as distinct features. Perhaps the subject lines of spam messages have a lot in common?</li>
<li>Treat <a href="http://en.wikipedia.org/wiki/Bigram">bigrams</a> as features. Maybe spam messages use distinctive two-word combinations that aren't often seen in regular messages.</li>
<li>Check for things like words in all caps or the presence of links in the text and record these as boolean features.</li>
<li>Since the features themselves are identified by a string, you can indicate a feature is a <em>subject</em> word by prefixing it with an <em>s:</em>. Or you can add <em>meta</em>-features like <em>ALL_CAPS</em> or <em>CONTAINS_LINKS</em>.</li>
</ul>
<p>For instance, simply by filtering out stop words I was able to bump the accuracy up by 2%:</p>
<div class="highlight"><pre>$ python enron2.py
<span class="o">[</span>corpus2<span class="o">]</span>: processed <span class="m">5175</span> documents, 91.826087% accurate
<span class="o">[</span>corpus3<span class="o">]</span>: processed <span class="m">6000</span> documents, 87.350000% accurate
</pre></div>
<h3>Closing Remarks</h3>
<p>I hope you enjoyed reading this post! As you may have noticed, the classifier module is not written in such a way that it is <em>spam</em>-specific, so you can adapt it to all sorts of other uses. One example might be suggesting tags for a blog post. If you’re interested in learning more, I again would suggest picking up a copy of <a href="http://shop.oreilly.com/product/9780596529321.do">Programming Collective Intelligence</a>.</p>
<p>Additionally, the <code>ClassifierDB</code> wrapper class implements a fairly simple interface, so you could try implementing the persistence layer using <a href="http://redis.io">Redis</a> or <a href="http://sqlite.org/">SQLite</a>.</p>
<p>All the source code can be found on GitHub: <a href="https://gist.github.com/coleifer/2d66b9671420ca2856a8">https://gist.github.com/coleifer/2d66b9671420ca2856a8</a></p>
<p>You can also clone the code using git:</p>
<div class="highlight"><pre><span class="gp">$</span> git clone https://gist.github.com/coleifer/2d66b9671420ca2856a8 classifier
</pre></div>
<p>Thanks for taking the time to read this post!</p>
<h3>How will we ever classify all these kitties?</h3>
<p><a href="http://media.charlesleifer.com/blog/photos/p1423115507.82.jpg" title="photos/p1423115507.82.jpg"><img alt="photos/p1423115507.82.jpg" class="img-responsive" src="http://m.charlesleifer.com/t/800x-/blog/photos/p1423115507.82.jpg?key=k257yF0wdQMiJT37gGZegg"/></a></p>
<h3>Links</h3>
<p>Here are some blog posts on related topics:</p>

  
  

  
  
  


<hr/>

  <p>Commenting has been closed, but please feel free to <a href="/contact/">contact me</a></p>


    </div>

    </div></body></html>