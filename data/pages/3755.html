<html><body><div><div class="container-narrow">
  <a href="/">Back to main page</a>
<h1>Mapping Your Music Collection</h1>

<p>In this article we'll explore a neat way of visualizing your MP3
music collection. The end result will be a hexagonal map of all your
songs, with similar sounding tracks located next to each other. The
color of different regions corresponds to different genres of music
(e.g. classical, hip hop, hard rock). As an example, here's a map of
three albums from my music collection: Paganini's Violin Caprices,
Eminem's The Eminem Show, and Coldplay's X&amp;Y.</p>

<p><img src="res/img/smallmap.png" alt="Small Music Map"/></p>

<p>To make things more interesting (and in some cases simpler), I
imposed some constraints. First, the solution should not rely on any
pre-existing ID3 tags (e.g. Arist, Genre) in the MP3 files—only
the statistical properties of the sound should be used to calculate
the similarity of songs. A lot of my MP3 files are poorly tagged
anyways, and I wanted to keep the solution applicable to any music
collection no matter how bad its metadata. Second, no other external
information should be used to create the visualization—the only
required inputs are the user's set of MP3 files. It is possible to
improve the quality of the solution by leveraging a large database of
songs which have already been tagged with a specific genre, but for
simplicity I wanted to keep this solution completely standalone. And
lastly, although digital music comes in many formats (MP3, WMA, M4A,
OGG, etc.) to keep things simple I just focused on MP3 files. The
algorithm developed here should work fine for any other format as long
as it can be extracted into a WAV file.</p>

<p>Creating the music map is an interesting exercise. It involves
audio processing, machine learning, and visualization techniques. The
basic steps are as as follows:</p>

<ol>

<li>Convert MP3 files to low bitrate WAV files.</li>

<li>Extract statistical features from the raw WAV data.</li>

<li>Find an optimal subset of these features such that songs which are
"close" to each other in this feature space also sound similar to the
human ear.</li>

<li>Use dimension reduction techniques to map the feature vectors
down to two dimensions for plotting on an XY plane.</li>

<li>Generate a hexagonal grid of points then use nearest neighbor techniques
to map each song in the XY plane to a point on the hexagonal grid.</li>

<li>Back in the original high-dimensional feature space, cluster the
songs into a user-defined number of groups (<i>k</i>=10 works well for
visualization purposes). For each cluster, find the song closest to
the cluster center.</li>

<li>On the hexagonal grid, color the songs corresponding to the
<i>k</i> cluster centers with different colors.</li>

<li>Interpolate the colors for other songs based on their proximity in the
XY plane to each cluster center.</li>

</ol>

<p>Let's look at some of these steps in more detail.</p>

<h2>Convert MP3 files to WAV format</h2>

<p>The main advantage of converting our music into WAV format is that
we can use the wave module in Python's standard library to easily read
in the data for manipulation with NumPy. We will also downsample the
sound files to mono 10kHz to make the statistical feature extraction
less computationally intensive. To handle both the conversion and
downsampling I used the well-known MPG123. This is a freely-available
command line MP3 player which can be easily called from within Python.
The code below recursively searches through a Music folder to find all
MP3 files, then calls MPG123 to convert them to a temporary 10kHz WAV
file. The feature computation code (covered in the next section) is
then run on this WAV file.</p>

<div class="highlight"><pre><span>import</span> subprocess
<span>import</span> wave
<span>import</span> struct
<span>import</span> numpy
<span>import</span> csv
<span>import</span> sys

<span>def</span> read_wav(wav_file):
    <span>"""Returns two chunks of sound data from wave file."""</span>
    w = wave.open(wav_file)
    n = 60 * 10000
    <span>if</span> w.getnframes() &lt; n * 2:
        <span>raise</span> ValueError(<span>'Wave file too short'</span>)
    frames = w.readframes(n)
    wav_data1 = struct.unpack(<span>'%dh'</span> % n, frames)
    frames = w.readframes(n)
    wav_data2 = struct.unpack(<span>'%dh'</span> % n, frames)
    <span>return</span> wav_data1, wav_data2

<span>def</span> compute_chunk_features(mp3_file):
    <span>"""Return feature vectors for two chunks of an MP3 file."""</span>
    <span># Extract MP3 file to a mono, 10kHz WAV file</span>
    mpg123_command = <span>'..\\mpg123-1.12.3-x86-64\\mpg123.exe -w "%s" -r 10000 -m "%s"'</span>
    out_file = <span>'temp.wav'</span>
    cmd = mpg123_command % (out_file, mp3_file)
    temp = subprocess.call(cmd)
    <span># Read in chunks of data from WAV file</span>
    wav_data1, wav_data2 = read_wav(out_file)
    <span># We'll cover how the features are computed in the next section!</span>
    <span>return</span> features(wav_data1), features(wav_data2)

<span># Main script starts here</span>
<span># =======================</span>

<span>for</span> path, dirs, files <span>in</span> os.walk(<span>'C:/Users/Christian/Music/'</span>):
    <span>for</span> f <span>in</span> files:
        <span>if</span> <span>not</span> f.endswith(<span>'.mp3'</span>):
            <span># Skip any non-MP3 files</span>
            <span>continue</span>
        mp3_file = os.path.join(path, f)
        <span># Extract the track name (i.e. the file name) plus the names</span>
        <span># of the two preceding directories. This will be useful</span>
        <span># later for plotting.</span>
        tail, track = os.path.split(mp3_file)
        tail, dir1 = os.path.split(tail)
        tail, dir2 = os.path.split(tail)
        <span># Compute features. feature_vec1 and feature_vec2 are lists of floating</span>
        <span># point numbers representing the statistical features we have extracted</span>
        <span># from the raw sound data.</span>
        <span>try</span>:
            feature_vec1, feature_vec2 = compute_chunk_features(mp3_file)
        <span>except</span>:
            <span>continue</span>
 
</pre></div>


<h2>Feature Extraction</h2>

<p>A mono 10kHz wave file is represented in Python as a list of
integers ranging from -254 to 255, with 10,000 integers per second of
sound. Each integer represents the relative amplitude of the song at
that point in time. We will take two 60 second clips from each song,
so each will be represented by a list of 600,000 integers. The
read_wav function in the code above returns these lists. Here's a plot
of 10 seconds of sound from four songs on Eminem's The Eminem
Show:</p>

<p><img src="res/img/eminem.png" alt="Eminem sound plots"/></p>

<p>And for comparison, here are clips from four of Paganini's violin
caprices:</p>

<p><img src="res/img/paganini.png" alt="Paganini sound plots"/></p>

<p>There are some pretty clear differences in the structure of those
waveforms, but in general the Eminem songs all look somewhat similar
to each other, as do the violin caprices. We will now extract some
statistical features from these waveforms that will capture those
differences and let us apply machine learning techniques to group
together songs by how similar they sound to the human ear.</p>

<p>The first set of features we'll extract are statistical moments of
the waveforms (mean, standard deviation, skewness and kurtosis). In
addition to computing these on the raw amplitudes, we'll also compute
them on increasingly smoothed versions of the amplitudes to capture
properties of the music at various timescales. I used smoothing
windows of  1, 10, 100 and 1000 samples, but it is certainly possible
that other values would give good results too.</p>

<p>All the quantities above were computed on the amplitudes
themselves. To capture the short term changes in the signal, I also
computed these statistics on the first-order difference of the
(smoothed) amplitudes.</p>

<p>The features above give a pretty comprehensive statistical summary
of the waveforms in the time domain, but it is also useful to compute
some frequency domain features. Bass heavy music like hip hop will
have a lot more power in the lower end of the spectrum, whereas
classical music has a greater proportion of its energy in the higher
frequency bands.</p>

<p>Putting this all together gives us 42 different features for each
song. Here's the Python code to compute these features from a list
amplitudes:</p>

<div class="highlight"><pre><span>def</span> moments(x):
    mean = x.mean()
    std = x.var()**0.5
    skewness = ((x - mean)**3).mean() / std**3
    kurtosis = ((x - mean)**4).mean() / std**4
    <span>return</span> [mean, std, skewness, kurtosis]

<span>def</span> fftfeatures(wavdata):
    f = numpy.fft.fft(wavdata)
    f = f[2:(f.size / 2 + 1)]
    f = abs(f)
    total_power = f.sum()
    f = numpy.array_split(f, 10)
    <span>return</span> [e.sum() / total_power <span>for</span> e <span>in</span> f]

<span>def</span> features(x):
    x = numpy.array(x)
    f = []

    xs = x
    diff = xs[1:] - xs[:-1]
    f.extend(moments(xs))
    f.extend(moments(diff))

    xs = x.reshape(-1, 10).mean(1)
    diff = xs[1:] - xs[:-1]
    f.extend(moments(xs))
    f.extend(moments(diff))

    xs = x.reshape(-1, 100).mean(1)
    diff = xs[1:] - xs[:-1]
    f.extend(moments(xs))
    f.extend(moments(diff))

    xs = x.reshape(-1, 1000).mean(1)
    diff = xs[1:] - xs[:-1]
    f.extend(moments(xs))
    f.extend(moments(diff))

    f.extend(fftfeatures(x))
    <span>return</span> f

<span># f will be a list of 42 floating point features with the following</span>
<span># names:</span>

<span># amp1mean</span>
<span># amp1std</span>
<span># amp1skew</span>
<span># amp1kurt</span>
<span># amp1dmean</span>
<span># amp1dstd</span>
<span># amp1dskew</span>
<span># amp1dkurt</span>
<span># amp10mean</span>
<span># amp10std</span>
<span># amp10skew</span>
<span># amp10kurt</span>
<span># amp10dmean</span>
<span># amp10dstd</span>
<span># amp10dskew</span>
<span># amp10dkurt</span>
<span># amp100mean</span>
<span># amp100std</span>
<span># amp100skew</span>
<span># amp100kurt</span>
<span># amp100dmean</span>
<span># amp100dstd</span>
<span># amp100dskew</span>
<span># amp100dkurt</span>
<span># amp1000mean</span>
<span># amp1000std</span>
<span># amp1000skew</span>
<span># amp1000kurt</span>
<span># amp1000dmean</span>
<span># amp1000dstd</span>
<span># amp1000dskew</span>
<span># amp1000dkurt</span>
<span># power1</span>
<span># power2</span>
<span># power3</span>
<span># power4</span>
<span># power5</span>
<span># power6</span>
<span># power7</span>
<span># power8</span>
<span># power9</span>
<span># power10</span>
 
</pre></div>


<h2>Selecting an Optimal Subset of Features</h2>

<p>We've computed 42 different features but not all of them will be
useful for deciding whether two songs sound the same. The next step is
to find an optimal subset of these features which work well together
so that in this reduced feature space the Euclidean distance between
two feature vectors correlates well with how similar two songs sound
to the human ear.</p>

<p>This process of variable selection is a <a href="http://en.wikipedia.org/wiki/Supervised_learning">supervised</a>
machine learning problem so we need a set of training data that can
help guide the algorithm towards finding the best subset of variables.
Instead of manually going through my music collection and marking down
which songs sound similar to create a training set for the algorithm,
I used a much simpler approach: take two 1 minute samples from each
song and try to find an algorithm that does the best job of matching
the two samples from each song together.</p>

<p>To find the set of features that gives the best average match
across all songs I used a genetic algorithm (the
<a href="http://cran.r-project.org/web/packages/genalg/index.html">genalg</a>
package in R) to switch on and off each of the 42 variables. The plot below
shows the improvement in the objective function (i.e. how reliably a
song's two samples are matched together by the nearest neighbor
classifier) over 100 generations of the genetic algorithm.</p>

<p><img src="res/img/ga.png" alt="Genetic algorithm output"/></p>

<p>If we had forced our distance function to use all 42 features the
value of the objective function would have been 275. By judicious use
of a genetic algorithm to select variables we have reduced the
objective function (i.e. the error rate) down to 90, a significant
improvement. The optimal set of features was found to be:</p>

<ul>
<li>amp10mean</li>
<li>amp10std</li>
<li>amp10skew</li>
<li>amp10dstd</li>
<li>amp10dskew</li>
<li>amp10dkurt</li>
<li>amp100mean</li>
<li>amp100std</li>
<li>amp100dstd</li>
<li>amp1000mean</li>
<li>power2</li>
<li>power3</li>
<li>power4</li>
<li>power5</li>
<li>power6</li>
<li>power7</li>
<li>power8</li>
<li>power9</li>
</ul>

<h2>Visualize Data in Two Dimensions</h2>

<p>Our optimal set of features uses 18 variables to compare the
similarity of songs but ultimately we want to visualize our music
collection on a 2D plane, so we need to project this 18-dimensional
space down into two dimensions for plotting. To do this I simply used
the first two principal components as the x and y coordinates. This
will of course introduce some errors into the visualization so that
some songs which appear "close" to each other in the 18-dimensional
space are not as close in the 2D plane. These errors are unavoidable,
but thankfully they do not distort the relationships too
badly—similar sounding songs still cluster together into
roughly the same region of the 2D plane.</p>

<h2>Map Points to a Hexagonal Grid</h2>

<p>The 2D points generated from the principal components are
irregularly spaced on the plane. Although this irregular spacing
represents the most "accurate" placement of the 18-dimensional feature
vectors in 2D, I was willing to sacrifice some of this accuracy to map
them onto a cool looking, regularly spaced hexagonal grid. This was
accomplished by:</p>

<ol>

<li>Embedding the xy points inside a much larger hexagonal grid of
points</li>

<li>Starting with the outermost points on the hexagon, assign to each
hex grid point the nearest irregularly spaced principal component
point.</li>

<li>This stretches out the 2D points so they completely fill the
hexagonal grid and make an attractive plot.</li>

</ol>

<p><img src="res/img/hexgrid.png" alt="Mapping to hex grid"/></p>

<h2>Color the Plot</h2>

<p>One of the main goals for this exercise was to make no assumptions
about the content of the music collection. That meant I did not want
to assign pre-defined colors to certain musical genres. Instead, I
clustered the feature vectors in 18-dimensional space to find pockets
of similar-sounding music and then assigned colors to those cluster
centers. The result is an adaptive coloring algorithm which will find
as much detail as you ask of it (since the user can define the number
of clusters and hence colors). As mentioned earlier, I found that
using k=10 for the number of clusters tends to give good results.</p>

<h2>Final Output</h2>

<p>Just for fun, here is a visualization of 3,668 songs in my music
collection. The full resolution image is available <a href="res/img/musicmapfull.png">here</a>. If you zoom in you will see that
algorithm works quite well: the colored regions correspond to tracks
from the same genre and usually the same artist, as we would hope.</p>

<p><img src="res/img/musicmapsmall.png" alt="Music Map (small)"/></p>

</div>

  </div></body></html>