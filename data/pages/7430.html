<html><body><div><div class="post-text" itemprop="text">
<p>There's a simple solution using <code>gevent</code> and <code>Requests</code> <a href="https://pypi.python.org/pypi/simple-requests/1.0.0" rel="nofollow">simple-requests</a>  </p>

<p>Use <code>Requests</code> <a href="http://docs.python-requests.org/en/latest/user/advanced/#session-objects" rel="nofollow">Session</a> for HTTP persistent connection. Since <code>gevent</code> makes <code>Requests</code> asynchronous, I think there's no need for <code>timeout</code> in HTTP requests.  </p>

<p>By default, <code>requests.Session</code> caches TCP connections (<code>pool_connections</code>) for 10 hosts and limits 10 concurrent HTTP requests per cached TCP connections (<code>pool_maxsize</code>). The default configuration should be tweaked to suit the need by explicitly creating an http adapter.  </p>

<pre><code>session = requests.Session()
http_adapter = requests.adapters.HTTPAdapter(pool_connections=100, pool_maxsize=100)
session.mount('http://', http_adapter)
</code></pre>

<p>Break the tasks as producer-consumer. Image downloading is producer task and Image processing is consumer task.  </p>

<p>If the image processing library <code>PIL</code> is not asynchronous, it may block producer coroutines. If so, consumer pool can be a <code>gevent.threadpool.ThreadPool</code>. f.e.  </p>

<pre><code>from gevent.threadpool import ThreadPool
consumer = ThreadPool(POOL_SIZE)  
</code></pre>

<p>This is an overview of how it can be done. I didn't test the code.  </p>

<pre><code>from gevent import monkey; monkey.patch_all()
from time import time
import requests
from PIL import Image
from io import BytesIO
import os
from urlparse import urlparse
from gevent.pool import Pool

def download(url):
    try:
        response = session.get(url)
    except Exception as e:
        print(e)
    else:
        if response.status_code == requests.codes.ok:
            file_name = urlparse(url).path.rsplit('/',1)[-1]
            return (response.content,file_name)
        response.raise_for_status()

def process(img):
    if img is None:
        return None
    img, name = img
    img = Image.open(BytesIO(img))
    path = os.path.join(base_folder, name)
    try:
        img.save(path)
    except Exception as e:
        print(e)
    else:
        return True

def run(urls):        
    consumer.map(process, producer.imap_unordered(download, urls))

if __name__ == '__main__':
        POOL_SIZE = 300
        producer = Pool(POOL_SIZE)
        consumer = Pool(POOL_SIZE)

        session = requests.Session()
        http_adapter = requests.adapters.HTTPAdapter(pool_connections=100, pool_maxsize=100)
        session.mount('http://', http_adapter)

        test_urls = # list of 100 urls
        base_folder = 'download_temp'
        t1 = time()
        run(test_urls)
        print time() - t1  
</code></pre>
    </div>
    </div></body></html>