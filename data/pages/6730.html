<html><body><div><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-lightfm" class="anchor" href="#lightfm" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>LightFM</h1>

<p><a href="/lyst/lightfm/blob/master/lightfm.png" target="_blank"><img src="/lyst/lightfm/raw/master/lightfm.png" alt="LightFM logo"/></a></p>

<p><a href="https://circleci.com/gh/lyst/lightfm"><img src="https://camo.githubusercontent.com/9189569b17a4f7e4f8d942d36ea26522fc6a7073/68747470733a2f2f636972636c6563692e636f6d2f67682f6c7973742f6c69676874666d2e7376673f7374796c653d737667" alt="Circle CI" data-canonical-src="https://circleci.com/gh/lyst/lightfm.svg?style=svg"/></a></p>

<p>A Python implementation of LightFM, a hybrid recommendation algorithm.</p>

<p>The LightFM model incorporates both item and user metadata into the traditional matrix factorization algorithm. It represents each user and item as the sum of the latent representations of their features, thus allowing recommendations to generalise to new items (via item features) and to new users (via user features).</p>

<p>The details of the approach are described in the LightFM paper, available on <a href="http://arxiv.org/abs/1507.08439">arXiv</a>.</p>

<p>The model can be trained using four methods:</p>

<ul>
<li>logistic loss: useful when both positive (1) and negative (-1) interactions
             are present.</li>
<li>BPR: Bayesian Personalised Ranking [1] pairwise loss. Maximises the
   prediction difference between a positive example and a randomly
   chosen negative example. Useful when only positive interactions
   are present and optimising ROC AUC is desired.</li>
<li>WARP: Weighted Approximate-Rank Pairwise [2] loss. Maximises
    the rank of positive examples by repeatedly sampling negative
    examples until a rank violating one is found. Useful when only
    positive interactions are present and optimising the top of
    the recommendation list (precision@k) is desired.</li>
<li>k-OS WARP: k-th order statistic loss [3]. A modification of WARP that uses the k-th
         positive example for any given user as a basis for pairwise updates.</li>
</ul>

<p>Two learning rate schedules are implemented:</p>

<ul>
<li>adagrad: [4]</li>
<li>adadelta: [5]</li>
</ul>

<h2><a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Installation</h2>

<p>Install from pypi using pip: <code>pip install lightfm</code>. Everything should work out-of-the box on Linux and Windows using MSVC.</p>

<p>Note for OSX users: due to its use of OpenMP, <code>lightfm</code> does not compile under Clang. To install it, you will need a reasonably recent version of <code>gcc</code> (from Homebrew for instance). This should be picked up by <code>setup.py</code>; if it is not, please open an issue.</p>

<p>Building with the default Python distribution included in OSX is also not supported; please try the version from Homebrew or Anaconda.</p>

<h2><a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Usage</h2>

<p>Model fitting is very straightforward.</p>

<p>Create a model instance with the desired latent dimensionality</p>

<div class="highlight highlight-source-python"><pre><span class="pl-k">from</span> lightfm <span class="pl-k">import</span> LightFM

model <span class="pl-k">=</span> LightFM(<span class="pl-v">no_components</span><span class="pl-k">=</span><span class="pl-c1">30</span>)</pre></div>

<p>Assuming <code>train</code> is a (no_users, no_items) sparse matrix (with 1s denoting positive, and -1s negative interactions), you can fit a traditional matrix factorization model by calling</p>

<div class="highlight highlight-source-python"><pre>model.fit(train, <span class="pl-v">epochs</span><span class="pl-k">=</span><span class="pl-c1">20</span>)</pre></div>

<p>This will train a traditional MF model, as no user or item features have been supplied.</p>

<p>To get predictions, call <code>model.predict</code>:</p>

<div class="highlight highlight-source-python"><pre>predictions <span class="pl-k">=</span> model.predict(test_user_ids, test_item_ids)</pre></div>

<p>User and item features can be incorporated into training by passing them into the <code>fit</code> method. Assuming <code>user_features</code> is a (no_users, no_user_features) sparse matrix (and similarly for <code>item_features</code>), you can call</p>

<div class="highlight highlight-source-python"><pre>model.fit(train,
          <span class="pl-v">user_features</span><span class="pl-k">=</span>user_features,
          <span class="pl-v">item_features</span><span class="pl-k">=</span>item_features,
          <span class="pl-v">epochs</span><span class="pl-k">=</span><span class="pl-c1">20</span>)
predictions <span class="pl-k">=</span> model.predict(test_user_ids,
                            test_item_ids,
                            <span class="pl-v">user_features</span><span class="pl-k">=</span>user_features,
                            <span class="pl-v">item_features</span><span class="pl-k">=</span>item_features)</pre></div>

<p>to train the model and obtain predictions.</p>

<p>Both training and prediction can employ multiple cores for speed:</p>

<div class="highlight highlight-source-python"><pre>model.fit(train, <span class="pl-v">epochs</span><span class="pl-k">=</span><span class="pl-c1">20</span>, <span class="pl-v">num_threads</span><span class="pl-k">=</span><span class="pl-c1">4</span>)
predictions <span class="pl-k">=</span> model.predict(test_user_ids, test_item_ids, <span class="pl-v">num_threads</span><span class="pl-k">=</span><span class="pl-c1">4</span>)</pre></div>

<p>This implementation uses asynchronous stochastic gradient descent [6] for training. This can lead to lower accuracy when the interaction matrix (or the feature matrices) are very dense and a large number of threads is used. In practice, however, training on a sparse dataset with 20 threads does not lead to a measurable loss of accuracy.</p>

<p>In an implicit feedback setting, the BPR, WARP, or k-OS WARP loss functions can be used. If <code>train</code> is a sparse matrix with positive entries representing positive interactions, the model can be trained as follows:</p>

<div class="highlight highlight-source-python"><pre>model <span class="pl-k">=</span> LightFM(<span class="pl-v">no_components</span><span class="pl-k">=</span><span class="pl-c1">30</span>, <span class="pl-v">loss</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>warp<span class="pl-pds">'</span></span>)
model.fit(train, <span class="pl-v">epochs</span><span class="pl-k">=</span><span class="pl-c1">20</span>)</pre></div>

<h2><a id="user-content-examples" class="anchor" href="#examples" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Examples</h2>

<p>Check the <code>examples</code> directory for more examples.</p>

<p>The <a href="/lyst/lightfm/blob/master/examples/movielens/example.ipynb">Movielens example</a> shows how to use <code>lightfm</code> on the Movielens dataset, both with and without using movie metadata. <a href="/lyst/lightfm/blob/master/examples/movielens/learning_schedules.ipynb">Another example</a> compares the performance of the adagrad and adadelta learning schedules.</p>

<p>The <a href="/lyst/lightfm/blob/master/examples/crossvalidated/example.ipynb">Cross Validated example</a> shows how to use <code>lightfm</code> on a dataset from <a href="http://stats.stackexchange.com">stats.stackexchange.com</a> with both item and user features.</p>

<p>The <a href="https://github.com/tdeboissiere/Kaggle/blob/master/Ponpare/ponpare_lightfm.ipynb">Kaggle coupon purchase prediction</a> example applies LightFM to predicting coupon purchases.</p>

<h2><a id="user-content-development" class="anchor" href="#development" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Development</h2>

<p>Pull requests are welcome. To install for development:</p>

<ol>
<li>Clone the repository: <code>git clone git@github.com:lyst/lightfm.git</code></li>
<li>Install it for development using pip: <code>cd lightfm &amp;&amp; pip install -e .</code></li>
<li>You can run tests by running <code>python setupy.py test</code>.</li>
</ol>

<p>When making changes to the <code>.pyx</code> extension files, you'll need to run <code>python setup.py cythonize</code> in order to produce the extension <code>.c</code> files before running <code>pip install -e .</code>.</p>

<h2><a id="user-content-references" class="anchor" href="#references" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>References</h2>

<p>[1] Rendle, Steffen, et al. "BPR: Bayesian personalized ranking from implicit feedback."
Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial
Intelligence. AUAI Press, 2009.</p>

<p>[2] Weston, Jason, Samy Bengio, and Nicolas Usunier. "Wsabie: Scaling up to large
vocabulary image annotation." IJCAI. Vol. 11. 2011.</p>

<p>[3] Weston, Jason, Hector Yee, and Ron J. Weiss. "Learning to rank recommendations with
the k-order statistic loss." Proceedings of the 7th ACM conference on Recommender systems. ACM, 2013.</p>

<p>[4] Duchi, John, Elad Hazan, and Yoram Singer. "Adaptive subgradient methods
for online learning and stochastic optimization." The Journal of Machine Learning Research 12 (2011): 2121-2159.</p>

<p>[5] Zeiler, Matthew D. "ADADELTA: An adaptive learning rate method."
arXiv preprint arXiv:1212.5701 (2012).</p>

<p>[6] Recht, Benjamin, et al. "Hogwild: A lock-free approach to parallelizing stochastic gradient descent." Advances in Neural Information Processing Systems. 2011.</p>
</article>
  </div></body></html>