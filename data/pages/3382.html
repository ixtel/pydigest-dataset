<html><body><div><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-reinforce" class="anchor" href="#reinforce" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>reinforce</h1>

<p>A 'plug and play' reinforcement learning library in Python.</p>

<p>Infers a Markov Decision Process from data and solves for the optimal policy.</p>

<p>Implementation based on Andrew Ng's <a href="http://cs229.stanford.edu/notes/cs229-notes12.pdf">notes.</a></p>

<h2><a id="user-content-motivation" class="anchor" href="#motivation" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Motivation</h2>

<p><a href="https://github.com/scikit-learn/scikit-learn">scikit-learn</a> provides excellent tools for supervised and unsupervised learning but explicitly does not deal with reinforcement learning.</p>

<p>reinforce is intended to compliment the functionality of scikit-learn and together form a more complete machine learning toolkit.</p>

<h2><a id="user-content-install" class="anchor" href="#install" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Install</h2>

<p><code>pip install reinforce</code></p>

<h2><a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Usage</h2>

<div class="highlight highlight-source-python"><pre><span class="pl-k">import</span> learn <span class="pl-k">as</span> l

l.learn(obs)
<span class="pl-c"># or</span>
l.learn(obs,gamma)
<span class="pl-c"># or</span>
l.learn(obs,gamma,<span class="pl-c1">R</span>)
</pre></div>

<h3><a id="user-content-output" class="anchor" href="#output" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Output</h3>

<div class="highlight highlight-source-python"><pre><span class="pl-k">import</span> learn <span class="pl-k">as</span> l

model <span class="pl-k">=</span> l.learn(obs,gamma,<span class="pl-c1">R</span>)</pre></div>

<p><code>model</code> is a dictionary which contains the estimated optimal action for each state.</p>

<h3><a id="user-content-inputs" class="anchor" href="#inputs" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Inputs</h3>

<h4><a id="user-content-obs" class="anchor" href="#obs" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>obs</h4>

<p>obs is a 3-dimensional list. Each element of obs is a 2-d list of time-steps. Each time-step is a list of the form [state, action, reward] if no R is specified, or [state,action] if R is specified. See examples for more detail.</p>

<div class="highlight highlight-source-python"><pre>obsA <span class="pl-k">=</span> [[stateA1,actionA1,rewardA1],[stateA2,actionA2,rewardA2],<span class="pl-c1">...</span>]
obsB <span class="pl-k">=</span> [[stateB1,actionB1,rewardB1],[stateB2,actionB2,rewardB2],<span class="pl-c1">...</span>]

obs <span class="pl-k">=</span> [obsA,obsB]</pre></div>

<h4><a id="user-content-gamma" class="anchor" href="#gamma" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>gamma</h4>

<p>A value specifying the discount factor for future rewards. In the range (0,1]</p>



<h4><a id="user-content-r" class="anchor" href="#r" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>R</h4>

<p>If rewards are ommitted in obs, R is a list of length = len(obs) specifying the reward for each observation. See examples for more detail.</p>

<div class="highlight highlight-source-python"><pre>obsA <span class="pl-k">=</span> [[stateA1,actionA1,rewardA1],[stateA2,actionA2,rewardA2],<span class="pl-c1">...</span>]
obsB <span class="pl-k">=</span> [[stateB1,actionB1,rewardB1],[stateB2,actionB2,rewardB2],<span class="pl-c1">...</span>]

obs <span class="pl-k">=</span> [obsA,obsB]
<span class="pl-c1">R</span> <span class="pl-k">=</span> [rewardA,rewardB]</pre></div>

<h2><a id="user-content-examples" class="anchor" href="#examples" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Examples</h2>

<p><a href="/NathanEpstein/reinforce/blob/master/example.png" target="_blank"><img src="/NathanEpstein/reinforce/raw/master/example.png"/></a></p>

<h3><a id="user-content-example1" class="anchor" href="#example1" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Example1</h3>

<div class="highlight highlight-source-python"><pre><span class="pl-k">import</span> learn <span class="pl-k">as</span> l

<span class="pl-k">def</span> <span class="pl-en">main</span>():
  obs1 <span class="pl-k">=</span> [[<span class="pl-s"><span class="pl-pds">"</span>A<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>F<span class="pl-pds">"</span></span>,<span class="pl-c1">0</span>],[<span class="pl-s"><span class="pl-pds">"</span>A<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>L<span class="pl-pds">"</span></span>,<span class="pl-c1">0</span>],[<span class="pl-s"><span class="pl-pds">"</span>Prize<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>F<span class="pl-pds">"</span></span>,<span class="pl-c1">1</span>]]
  obs2 <span class="pl-k">=</span> [[<span class="pl-s"><span class="pl-pds">"</span>C<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>R<span class="pl-pds">"</span></span>,<span class="pl-c1">0</span>],[<span class="pl-s"><span class="pl-pds">"</span>D<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>F<span class="pl-pds">"</span></span>,<span class="pl-c1">0</span>],[<span class="pl-s"><span class="pl-pds">"</span>B<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>B<span class="pl-pds">"</span></span>,<span class="pl-c1">0</span>],[<span class="pl-s"><span class="pl-pds">"</span>D<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>L<span class="pl-pds">"</span></span>,<span class="pl-c1">0</span>]]
  obs3 <span class="pl-k">=</span> [[<span class="pl-s"><span class="pl-pds">"</span>C<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>F<span class="pl-pds">"</span></span>,<span class="pl-c1">0</span>],[<span class="pl-s"><span class="pl-pds">"</span>A<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>R<span class="pl-pds">"</span></span>,<span class="pl-c1">0</span>],[<span class="pl-s"><span class="pl-pds">"</span>B<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>L<span class="pl-pds">"</span></span>,<span class="pl-c1">0</span>],[<span class="pl-s"><span class="pl-pds">"</span>A<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>L<span class="pl-pds">"</span></span>,<span class="pl-c1">0</span>],[<span class="pl-s"><span class="pl-pds">"</span>Prize<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>L<span class="pl-pds">"</span></span>,<span class="pl-c1">1</span>]]

  obs <span class="pl-k">=</span> [obs1,obs2,obs3]
  gamma <span class="pl-k">=</span> <span class="pl-c1">0.95</span> <span class="pl-c">#slight discount to rewards farther in the future</span>

  model <span class="pl-k">=</span> l.learn(obs,gamma)
  <span class="pl-c"># or try it without gamma</span>
  <span class="pl-c"># model = l.learn(obs)</span>

  <span class="pl-c1">print</span> (<span class="pl-s"><span class="pl-pds">"</span>From these three paths, the learned strategy is: <span class="pl-pds">"</span></span>)
  <span class="pl-c1">print</span> (model)

  <span class="pl-c">#note that many transition probabilities are estimated as uniform because there isn't yet data</span>
main()

From these three paths, the learned strategy <span class="pl-k">is</span>:
<span class="pl-c"># {'A': 'L', 'C': 'F', 'B': 'L', 'Prize': 'F', 'D': 'L'}</span></pre></div>

<h3><a id="user-content-example2" class="anchor" href="#example2" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Example2</h3>

<div class="highlight highlight-source-python"><pre><span class="pl-k">import</span> learn <span class="pl-k">as</span> l

<span class="pl-k">def</span> <span class="pl-en">main</span>():
  obs1 <span class="pl-k">=</span> [[<span class="pl-s"><span class="pl-pds">"</span>A<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>F<span class="pl-pds">"</span></span>],[<span class="pl-s"><span class="pl-pds">"</span>A<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>L<span class="pl-pds">"</span></span>],[<span class="pl-s"><span class="pl-pds">"</span>Prize<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>F<span class="pl-pds">"</span></span>]]
  obs2 <span class="pl-k">=</span> [[<span class="pl-s"><span class="pl-pds">"</span>C<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>R<span class="pl-pds">"</span></span>],[<span class="pl-s"><span class="pl-pds">"</span>D<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>F<span class="pl-pds">"</span></span>],[<span class="pl-s"><span class="pl-pds">"</span>B<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>B<span class="pl-pds">"</span></span>],[<span class="pl-s"><span class="pl-pds">"</span>D<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>L<span class="pl-pds">"</span></span>]]
  obs3 <span class="pl-k">=</span> [[<span class="pl-s"><span class="pl-pds">"</span>C<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>F<span class="pl-pds">"</span></span>],[<span class="pl-s"><span class="pl-pds">"</span>A<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>R<span class="pl-pds">"</span></span>],[<span class="pl-s"><span class="pl-pds">"</span>B<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>L<span class="pl-pds">"</span></span>],[<span class="pl-s"><span class="pl-pds">"</span>A<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>L<span class="pl-pds">"</span></span>],[<span class="pl-s"><span class="pl-pds">"</span>Prize<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>L<span class="pl-pds">"</span></span>]]

  obs <span class="pl-k">=</span> [obs1,obs2,obs3]
  gamma <span class="pl-k">=</span> <span class="pl-c1">1</span> <span class="pl-c">#no discount</span>
  rewards <span class="pl-k">=</span> [<span class="pl-c1">1</span>,<span class="pl-c1">0</span>,<span class="pl-c1">1</span>]

  model <span class="pl-k">=</span> l.learn(obs,gamma,rewards)

  <span class="pl-c1">print</span> (<span class="pl-s"><span class="pl-pds">"</span>From these three paths, the learned strategy is: <span class="pl-pds">"</span></span>)
  <span class="pl-c1">print</span> (model)

  <span class="pl-c">#note that many transition probabilities are estimated as uniform because there isn't yet data</span>
main()

<span class="pl-c"># From these three paths, the learned strategy is:</span>
<span class="pl-c"># {'A': 'R', 'C': 'F', 'B': 'L', 'Prize': 'F', 'D': 'L'}</span></pre></div>

<h3><a id="user-content-practical-note" class="anchor" href="#practical-note" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Practical Note</h3>

<p>It is worth mentioning that the algorithm will learn (and the strategy will improve) <strong>much faster if rewards for each step can be included</strong> (as opposed to a reward for each observation). This is only for the rare case in which the user can choose between the two types of data - in practice it is more likely that only one reward per observation will be available.</p>

<p>This can be seen in the above examples - the model in example 1 is more effective than that in example 2 (clearly, going left in state A is preferable to going right in state A).</p>
</article>
  </div></body></html>