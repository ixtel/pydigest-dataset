<html><body><div><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-hyperas" class="anchor" href="#hyperas" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Hyperas</h1>

<p>A very simple convenience wrapper around hyperopt for fast prototyping with keras models. Hyperas lets you use the power of hyperopt without having to learn the syntax of it. Instead, just define your keras model as you are used to, but use a simple template notation to define hyper-parameter ranges to tune.</p>

<h2><a id="user-content-installation" class="anchor" href="#installation" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Installation</h2>



<h2><a id="user-content-quick-start" class="anchor" href="#quick-start" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Quick start</h2>

<p>Assume you have an existing keras model like the following.</p>

<div class="highlight highlight-source-python"><pre>model <span class="pl-k">=</span> Sequential()
model.add(Dense(<span class="pl-c1">512</span>, <span class="pl-v">input_shape</span><span class="pl-k">=</span>(<span class="pl-c1">784</span>,)))
model.add(Activation(<span class="pl-s"><span class="pl-pds">'</span>relu<span class="pl-pds">'</span></span>))
model.add(Dropout(<span class="pl-c1">0.2</span>))
model.add(Dense(<span class="pl-c1">512</span>))
model.add(Activation(<span class="pl-s"><span class="pl-pds">'</span>relu<span class="pl-pds">'</span></span>))
model.add(Dropout(<span class="pl-c1">0.2</span>)
model.add(Dense(<span class="pl-c1">10</span>))
model.add(Activation(<span class="pl-s"><span class="pl-pds">'</span>softmax<span class="pl-pds">'</span></span>))</pre></div>

<p>To do hyper-parameter optimization on this model, just wrap the parameters you want to optimize into double curly brackets and choose a distribution over which to run the algorithm. In the above example, let's say we want to optimize for the best dropout probability in both dropout layers. Choosing a uniform distribution over the interval <code>[0,1]</code>, this translates into the following definition.</p>

<div class="highlight highlight-source-python"><pre><span class="pl-k">from</span> hyperas.distributions <span class="pl-k">import</span> uniform

model <span class="pl-k">=</span> Sequential()
model.add(Dense(<span class="pl-c1">512</span>, <span class="pl-v">input_shape</span><span class="pl-k">=</span>(<span class="pl-c1">784</span>,)))
model.add(Activation(<span class="pl-s"><span class="pl-pds">'</span>relu<span class="pl-pds">'</span></span>))
model.add(Dropout({{uniform(<span class="pl-c1">0</span>, <span class="pl-c1">1</span>)}}))
model.add(Dense(<span class="pl-c1">512</span>))
model.add(Activation(<span class="pl-s"><span class="pl-pds">'</span>relu<span class="pl-pds">'</span></span>))
model.add(Dropout({{uniform(<span class="pl-c1">0</span>, <span class="pl-c1">1</span>)}}))
model.add(Dense(<span class="pl-c1">10</span>))
model.add(Activation(<span class="pl-s"><span class="pl-pds">'</span>softmax<span class="pl-pds">'</span></span>))</pre></div>

<p>After having trained the model, to optimize, we also have to define which evaluation metric of the model is important to us. For example, if we wish to optimize for accuracy, the following example does the trick:</p>

<div class="highlight highlight-source-python"><pre>score <span class="pl-k">=</span> model.evaluate(<span class="pl-c1">X_test</span>, <span class="pl-c1">Y_test</span>, <span class="pl-v">show_accuracy</span><span class="pl-k">=</span><span class="pl-c1">True</span>, <span class="pl-v">verbose</span><span class="pl-k">=</span><span class="pl-c1">0</span>)
accuracy <span class="pl-k">=</span> score[<span class="pl-c1">1</span>]
<span class="pl-k">return</span> {<span class="pl-s"><span class="pl-pds">'</span>loss<span class="pl-pds">'</span></span>: <span class="pl-k">-</span>accuracy, <span class="pl-s"><span class="pl-pds">'</span>status<span class="pl-pds">'</span></span>: <span class="pl-c1">STATUS_OK</span>}</pre></div>

<p>The last step is to actually run the optimization, which is done as follows:</p>

<div class="highlight highlight-source-python"><pre>best_run <span class="pl-k">=</span> optim.minimize(<span class="pl-v">model</span><span class="pl-k">=</span>model,
                          <span class="pl-v">data</span><span class="pl-k">=</span>data,
                          <span class="pl-v">algo</span><span class="pl-k">=</span>tpe.suggest,
                          <span class="pl-v">max_evals</span><span class="pl-k">=</span><span class="pl-c1">10</span>,
                          <span class="pl-v">trials</span><span class="pl-k">=</span>Trials())</pre></div>

<p>In this example we use at most 10 evaluation runs and the TPE algorithm from hyperopt for optimization.</p>

<h2><a id="user-content-complete-example" class="anchor" href="#complete-example" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Complete example</h2>

<p>An extended version of the above example in one script reads as follows:</p>

<p><strong>Note:</strong> It is important to wrap your data and model into functions, including necessary imports, as shown below, and then pass them as parameters to the minimizer. <code>data()</code> returns the data the <code>model()</code> needs. Internally, this is a cheap, but necessary trick to avoid loading data on each optimization run.</p>

<div class="highlight highlight-source-python"><pre><span class="pl-k">from</span> __future__ <span class="pl-k">import</span> print_function
<span class="pl-k">from</span> hyperopt <span class="pl-k">import</span> Trials, <span class="pl-c1">STATUS_OK</span>, tpe
<span class="pl-k">from</span> hyperas <span class="pl-k">import</span> optim
<span class="pl-k">from</span> hyperas.distributions <span class="pl-k">import</span> choice, uniform

<span class="pl-k">def</span> <span class="pl-en">data</span>():
    <span class="pl-k">from</span> keras.datasets <span class="pl-k">import</span> mnist
    <span class="pl-k">from</span> keras.utils <span class="pl-k">import</span> np_utils
    (<span class="pl-c1">X_train</span>, y_train), (<span class="pl-c1">X_test</span>, y_test) <span class="pl-k">=</span> mnist.load_data()
    <span class="pl-c1">X_train</span> <span class="pl-k">=</span> <span class="pl-c1">X_train</span>.reshape(<span class="pl-c1">60000</span>, <span class="pl-c1">784</span>)
    <span class="pl-c1">X_test</span> <span class="pl-k">=</span> <span class="pl-c1">X_test</span>.reshape(<span class="pl-c1">10000</span>, <span class="pl-c1">784</span>)
    <span class="pl-c1">X_train</span> <span class="pl-k">=</span> <span class="pl-c1">X_train</span>.astype(<span class="pl-s"><span class="pl-pds">'</span>float32<span class="pl-pds">'</span></span>)
    <span class="pl-c1">X_test</span> <span class="pl-k">=</span> <span class="pl-c1">X_test</span>.astype(<span class="pl-s"><span class="pl-pds">'</span>float32<span class="pl-pds">'</span></span>)
    <span class="pl-c1">X_train</span> <span class="pl-k">/=</span> <span class="pl-c1">255</span>
    <span class="pl-c1">X_test</span> <span class="pl-k">/=</span> <span class="pl-c1">255</span>
    nb_classes <span class="pl-k">=</span> <span class="pl-c1">10</span>
    <span class="pl-c1">Y_train</span> <span class="pl-k">=</span> np_utils.to_categorical(y_train, nb_classes)
    <span class="pl-c1">Y_test</span> <span class="pl-k">=</span> np_utils.to_categorical(y_test, nb_classes)
    <span class="pl-k">return</span> <span class="pl-c1">X_train</span>, <span class="pl-c1">Y_train</span>, <span class="pl-c1">X_test</span>, <span class="pl-c1">Y_test</span>


<span class="pl-k">def</span> <span class="pl-en">model</span>(<span class="pl-smi">X_train</span>, <span class="pl-smi">Y_train</span>, <span class="pl-smi">X_test</span>, <span class="pl-smi">Y_test</span>):
    <span class="pl-k">from</span> keras.models <span class="pl-k">import</span> Sequential
    <span class="pl-k">from</span> keras.layers.core <span class="pl-k">import</span> Dense, Dropout, Activation
    <span class="pl-k">from</span> keras.optimizers <span class="pl-k">import</span> RMSprop

    model <span class="pl-k">=</span> Sequential()
    model.add(Dense(<span class="pl-c1">512</span>, <span class="pl-v">input_shape</span><span class="pl-k">=</span>(<span class="pl-c1">784</span>,)))
    model.add(Activation(<span class="pl-s"><span class="pl-pds">'</span>relu<span class="pl-pds">'</span></span>))
    model.add(Dropout({{uniform(<span class="pl-c1">0</span>, <span class="pl-c1">1</span>)}}))
    model.add(Dense({{choice([<span class="pl-c1">256</span>, <span class="pl-c1">512</span>, <span class="pl-c1">1024</span>])}}))
    model.add(Activation(<span class="pl-s"><span class="pl-pds">'</span>relu<span class="pl-pds">'</span></span>))
    model.add(Dropout({{uniform(<span class="pl-c1">0</span>, <span class="pl-c1">1</span>)}}))
    model.add(Dense(<span class="pl-c1">10</span>))
    model.add(Activation(<span class="pl-s"><span class="pl-pds">'</span>softmax<span class="pl-pds">'</span></span>))

    rms <span class="pl-k">=</span> RMSprop()
    model.compile(<span class="pl-v">loss</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>categorical_crossentropy<span class="pl-pds">'</span></span>, <span class="pl-v">optimizer</span><span class="pl-k">=</span>rms)

    model.fit(<span class="pl-c1">X_train</span>, <span class="pl-c1">Y_train</span>,
              <span class="pl-v">batch_size</span><span class="pl-k">=</span>{{choice([<span class="pl-c1">64</span>, <span class="pl-c1">128</span>])}},
              <span class="pl-v">nb_epoch</span><span class="pl-k">=</span><span class="pl-c1">1</span>,
              <span class="pl-v">show_accuracy</span><span class="pl-k">=</span><span class="pl-c1">True</span>,
              <span class="pl-v">verbose</span><span class="pl-k">=</span><span class="pl-c1">2</span>,
              <span class="pl-v">validation_data</span><span class="pl-k">=</span>(<span class="pl-c1">X_test</span>, <span class="pl-c1">Y_test</span>))
    score <span class="pl-k">=</span> model.evaluate(<span class="pl-c1">X_test</span>, <span class="pl-c1">Y_test</span>,
                           <span class="pl-v">show_accuracy</span><span class="pl-k">=</span><span class="pl-c1">True</span>, <span class="pl-v">verbose</span><span class="pl-k">=</span><span class="pl-c1">0</span>)
    <span class="pl-c1">print</span>(<span class="pl-s"><span class="pl-pds">'</span>Test accuracy:<span class="pl-pds">'</span></span>, score[<span class="pl-c1">1</span>])
    <span class="pl-k">return</span> {<span class="pl-s"><span class="pl-pds">'</span>loss<span class="pl-pds">'</span></span>: <span class="pl-k">-</span>score[<span class="pl-c1">1</span>], <span class="pl-s"><span class="pl-pds">'</span>status<span class="pl-pds">'</span></span>: <span class="pl-c1">STATUS_OK</span>}

<span class="pl-k">if</span> <span class="pl-c1">__name__</span> <span class="pl-k">==</span> <span class="pl-s"><span class="pl-pds">'</span>__main__<span class="pl-pds">'</span></span>:
    best_run <span class="pl-k">=</span> optim.minimize(<span class="pl-v">model</span><span class="pl-k">=</span>model, <span class="pl-v">data</span><span class="pl-k">=</span>data,
                              <span class="pl-v">algo</span><span class="pl-k">=</span>tpe.suggest, <span class="pl-v">max_evals</span><span class="pl-k">=</span><span class="pl-c1">10</span>, <span class="pl-v">trials</span><span class="pl-k">=</span>Trials())
    <span class="pl-c1">print</span>(best_run)</pre></div>
</article>
  </div></body></html>