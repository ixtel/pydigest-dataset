<html><body><div><article class="markdown-body entry-content" itemprop="text"><h2><a id="user-content-you-can-try-web-demo-here-" class="anchor" href="#you-can-try-web-demo-here-" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>You can try web demo <a href="http://mattya.github.io/chainer-DCGAN/">here</a> !!</h2>

<h1><a id="user-content-chainer-dcgan" class="anchor" href="#chainer-dcgan" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>chainer-DCGAN</h1>

<p>Chainer implementation of Deep Convolutional Generative Adversarial Network (<a href="http://arxiv.org/abs/1511.06434">http://arxiv.org/abs/1511.06434</a>)</p>

<h2><a id="user-content-説明" class="anchor" href="#説明" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>説明</h2>

<p>画像を生成するニューラルネットです。<br/>
12/24のchainer advent calendarに解説を書きました。 <a href="http://qiita.com/mattya/items/e5bfe5e04b9d2f0bbd47">http://qiita.com/mattya/items/e5bfe5e04b9d2f0bbd47</a> <br/>
このコードは現在試行錯誤の途中であり、突然の変更などの可能性が十分あります。ご了承ください。</p>

<h2><a id="user-content-使い方暫定" class="anchor" href="#使い方暫定" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>使い方(暫定)</h2>

<ul>
<li>chainer 1.5が必要</li>
<li>学習済みモデルから生成のみを行うには、visualizer.pyを使用する。GPU無くてもOK。
<code>python visualizer.py</code></li>
<li>学習を行うにはDCGAN.pyを実行する。image_dir変数で指定されたディレクトリに、学習元となる画像ファイルを置く。GPUが必要で、何時間かかかる。</li>
</ul>

<h2><a id="user-content-サンプル" class="anchor" href="#サンプル" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>サンプル</h2>

<p>20万枚の顔イラスト画像で約3時間学習を行った結果(GTX 970使用)。
<a href="https://raw.githubusercontent.com/mattya/chainer-DCGAN/master/sample4.png" target="_blank"><img src="https://raw.githubusercontent.com/mattya/chainer-DCGAN/master/sample4.png"/></a></p>

<p>特定の画像の生成元となったベクトルzにノイズを加えると、髪型や服装などが少しずつ異なる画像を生成できる。
このことから、本モデルが過学習しているわけではない(特定の画像を暗記しているわけではない)ことが示唆される。
<a href="https://raw.githubusercontent.com/mattya/chainer-DCGAN/master/sample2.png" target="_blank"><img src="https://raw.githubusercontent.com/mattya/chainer-DCGAN/master/sample2.png"/></a></p>

<p>画像間の連続的変換。
<a href="https://raw.githubusercontent.com/mattya/chainer-DCGAN/master/sample3.png" target="_blank"><img src="https://raw.githubusercontent.com/mattya/chainer-DCGAN/master/sample3.png"/></a></p>

<h2><a id="user-content-参考文献" class="anchor" href="#参考文献" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>参考文献</h2>

<p>本家の実装です。モデルの相違点はleaky_reluの代わりにeluを使っているくらいです。 <a href="https://github.com/soumith/dcgan.torch">https://github.com/soumith/dcgan.torch</a></p>
</article>
  </div></body></html>