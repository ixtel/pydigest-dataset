<html><body><div><div class="content html_format">
      <blockquote>Когда видите единственное решение – спросите других </blockquote>

<p>
В данной статье я хотел бы рассказать о некоторых предпосылках появления </p><a href="https://sourceforge.net/p/meta-net-project/code/HEAD/tree/">инструмента</a><p> для моделирования метасетей.

</p><h2>Автоматизация обучения</h2><p>
Изначально возникла проблема автоматизации обучения искусственных нейронных сетей с определёнными временными ограничениями. На пути ее решения был предложен подход к использованию оппозитных нейронных сетей [1]. Суть в том, что бы обучать две сети, одну как обычно:
</p><a name="habracut"/>

<p>
А вторую на обратных эталонных выходах:

</p>
<p>
где </p><img src="https://habrastorage.org/files/0cf/a85/c46/0cfa85c468b94d18bf37ddc938a33026.png"/><p> – эталонное множество выходов, y — выход сети, ε — значение отклонения (погрешность), N количество нейронов в заданном слое, L — номер выходного слоя и n — момент времени. Тогда для определения насыщения одной сети или попадании в локальный минимум – мы могли бы сравнивать ее с оппозитной. И в случае существенного различия не тратить время на очередное обучение. Но встает новый вопрос – как производить подобный анализ? Тогда мы решили определять достоверность ответов сети по значению дисперсии ответов. Целесообразность подобного решения была обусловлена эмпирическим наблюдением – при насыщении или попадании в локальный минимум – значения выхода отличаются небольшим разбросом. Построив модели, мы нашли, что в ряде случаев данный подход оправдывает себя. Но тут же появился другой вопрос – а как определить, около каких векторов входов ошибается сеть? Или – как должна изменяться степень доверия ответам сети, например, в задачах классификации, при приближении к конкретным классам распознаваемых объектов? 

</p><h2>Степень доверия ответам сети</h2><p>
Интуитивно мы просто решили доверить эту задачу другой НС, построив ее таким образом, что входами ее являются состояния всех нейронов наблюдаемой сети. 
</p><p>
Тогда обучив НС, мы можем сформировать банк ее ответов на тестовой выборке. Зная ошибки и матрицу состояний сети – обучить метасеть с тем, что бы она классифицировала ошибки во время работы базовой сети. 

</p>
<p>
В итоге мы получили следующий график:

</p>
<p>
Представленные графики дают основание полагать, что с помощью метасети можно строить аналог доверительных интервалов. MetaOut на рисунке 2 представляет собой вывод метасети. Видно, что чем ближе пример выборки к тому, на котором базовая сеть ошибается – тем выше метасеть оценивает ошибку. Err на графике представляет собой модуль разницы между целевым выходом и тем что дает базовая сеть. Стоит отметить, что данные предоставлялись сети последовательно. Таким образом, график можно разбить на 3 области (по подклассам набора Iris) [2]. 

</p><h2>Метасети</h2><p>
Но по настоящему мое свободное время занял вопрос познания в НС [3]. Обращаясь к вопросу познания, нельзя не обратиться к И. Канту. Исследуя природу познания, Кант определил его как суждение. Судя о предмете синтетически, мы присоединяем к нему признак, или черту, прямо в нем не заключающуюся. Если же мы судим о предмете аналитически, мы ограничиваемся его понятием и не присоединяем к нему ничего, что не заключалось бы в нем. Таким образом, можно определить познание как синтетическое суждение, так как познанием мы называем только то, что расширяет наше знание о предмете. Далее синтетическое мышление можно разделить на познание заимствованное из опыта (a posteriori) и независимое (a priori). Безусловно, возможность априорного синтетического познания подробно рассмотрена Кантом с точки зрения философии, мы взяли на себя смелость рассмотреть возможность последнего с точки зрения нейронных сетей (НС). За более подробной информацией вы можете обратиться к работе [3]. Главная идея использования метасетей состоит в попытке объяснения работы НС. Таким образом вводя взаимное соответствие между множеством состояний НС и активностью элемента метасети – мы можем проектировать сети, а вводя однозначное – пытаться проследить логику вывода ответа сети. 
</p><p>
Второй связанный с этой темой вопрос – это сознание. Ряд вопросов возник когда в кругозоре объявился Thomas Metzinger. Вопрос заключался в том, можно ли его модель сознания представить в виде активности элемента метасети. 
</p><p>
Увидев, что существующие решения для моделирования НС адаптировать к поставленной задаче довольно сложно – было принято решение написать небольшую библиотеку. Так появился проект MetaNet. Сейчас он находится в стадии глубокой альфы. Разберем один пример. Сначала создадим многослойные персептроны:

</p><pre><code class="python">out_net = FeedforwardNetwork(2, [4], 2)
inp_net_1 = FeedforwardNetwork(2, [4], 2)
inp_net_2 = FeedforwardNetwork(2, [4], 2)
</code></pre><p>
Обучим их на XOR, после чего создадим метасеть – 

</p><pre><code class="python">metanet = MetaNet()
metanet.add_input_net(inp_net_1)
metanet.add_input_net(inp_net_2)
metanet.add_output_net(out_net)
metanet.connect_nets(inp_net_1, out_net)
metanet.connect_nets(inp_net_2, out_net)
metanet.connect_nets(out_net, inp_net_1)
metanet.connect_nets(out_net, inp_net_2)
</code></pre><p>
Вообще говоря – сейчас есть две функции – test и simulate. Первая распространяет сигнал до тех пор, пока все выходные сети не активируются (можно задать максимальное число итераций). Вторая – позволяет гулять сигналу, сколько позволит пользователь. 

</p>
<p>
Если к такой метасети применить сигнал [[1.0, 0.0], [0.0, 0.0]] то первая сеть даст 1, вторая 0 – и выходная сеть соответственно 1. Пока мы не реализовали соединение нейрон-нейрон между сетями, т.к. по умолчанию связи инъективные – мы добавляли один фиктивный выход, который всегда равен 0. При прохождении сигнала нейронам пост-сети присваивается наибольшее значение между текущим и предлагаемым состоянием. 
</p><p>
При применении функции test – сеть, несмотря на циклические связи, вернет выход при активации out_net. Если использовать функцию simulate то можно наблюдать следующую картину состояний выходов сети 

</p><table>
<tr>
<td/>
<td>t1</td>
<td>t2</td>
<td>t3</td>
<td>t4</td>
</tr>
<tr>
<td>inp_net_1</td>
<td>[1, 0]</td>
<td>[0, 0]</td>
<td>[1, 0]</td>
<td>[0, 0]</td>
</tr>
<tr>
<td>inp_net_2</td>
<td>[0, 0]</td>
<td>[0, 0]</td>
<td>[1, 0]</td>
<td>[0, 0]</td>
</tr>
<tr>
<td>out_net</td>
<td>[0, 0]</td>
<td>[1, 0]</td>
<td>[0, 0]</td>
<td>[0, 0]</td>
</tr>
</table>
<p>
Стоит отметить, что по прохождении сигнала сквозь сеть — входы и выходы этой сети обнуляются. 

</p><h2>Выводы</h2><p>
В статье показан контекст, в котором разрабатывается библиотека MetaNet. Я хотел бы обратиться к пользователям habrahabr с просьбой о критике, как идей, так и библиотеки. Хотелось бы на раннем этапе разработки учесть возможные пути развития, которые, вероятно, сейчас я упускаю из вида. Касаемо основного требования к коду – читаемость. Хотелось бы сделать инструмент для исследований, но упускать производительность из вида тоже не выход. Сейчас при использовании, например, mnist время обучения увеличивается до недопустимо больших значений. 
</p><p>
Библиотека доступна по следующей </p><a href="https://sourceforge.net/p/meta-net-project/code/HEAD/tree/">ссылке</a><p>.

</p><h3>Литература</h3>

      <p class="clear"/>
    </div>

    
  </div></body></html>