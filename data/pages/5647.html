<html><body><div><div class="section-content"><div class="section-inner layoutSingleColumn"><p name="65bf" id="65bf" class="graf--p graf--first"><a href="https://launchkit.io/" data-href="https://launchkit.io/" class="markup--anchor markup--p-anchor">LaunchKit</a> and <a href="https://cluster.co/" data-href="https://cluster.co/" class="markup--anchor markup--p-anchor">Cluster</a> use <a href="http://celery.readthedocs.org/en/latest/" data-href="http://celery.readthedocs.org/en/latest/" class="markup--anchor markup--p-anchor">Celery</a> extensively (using Redis as a broker) to handle all sorts of out-of-band background tasks. We have sent millions of push notifications, generated and delivered an insane amount of email, backed up millions of photos and much more all using Celery tasks over the past few years.</p><p name="aab0" id="aab0" class="graf--p graf-after--p">As a result, I have been woken up in the middle of the night many times by various Celery-related issues — and there are a few essential configuration tips I’ve taken away from the experience.</p><p name="7968" id="7968" class="graf--p graf-after--p">Here are a few tips for sleeping through the night while running a Celery task queue in production:</p><h3 name="669a" id="669a" class="graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">1. Set a global task timeout</strong></h3><p name="59d9" id="59d9" class="graf--p graf-after--h3">By default, tasks don’t time out. If a network connection inside your task hangs indefinitely, your queue will eventually back up and something about your service will mysteriously stop working.</p><p name="3a2f" id="3a2f" class="graf--p graf-after--p">So you should set some large global default timeout for tasks, and probably some more specific short timeouts on various tasks as well.</p><p name="eefa" id="eefa" class="graf--p graf-after--p">In a Django project, you can set a global timeout by adding this line to settings.py:</p><pre name="4a92" id="4a92" class="graf--pre graf-after--p"><em class="markup--em markup--pre-em"># Add a one-minute timeout to all Celery tasks.</em><br/>CELERYD_TASK_SOFT_TIME_LIMIT = 60</pre><p name="6f14" id="6f14" class="graf--p graf-after--pre">… which you can override in specific tasks:</p><pre name="9925" id="9925" class="graf--pre graf-after--p">@celery_app.task(soft_time_limit=5)<br/>def send_push_notification(device_token, message, data=None):<br/>  notification_json = build_notification_json(message, data=data)<br/>  ...</pre><p name="edd2" id="edd2" class="graf--p graf-after--pre">This will prevent unexpectedly never-ending tasks from clogging your queues.</p><p name="c4fc" id="c4fc" class="graf--p graf-after--p">Read more: <a href="http://celery.readthedocs.org/en/latest/configuration.html#celeryd-task-soft-time-limit" data-href="http://celery.readthedocs.org/en/latest/configuration.html#celeryd-task-soft-time-limit" class="markup--anchor markup--p-anchor">Soft time limits in Celery</a></p><h3 name="d7ec" id="d7ec" class="graf--h3 graf-after--p">2. Use -Ofair for your preforking workers</h3><p name="b0ba" id="b0ba" class="graf--p graf-after--h3">By default, preforking Celery workers distribute tasks to their worker processes as soon as they are received, regardless of whether the process is currently busy with other tasks.</p><p name="6501" id="6501" class="graf--p graf-after--p">If you have a set of tasks that take varying amounts of time to complete — either deliberately or due to unpredictable network conditions, etc. — this will cause unexpected delays in total execution time for tasks in the queue.</p><p name="6e6d" id="6e6d" class="graf--p graf-after--p">To demonstrate, here’s an example: Let’s say you have 20 tasks, each of which calls some remote API, and each takes 1 second to finish.</p><p name="fc30" id="fc30" class="graf--p graf-after--p">You set up 4 workers to run through these 20 tasks:</p><pre name="8844" id="8844" class="graf--pre graf-after--p">celery worker -A ... -Q random-tasks --concurrency=4</pre><p name="20bf" id="20bf" class="graf--p graf-after--pre">This will take about 5 seconds to finish. 4 subprocesses, 5 tasks each.</p><p name="7983" id="7983" class="graf--p graf-after--p"><strong class="markup--strong markup--p-strong">But</strong>, if instead of 1 second, the first task (task 1 of 20) takes 10 seconds to complete, the total amount of time this queue will take to execute? It’s not 10 seconds — it’s <strong class="markup--strong markup--p-strong">14 seconds</strong>.</p><p name="b4bf" id="b4bf" class="graf--p graf-after--p">That’s because the tasks get distributed evenly, so each subprocess gets 5 of the 20 tasks.</p></div><div class="section-inner sectionLayout--outsetColumn"><figure name="d131" id="d131" class="graf--figure graf--layoutOutsetCenter graf-after--p"><figcaption class="imageCaption">-Ofair results in more predictable task distribution behavior at a relatively small performance cost.</figcaption></figure></div><div class="section-inner layoutSingleColumn"><p name="ec8c" id="ec8c" class="graf--p graf-after--figure">The -Ofair option disables this behavior, <strong class="markup--strong markup--p-strong">waiting to distribute tasks until each worker process is actually <em class="markup--em markup--p-em">available for work</em></strong><em class="markup--em markup--p-em">.</em></p><p name="9f6a" id="9f6a" class="graf--p graf-after--p">This option comes with a coordination cost penalty, but results in a much more predictable behavior if your tasks have varying execution times, as most IO-bound tasks will.</p><p name="7190" id="7190" class="graf--p graf-after--p">Read more: <a href="http://celery.readthedocs.org/en/latest/userguide/optimizing.html#prefork-pool-prefetch-settings" data-href="http://celery.readthedocs.org/en/latest/userguide/optimizing.html#prefork-pool-prefetch-settings" class="markup--anchor markup--p-anchor">Optimizing Celery</a>, <a href="https://gist.github.com/taylorhughes/d5318101dc0fbf96ecdb" data-href="https://gist.github.com/taylorhughes/d5318101dc0fbf96ecdb" class="markup--anchor markup--p-anchor">Gist demonstrating this behavior</a></p><h3 name="509e" id="509e" class="graf--h3 graf-after--p">3. Use exponential retry delays</h3><p name="da85" id="da85" class="graf--p graf-after--h3">Hey, you should really retry that task when it fails — I bet your third-party provider will be back up in a jiffy.</p><p name="4c63" id="4c63" class="graf--p graf-after--p">The best way to retry a task is soon, but not over and over if the service you’re depending on is currently down. So your first retry should be quick, but you should back off fast as failures persist.</p><p name="769b" id="769b" class="graf--p graf-after--p">The <a href="https://en.wikipedia.org/wiki/Exponential_backoff" data-href="https://en.wikipedia.org/wiki/Exponential_backoff" class="markup--anchor markup--p-anchor">canonical way to do this</a> is by exponentially increasing the delay between retry attempts. To do that, you need to find the number of times you’ve already retried and calculate the next countdown using that as an exponent on some base retry time.</p><p name="68b2" id="68b2" class="graf--p graf-after--p">Here’s an example:</p><pre name="24b4" id="24b4" class="graf--pre graf-after--p">@celery_app.task(max_retries=10)<br/>def notify_gcm_device(device_token, message, data=None):<br/>  notification_json = build_gcm_json(message, data=data)<br/> <br/>  try:<br/>    gcm.notify_device(device_token, json=notification_json)</pre><pre name="1430" id="1430" class="graf--pre graf-after--pre">  except ServiceTemporarilyDownError:<br/>    <em class="markup--em markup--pre-em"># Find the number of attempts so far</em><br/>    num_retries = notify_gcm_device<strong class="markup--strong markup--pre-strong">.request.retries</strong><br/>    <strong class="markup--strong markup--pre-strong">seconds_to_wait</strong> = <strong class="markup--strong markup--pre-strong">2.0 ** num_retries</strong></pre><pre name="bd0a" id="bd0a" class="graf--pre graf-after--pre">    <em class="markup--em markup--pre-em"># First countdown will be 1.0, then 2.0, 4.0, etc.</em><br/>    <strong class="markup--strong markup--pre-strong">raise</strong> notify_gcm_device.<strong class="markup--strong markup--pre-strong">retry</strong>(<strong class="markup--strong markup--pre-strong">countdown</strong>=<strong class="markup--strong markup--pre-strong">seconds_to_wait</strong>)</pre><p name="4412" id="4412" class="graf--p graf-after--pre"><em class="markup--em markup--p-em">Note that you should set some sane number of max_retries, both on the task and globally as well.</em></p><p name="a3ff" id="a3ff" class="graf--p graf-after--p">Read more: <a href="https://en.wikipedia.org/wiki/Exponential_backoff" data-href="https://en.wikipedia.org/wiki/Exponential_backoff" class="markup--anchor markup--p-anchor">Exponential backoff</a>, <a href="http://stackoverflow.com/questions/9731435/retry-celery-tasks-with-exponential-back-off" data-href="http://stackoverflow.com/questions/9731435/retry-celery-tasks-with-exponential-back-off" class="markup--anchor markup--p-anchor">Stack Overflow question</a></p><h3 name="fe8d" id="fe8d" class="graf--h3 graf-after--p">4. Sleep soundly</h3><p name="47bd" id="47bd" class="graf--p graf-after--h3">That’s it. Celery has been fast and reliable, but these things have made the process so much more so for us.</p><p name="1a06" id="1a06" class="graf--p graf-after--p">Beyond these simple lessons, our task queues have operated normally with very little fanfare — Redis in particular has been a powerful and reliable broker for the service.</p><p name="f33c" id="f33c" class="graf--p graf-after--p graf--last">I hope these tips save you some sleep while on pager duty!</p></div></div></div></body></html>