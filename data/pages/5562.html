<html><body><div><div class="entry-content">
		<p>Cross validation is an essential tool in statistical learning <span id="easy-footnote-1" class="easy-footnote-margin-adjust"/><span class="easy-footnote"><a href="#easy-footnote-bottom-1" title="&lt;a href=&quot;https://www.youtube.com/watch?v=nZAM5OXrktY&quot;&gt;Lecture 1 on cross validation - Statistical Learning @ Stanford&lt;/a&gt;"><sup>1</sup></a></span> to estimate the accuracy of your algorithm. Despite its great power it also exposes some fundamental risk when done wrong which may terribly bias your accuracy estimate.</p>
<p>In this blog post I'll demonstrate - using the Python <a href="http://scikit-learn.org/stable/">scikit-learn</a><span id="easy-footnote-2" class="easy-footnote-margin-adjust"/><span class="easy-footnote"><a href="#easy-footnote-bottom-2" title="&lt;a href=&quot;http://scikit-learn.org/stable/&quot;&gt;Scikit-learn framework&lt;/a&gt;&lt;a&gt;"><sup>2</sup></a></span> framework - how to avoid the biggest and most common pitfall of cross validation in your experiments.</p>
<p><span id="more-910"/></p>
<h2>Theory first</h2>
<p>Cross validation involves randomly dividing the set of observations into <code>k</code> groups (or folds) of approximately equal size. The first fold is treated as a validation set, and the machine learning algorithm is trained on the remaining <code>k-1</code> folds. The mean squared error is then computed on the held-out fold. This procedure is repeated k times; each time, a different group of observations is treated as a validation set.</p>
<p>This process results in k estimates of the MSE quantity, namely <span class="MathJax_Preview"><img src="http://www.alfredo.motta.name/wp-content/plugins/latex/cache/tex_19a9931adc5570d43f4c3850cfe69b73.gif" class="tex" alt="MSE_1"/></span>, <span class="MathJax_Preview"><img src="http://www.alfredo.motta.name/wp-content/plugins/latex/cache/tex_cdbdf0b0075c3daed37d25f942109c33.gif" class="tex" alt="MSE_2"/></span>,...<span class="MathJax_Preview"><img src="http://www.alfredo.motta.name/wp-content/plugins/latex/cache/tex_1ff7d0d3e2142dd1257f3138eb4ba237.gif" class="tex" alt="MSE_k"/></span>. The cross validation estimate for the MSE is then computed by simply averaging these values:<br/>
</p><p><span class="MathJax_Preview"><img src="http://www.alfredo.motta.name/wp-content/plugins/latex/cache/tex_175a9fea021e3c91571dd25be9a3681c.gif" class="tex" alt="CV_{(k)} = 1/k \sum_{i=1}^k MSE_i"/></span></p>
<p>This value is an <em>estimate</em>, say <span class="MathJax_Preview"><img src="http://www.alfredo.motta.name/wp-content/plugins/latex/cache/tex_1340beca22fa995cf96bc5e2818c21fb.gif" class="tex" alt="\hat{MSE}"/></span>, of the real <span class="MathJax_Preview"><img src="http://www.alfredo.motta.name/wp-content/plugins/latex/cache/tex_a9fc1a03386ae38b64e06c8172994963.gif" class="tex" alt="MSE"/></span> and our goal is to make this estimate as accurate as possible. <span class="MathJax_Preview"><img src="http://www.alfredo.motta.name/wp-content/plugins/latex/cache/tex_a9fc1a03386ae38b64e06c8172994963.gif" class="tex" alt="MSE"/></span> is just one for the possible metrics you can estimate using cross validation but the results of this blog post are independent from the type of metric you use.</p>
<h2>Hands on</h2>
<p>Let's now have a look at one of the most typical mistakes when using cross validation. When cross validation is done wrong the result is that <span class="MathJax_Preview"><img src="http://www.alfredo.motta.name/wp-content/plugins/latex/cache/tex_1340beca22fa995cf96bc5e2818c21fb.gif" class="tex" alt="\hat{MSE}"/></span> does not reflect its real value <span class="MathJax_Preview"><img src="http://www.alfredo.motta.name/wp-content/plugins/latex/cache/tex_a9fc1a03386ae38b64e06c8172994963.gif" class="tex" alt="MSE"/></span>. In other words, you may think that you just found a perfect machine learning algorithm with incredibly low <span class="MathJax_Preview"><img src="http://www.alfredo.motta.name/wp-content/plugins/latex/cache/tex_a9fc1a03386ae38b64e06c8172994963.gif" class="tex" alt="MSE"/></span>, while in reality you simply wrongly applied CV.</p>
<p>I'll first show you - hands on - a wrong application of cross validation and then we will fix it together. The code is also available as an <a href="https://github.com/mottalrd/cross-validation-done-wrong">IPython notebook on github</a>.</p>
<h3>Dataset generation</h3>
<p/>

		<div id="crayon-56d5993cb337a136986083" class="crayon-syntax crayon-theme-classic crayon-font-monaco crayon-os-mac print-yes notranslate" data-settings=" minimize scroll-mouseover">
		
			
			<p class="crayon-info"/>
			<p class="crayon-plain-wrap"/>
			<div class="crayon-main">
				<table class="crayon-table">
					<tr class="crayon-row">
				<td class="crayon-nums " data-settings="show">
					
				</td>
						<td class="crayon-code"><div class="crayon-pre"><p class="crayon-line" id="crayon-56d5993cb337a136986083-1"><span class="crayon-p"># Import pandas</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb337a136986083-2"><span class="crayon-e">import </span><span class="crayon-e">pandas </span><span class="crayon-st">as</span><span class="crayon-h"> </span><span class="crayon-e">pd</span></p><p class="crayon-line" id="crayon-56d5993cb337a136986083-3"><span class="crayon-e">from </span><span class="crayon-e">pandas </span><span class="crayon-e ">import *</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb337a136986083-4"> </p><p class="crayon-line" id="crayon-56d5993cb337a136986083-5"><span class="crayon-p"># Import scikit-learn</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb337a136986083-6"><span class="crayon-e">from </span><span class="crayon-v">sklearn</span><span class="crayon-sy">.</span><span class="crayon-e">linear_model </span><span class="crayon-e">import </span><span class="crayon-e">LogisticRegression</span></p><p class="crayon-line" id="crayon-56d5993cb337a136986083-7"><span class="crayon-e">from </span><span class="crayon-v">sklearn</span><span class="crayon-sy">.</span><span class="crayon-e">cross_validation </span><span class="crayon-e ">import *</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb337a136986083-8"><span class="crayon-e">from </span><span class="crayon-v">sklearn</span><span class="crayon-sy">.</span><span class="crayon-e">metrics </span><span class="crayon-e ">import *</span></p><p class="crayon-line" id="crayon-56d5993cb337a136986083-9"><span class="crayon-e">import </span><span class="crayon-v">random</span></p></div></td>
					</tr>
				</table>
			</div>
		</div>

<p/>
<p>To make things simple let's first generate some random data and let's pretend that we want to build a machine learning algorithm to predict the outcome. I'll first generate a dataset of <code>100</code> entries. Each entry has <code>10.000</code> features. But, why so many? Well, to demonstrate our issue I need to generate some correlation between our inputs and output which is purely casual. You'll understand <em>the why</em> later in this post.</p>
<p/>

		<div id="crayon-56d5993cb33a2825103113" class="crayon-syntax crayon-theme-classic crayon-font-monaco crayon-os-mac print-yes notranslate" data-settings=" minimize scroll-mouseover">
		
			
			<p class="crayon-info"/>
			<p class="crayon-plain-wrap"/>
			<div class="crayon-main">
				<table class="crayon-table">
					<tr class="crayon-row">
				<td class="crayon-nums " data-settings="show">
					
				</td>
						<td class="crayon-code"><div class="crayon-pre"><p class="crayon-line" id="crayon-56d5993cb33a2825103113-1"><span class="crayon-v">np</span><span class="crayon-sy">.</span><span class="crayon-v">random</span><span class="crayon-sy">.</span><span class="crayon-e">seed</span><span class="crayon-sy">(</span><span class="crayon-cn">0</span><span class="crayon-sy">)</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33a2825103113-2"><span class="crayon-v">features</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-v">np</span><span class="crayon-sy">.</span><span class="crayon-v">random</span><span class="crayon-sy">.</span><span class="crayon-e">randint</span><span class="crayon-sy">(</span><span class="crayon-cn">0</span><span class="crayon-sy">,</span><span class="crayon-cn">10</span><span class="crayon-sy">,</span><span class="crayon-v">size</span><span class="crayon-o">=</span><span class="crayon-sy">[</span><span class="crayon-cn">100</span><span class="crayon-sy">,</span><span class="crayon-cn">10000</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span></p><p class="crayon-line" id="crayon-56d5993cb33a2825103113-3"><span class="crayon-v">target</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-v">np</span><span class="crayon-sy">.</span><span class="crayon-v">random</span><span class="crayon-sy">.</span><span class="crayon-e">randint</span><span class="crayon-sy">(</span><span class="crayon-cn">0</span><span class="crayon-sy">,</span><span class="crayon-cn">2</span><span class="crayon-sy">,</span><span class="crayon-v">size</span><span class="crayon-o">=</span><span class="crayon-cn">100</span><span class="crayon-sy">)</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33a2825103113-4"> </p><p class="crayon-line" id="crayon-56d5993cb33a2825103113-5"><span class="crayon-v">df</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">DataFrame</span><span class="crayon-sy">(</span><span class="crayon-v">features</span><span class="crayon-sy">)</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33a2825103113-6"><span class="crayon-v">df</span><span class="crayon-sy">[</span><span class="crayon-s">'target'</span><span class="crayon-sy">]</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">target</span></p><p class="crayon-line" id="crayon-56d5993cb33a2825103113-7"><span class="crayon-v">df</span><span class="crayon-sy">.</span><span class="crayon-e">head</span><span class="crayon-sy">(</span><span class="crayon-sy">)</span></p></div></td>
					</tr>
				</table>
			</div>
		</div>

<p/>
<h3>Feature selection</h3>
<p>At this point we would like to know what are the features that are more useful to train our predictor. This is called <em>feature selection</em>. The simplest approach to do that is to find which of the <code>10.000</code> features in our input is mostly correlated the target. Using <code>pandas</code> this is very easy to do thanks to the <a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.corr.html"><code>corr()</code></a> function. We run <code>corr()</code> on our dataframe, we order the correlation values, and we pick the first two features.</p>
<p/>

		<div id="crayon-56d5993cb33aa194761165" class="crayon-syntax crayon-theme-classic crayon-font-monaco crayon-os-mac print-yes notranslate" data-settings=" minimize scroll-mouseover">
		
			
			<p class="crayon-info"/>
			<p class="crayon-plain-wrap"/>
			<div class="crayon-main">
				<table class="crayon-table">
					<tr class="crayon-row">
				<td class="crayon-nums " data-settings="show">
					
				</td>
						<td class="crayon-code"><div class="crayon-pre"><p class="crayon-line" id="crayon-56d5993cb33aa194761165-1"><span class="crayon-v">corr</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-v">df</span><span class="crayon-sy">.</span><span class="crayon-e">corr</span><span class="crayon-sy">(</span><span class="crayon-sy">)</span><span class="crayon-sy">[</span><span class="crayon-s">'target'</span><span class="crayon-sy">]</span><span class="crayon-sy">[</span><span class="crayon-v">df</span><span class="crayon-sy">.</span><span class="crayon-e">corr</span><span class="crayon-sy">(</span><span class="crayon-sy">)</span><span class="crayon-sy">[</span><span class="crayon-s">'target'</span><span class="crayon-sy">]</span><span class="crayon-h"> </span><span class="crayon-o">&lt;</span><span class="crayon-h"> </span><span class="crayon-cn">1</span><span class="crayon-sy">]</span><span class="crayon-sy">.</span><span class="crayon-e">abs</span><span class="crayon-sy">(</span><span class="crayon-sy">)</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33aa194761165-2"><span class="crayon-v">corr</span><span class="crayon-sy">.</span><span class="crayon-e">sort</span><span class="crayon-sy">(</span><span class="crayon-v">ascending</span><span class="crayon-o">=</span><span class="crayon-t">False</span><span class="crayon-sy">)</span></p><p class="crayon-line" id="crayon-56d5993cb33aa194761165-3"><span class="crayon-v">corr</span><span class="crayon-sy">.</span><span class="crayon-e">head</span><span class="crayon-sy">(</span><span class="crayon-sy">)</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33aa194761165-4"> </p><p class="crayon-line" id="crayon-56d5993cb33aa194761165-5"><span class="crayon-p"># 8487    0.428223</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33aa194761165-6"><span class="crayon-p"># 3555    0.398636</span></p><p class="crayon-line" id="crayon-56d5993cb33aa194761165-7"><span class="crayon-p"># 627     0.365970</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33aa194761165-8"><span class="crayon-p"># 3987    0.361673</span></p><p class="crayon-line" id="crayon-56d5993cb33aa194761165-9"><span class="crayon-p"># 1409    0.357135</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33aa194761165-10"><span class="crayon-p"># Name: target, dtype: float64</span></p></div></td>
					</tr>
				</table>
			</div>
		</div>

<p/>
<h3>Start the training</h3>
<p>Great! Out of the <code>10.000</code> features we have been able to select two of them, i.e. feature number <code>8487</code> and <code>3555</code> that have a <code>0.42</code> and <code>0.39</code> correlation with the output. At this point let's just drop all the other columns and use these two features to train a simple <code>LogisticRegression</code>. We then use scikit-learn <code>cross_val_score</code> to compute <span class="MathJax_Preview"><img src="http://www.alfredo.motta.name/wp-content/plugins/latex/cache/tex_1340beca22fa995cf96bc5e2818c21fb.gif" class="tex" alt="\hat{MSE}"/></span> which in this case is equal to <code>0.249</code>. Pretty good!</p>
<p/>

		<div id="crayon-56d5993cb33b1977062089" class="crayon-syntax crayon-theme-classic crayon-font-monaco crayon-os-mac print-yes notranslate" data-settings=" minimize scroll-mouseover">
		
			
			<p class="crayon-info"/>
			<p class="crayon-plain-wrap"/>
			<div class="crayon-main">
				<table class="crayon-table">
					<tr class="crayon-row">
				<td class="crayon-nums " data-settings="show">
					
				</td>
						<td class="crayon-code"><div class="crayon-pre"><p class="crayon-line" id="crayon-56d5993cb33b1977062089-1"><span class="crayon-v">features</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-v">corr</span><span class="crayon-sy">.</span><span class="crayon-v">index</span><span class="crayon-sy">[</span><span class="crayon-sy">[</span><span class="crayon-cn">0</span><span class="crayon-sy">,</span><span class="crayon-cn">1</span><span class="crayon-sy">]</span><span class="crayon-sy">]</span><span class="crayon-sy">.</span><span class="crayon-e">values</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33b1977062089-2"><span class="crayon-v">training_input</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-v">df</span><span class="crayon-sy">[</span><span class="crayon-v">features</span><span class="crayon-sy">]</span><span class="crayon-sy">.</span><span class="crayon-e">values</span></p><p class="crayon-line" id="crayon-56d5993cb33b1977062089-3"><span class="crayon-v">training_output</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-v">df</span><span class="crayon-sy">[</span><span class="crayon-s">'target'</span><span class="crayon-sy">]</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33b1977062089-4"><span class="crayon-v">logreg</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">LogisticRegression</span><span class="crayon-sy">(</span><span class="crayon-sy">)</span></p><p class="crayon-line" id="crayon-56d5993cb33b1977062089-5"> </p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33b1977062089-6"><span class="crayon-p"># scikit learn return the negative value for MSE</span></p><p class="crayon-line" id="crayon-56d5993cb33b1977062089-7"><span class="crayon-p"># http://stackoverflow.com/questions/21443865/scikit-learn-cross-validation-negative-values-with-mean-squared-error</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33b1977062089-8"><span class="crayon-v">mse_estimate</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-o">-</span><span class="crayon-cn">1</span><span class="crayon-h"> </span><span class="crayon-o">*</span><span class="crayon-h"> </span><span class="crayon-e">cross_val_score</span><span class="crayon-sy">(</span><span class="crayon-v">logreg</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">training_input</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">training_output</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">cv</span><span class="crayon-o">=</span><span class="crayon-cn">10</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">scoring</span><span class="crayon-o">=</span><span class="crayon-s">'mean_squared_error'</span><span class="crayon-sy">)</span></p><p class="crayon-line" id="crayon-56d5993cb33b1977062089-9"> </p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33b1977062089-10"><span class="crayon-v">mse_estimate</span></p><p class="crayon-line" id="crayon-56d5993cb33b1977062089-11"><span class="crayon-p"># array([ 0.45454545, 0.2, 0.2, 0.1, 0.1,</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33b1977062089-12"><span class="crayon-p">#        0., 0.3, 0.4, 0.3, 0.44444444])</span></p><p class="crayon-line" id="crayon-56d5993cb33b1977062089-13"> </p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33b1977062089-14"><span class="crayon-e">DataFrame</span><span class="crayon-sy">(</span><span class="crayon-v">mse_estimate</span><span class="crayon-sy">)</span><span class="crayon-sy">.</span><span class="crayon-e">mean</span><span class="crayon-sy">(</span><span class="crayon-sy">)</span></p><p class="crayon-line" id="crayon-56d5993cb33b1977062089-15"><span class="crayon-p"># 0 0.249899</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33b1977062089-16"><span class="crayon-p"># dtype: float64</span></p></div></td>
					</tr>
				</table>
			</div>
		</div>

<p/>
<p><small>Note [1]: I am using <span class="MathJax_Preview"><img src="http://www.alfredo.motta.name/wp-content/plugins/latex/cache/tex_a9fc1a03386ae38b64e06c8172994963.gif" class="tex" alt="MSE"/></span> here to evaluate the quality of the logistic regression, but you should probably consider using a <a href="https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test">Chi-squared test</a>. The interpretation of the results that follows is equivalent.</small></p>
<p><small>Note [2]: By default scikit-learn use <a href="http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedKFold.html">Stratified KFold</a><span id="easy-footnote-3" class="easy-footnote-margin-adjust"/><span class="easy-footnote"><a href="#easy-footnote-bottom-3" title="&lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedKFold.html&quot;&gt;Stratified KFold Documentation&lt;/a&gt;"><sup>3</sup></a></span> where the folds are made by preserving the percentage of samples for each class.</small></p>
<h3>Knowledge leaking</h3>
<p>According to the previous estimate we built a system that can predict a random noise target from a random noise input with a <span class="MathJax_Preview"><img src="http://www.alfredo.motta.name/wp-content/plugins/latex/cache/tex_a9fc1a03386ae38b64e06c8172994963.gif" class="tex" alt="MSE"/></span> of just <code>0.249</code>. The result is, as you can expect, wrong. But why?</p>
<p>The reason is rather counterintuitive and this is why this mistake is so common<span id="easy-footnote-4" class="easy-footnote-margin-adjust"/><span class="easy-footnote"><a href="#easy-footnote-bottom-4" title="&lt;a href=&quot;https://www.youtube.com/watch?v=S06JpVoNaA0&quot;&gt;Lecture 2 on cross validation - Statistical Learning @ Stanford&lt;/a&gt;"><sup>4</sup></a></span>. When we applied the feature selection we used information from both the training set and the test sets used for the cross validation, i.e. the correlation values. As a consequence our <code>LogisticRegression</code> knew information in the test sets that were supposed to be hidden to it. In fact, when you are computing <span class="MathJax_Preview"><img src="http://www.alfredo.motta.name/wp-content/plugins/latex/cache/tex_9d5d68cf2144542c7ddfb0e2f3f6ff52.gif" class="tex" alt="MSE_i"/></span> in the i-th iteration of the cross validation you should be using only the information on the training fold, and nothing should come from the test fold. In our case the model did indeed have information from the test fold, i.e. the top correlated features. I think the term <strong>knowledge leaking</strong> express this concept fairly well.</p>
<p>The schema that follows shows you how the knowledge leaked into the <code>LogisticRegression</code> because the feature selection has been applied <em>before</em> the cross validation procedure started. The model knows something about the data highlighted in yellow that it shoulnd't know, its top correlated features.</p>
<figure id="attachment_922" class="wp-caption aligncenter"><a href="http://www.alfredo.motta.name/wp-content/uploads/2015/07/cv_example.png"><img class="wp-image-922 size-large" src="http://www.alfredo.motta.name/wp-content/uploads/2015/07/cv_example-1024x726.png" alt="The exposed knowledge leaking. The LogisticRegression knows the top correlated features of the entire dataset (hence including test folds) because of the initial correlation operation, whilst it should be exposed only to the training fold information." srcset="http://www.alfredo.motta.name/wp-content/uploads/2015/07/cv_example-300x213.png 300w, http://www.alfredo.motta.name/wp-content/uploads/2015/07/cv_example-1024x726.png 1024w, http://www.alfredo.motta.name/wp-content/uploads/2015/07/cv_example.png 1220w" sizes="(max-width: 660px) 100vw, 660px"/></a><figcaption class="wp-caption-text">Figure 1. The exposed knowledge leaking. The LogisticRegression knows the top correlated features of the entire dataset (hence including test folds) because of the initial correlation operation, whilst it should be exposed only to the training fold information.</figcaption></figure>
<h3>Proof that our model is biased</h3>
<p>To check that we were actually wrong let's do the following:<br/>
* Take out a portion of the data set (take_out_set).<br/>
* Train the LogisticRegression on the remaining data using the same feature selection we did before.<br/>
* After the training is done check the <span class="MathJax_Preview"><img src="http://www.alfredo.motta.name/wp-content/plugins/latex/cache/tex_a9fc1a03386ae38b64e06c8172994963.gif" class="tex" alt="MSE"/></span> on the take_out_set.</p>
<p>Is the <span class="MathJax_Preview"><img src="http://www.alfredo.motta.name/wp-content/plugins/latex/cache/tex_a9fc1a03386ae38b64e06c8172994963.gif" class="tex" alt="MSE"/></span> on the take_out_set similar to the <span class="MathJax_Preview"><img src="http://www.alfredo.motta.name/wp-content/plugins/latex/cache/tex_1340beca22fa995cf96bc5e2818c21fb.gif" class="tex" alt="\hat{MSE}"/></span> we estimated with the CV? The answer is no, and we got a much more reasonable <span class="MathJax_Preview"><img src="http://www.alfredo.motta.name/wp-content/plugins/latex/cache/tex_a9fc1a03386ae38b64e06c8172994963.gif" class="tex" alt="MSE"/></span> of <code>0.53</code> that is much higher than the <span class="MathJax_Preview"><img src="http://www.alfredo.motta.name/wp-content/plugins/latex/cache/tex_1340beca22fa995cf96bc5e2818c21fb.gif" class="tex" alt="\hat{MSE}"/></span> of <code>0.249</code>.</p>
<p/>

		<div id="crayon-56d5993cb33bb665829656" class="crayon-syntax crayon-theme-classic crayon-font-monaco crayon-os-mac print-yes notranslate" data-settings=" minimize scroll-mouseover">
		
			
			<p class="crayon-info"/>
			<p class="crayon-plain-wrap"/>
			<div class="crayon-main">
				<table class="crayon-table">
					<tr class="crayon-row">
				<td class="crayon-nums " data-settings="show">
					<div class="crayon-nums-content"><p class="crayon-num" data-line="crayon-56d5993cb33bb665829656-1">1</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33bb665829656-2">2</p><p class="crayon-num" data-line="crayon-56d5993cb33bb665829656-3">3</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33bb665829656-4">4</p><p class="crayon-num" data-line="crayon-56d5993cb33bb665829656-5">5</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33bb665829656-6">6</p><p class="crayon-num" data-line="crayon-56d5993cb33bb665829656-7">7</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33bb665829656-8">8</p><p class="crayon-num" data-line="crayon-56d5993cb33bb665829656-9">9</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33bb665829656-10">10</p><p class="crayon-num" data-line="crayon-56d5993cb33bb665829656-11">11</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33bb665829656-12">12</p><p class="crayon-num" data-line="crayon-56d5993cb33bb665829656-13">13</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33bb665829656-14">14</p><p class="crayon-num" data-line="crayon-56d5993cb33bb665829656-15">15</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33bb665829656-16">16</p><p class="crayon-num" data-line="crayon-56d5993cb33bb665829656-17">17</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33bb665829656-18">18</p><p class="crayon-num" data-line="crayon-56d5993cb33bb665829656-19">19</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33bb665829656-20">20</p><p class="crayon-num" data-line="crayon-56d5993cb33bb665829656-21">21</p></div>
				</td>
						<td class="crayon-code"><div class="crayon-pre"><p class="crayon-line" id="crayon-56d5993cb33bb665829656-1"><span class="crayon-v">take_out_set</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-v">df</span><span class="crayon-sy">.</span><span class="crayon-v">ix</span><span class="crayon-sy">[</span><span class="crayon-v">random</span><span class="crayon-sy">.</span><span class="crayon-e">sample</span><span class="crayon-sy">(</span><span class="crayon-v">df</span><span class="crayon-sy">.</span><span class="crayon-v">index</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">30</span><span class="crayon-sy">)</span><span class="crayon-sy">]</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33bb665829656-2"><span class="crayon-v">training_set</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-v">df</span><span class="crayon-sy">[</span><span class="crayon-o">~</span><span class="crayon-sy">(</span><span class="crayon-v">df</span><span class="crayon-sy">.</span><span class="crayon-e">isin</span><span class="crayon-sy">(</span><span class="crayon-v">take_out_set</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span><span class="crayon-sy">.</span><span class="crayon-e">all</span><span class="crayon-sy">(</span><span class="crayon-v">axis</span><span class="crayon-o">=</span><span class="crayon-cn">1</span><span class="crayon-sy">)</span><span class="crayon-sy">]</span></p><p class="crayon-line" id="crayon-56d5993cb33bb665829656-3"> </p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33bb665829656-4"><span class="crayon-v">corr</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-v">training_set</span><span class="crayon-sy">.</span><span class="crayon-e">corr</span><span class="crayon-sy">(</span><span class="crayon-sy">)</span><span class="crayon-sy">[</span><span class="crayon-s">'target'</span><span class="crayon-sy">]</span><span class="crayon-sy">[</span><span class="crayon-v">df</span><span class="crayon-sy">.</span><span class="crayon-e">corr</span><span class="crayon-sy">(</span><span class="crayon-sy">)</span><span class="crayon-sy">[</span><span class="crayon-s">'target'</span><span class="crayon-sy">]</span><span class="crayon-h"> </span><span class="crayon-o">&lt;</span><span class="crayon-h"> </span><span class="crayon-cn">1</span><span class="crayon-sy">]</span><span class="crayon-sy">.</span><span class="crayon-e">abs</span><span class="crayon-sy">(</span><span class="crayon-sy">)</span></p><p class="crayon-line" id="crayon-56d5993cb33bb665829656-5"><span class="crayon-v">corr</span><span class="crayon-sy">.</span><span class="crayon-e">sort</span><span class="crayon-sy">(</span><span class="crayon-v">ascending</span><span class="crayon-o">=</span><span class="crayon-t">False</span><span class="crayon-sy">)</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33bb665829656-6"><span class="crayon-v">features</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-v">corr</span><span class="crayon-sy">.</span><span class="crayon-v">index</span><span class="crayon-sy">[</span><span class="crayon-sy">[</span><span class="crayon-cn">0</span><span class="crayon-sy">,</span><span class="crayon-cn">1</span><span class="crayon-sy">]</span><span class="crayon-sy">]</span><span class="crayon-sy">.</span><span class="crayon-e">values</span></p><p class="crayon-line" id="crayon-56d5993cb33bb665829656-7"> </p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33bb665829656-8"><span class="crayon-v">training_input</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-v">training_set</span><span class="crayon-sy">[</span><span class="crayon-v">features</span><span class="crayon-sy">]</span><span class="crayon-sy">.</span><span class="crayon-e">values</span></p><p class="crayon-line" id="crayon-56d5993cb33bb665829656-9"><span class="crayon-v">training_output</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-v">training_set</span><span class="crayon-sy">[</span><span class="crayon-s">'target'</span><span class="crayon-sy">]</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33bb665829656-10"> </p><p class="crayon-line" id="crayon-56d5993cb33bb665829656-11"><span class="crayon-v">logreg</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">LogisticRegression</span><span class="crayon-sy">(</span><span class="crayon-sy">)</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33bb665829656-12"><span class="crayon-v">logreg</span><span class="crayon-sy">.</span><span class="crayon-e">fit</span><span class="crayon-sy">(</span><span class="crayon-v">training_input</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">training_output</span><span class="crayon-sy">)</span></p><p class="crayon-line" id="crayon-56d5993cb33bb665829656-13"> </p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33bb665829656-14"><span class="crayon-p"># LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,</span></p><p class="crayon-line" id="crayon-56d5993cb33bb665829656-15"><span class="crayon-p"># intercept_scaling=1, max_iter=100, multi_class='ovr',</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33bb665829656-16"><span class="crayon-p"># penalty='l2', random_state=None, solver='liblinear', tol=0.0001,</span></p><p class="crayon-line" id="crayon-56d5993cb33bb665829656-17"><span class="crayon-p"># verbose=0)</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33bb665829656-18"> </p><p class="crayon-line" id="crayon-56d5993cb33bb665829656-19"><span class="crayon-v">y_take_out</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-v">logreg</span><span class="crayon-sy">.</span><span class="crayon-e">predict</span><span class="crayon-sy">(</span><span class="crayon-v">take_out_set</span><span class="crayon-sy">[</span><span class="crayon-v">features</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33bb665829656-20"><span class="crayon-e">mean_squared_error</span><span class="crayon-sy">(</span><span class="crayon-v">take_out_set</span><span class="crayon-sy">.</span><span class="crayon-v">target</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">y_take_out</span><span class="crayon-sy">)</span></p><p class="crayon-line" id="crayon-56d5993cb33bb665829656-21"><span class="crayon-p"># 0.53333333333333333</span></p></div></td>
					</tr>
				</table>
			</div>
		</div>

<p/>
<h2>Cross validation done right</h2>
<p>In the previous section we have seen that if you inject test knowledge in your model your cross validation procedure will be biased. To avoid this let's compute the features correlation during each cross validation batch. The difference is that now the features correlation will use only the information in the training fold instead of the entire dataset. That's the key insight causing the bias we saw previously. The following graph shows you the revisited procedure. This time we got a realistic <span class="MathJax_Preview"><img src="http://www.alfredo.motta.name/wp-content/plugins/latex/cache/tex_1340beca22fa995cf96bc5e2818c21fb.gif" class="tex" alt="\hat{MSE}"/></span> of <code>0.44</code> that confirms the data is randomly distributed.</p>
<figure id="attachment_928" class="wp-caption aligncenter"><a href="http://www.alfredo.motta.name/wp-content/uploads/2015/07/cv_example_2.png"><img src="http://www.alfredo.motta.name/wp-content/uploads/2015/07/cv_example_2-1024x522.png" alt="Figure 2.  Revisited cross validation workflow with the correlation step performed for each of the K fold train/test data folds." class="size-large wp-image-928" srcset="http://www.alfredo.motta.name/wp-content/uploads/2015/07/cv_example_2-300x153.png 300w, http://www.alfredo.motta.name/wp-content/uploads/2015/07/cv_example_2-1024x522.png 1024w, http://www.alfredo.motta.name/wp-content/uploads/2015/07/cv_example_2.png 1260w" sizes="(max-width: 660px) 100vw, 660px"/></a><figcaption class="wp-caption-text">Figure 2.  Revisited cross validation workflow with the correlation step performed for each of the K train/test folds.</figcaption></figure>
<p/>

		<div id="crayon-56d5993cb33c4351193205" class="crayon-syntax crayon-theme-classic crayon-font-monaco crayon-os-mac print-yes notranslate" data-settings=" minimize scroll-mouseover">
		
			
			<p class="crayon-info"/>
			<p class="crayon-plain-wrap"/>
			<div class="crayon-main">
				<table class="crayon-table">
					<tr class="crayon-row">
				<td class="crayon-nums " data-settings="show">
					<div class="crayon-nums-content"><p class="crayon-num" data-line="crayon-56d5993cb33c4351193205-1">1</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33c4351193205-2">2</p><p class="crayon-num" data-line="crayon-56d5993cb33c4351193205-3">3</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33c4351193205-4">4</p><p class="crayon-num" data-line="crayon-56d5993cb33c4351193205-5">5</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33c4351193205-6">6</p><p class="crayon-num" data-line="crayon-56d5993cb33c4351193205-7">7</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33c4351193205-8">8</p><p class="crayon-num" data-line="crayon-56d5993cb33c4351193205-9">9</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33c4351193205-10">10</p><p class="crayon-num" data-line="crayon-56d5993cb33c4351193205-11">11</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33c4351193205-12">12</p><p class="crayon-num" data-line="crayon-56d5993cb33c4351193205-13">13</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33c4351193205-14">14</p><p class="crayon-num" data-line="crayon-56d5993cb33c4351193205-15">15</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33c4351193205-16">16</p><p class="crayon-num" data-line="crayon-56d5993cb33c4351193205-17">17</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33c4351193205-18">18</p><p class="crayon-num" data-line="crayon-56d5993cb33c4351193205-19">19</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33c4351193205-20">20</p><p class="crayon-num" data-line="crayon-56d5993cb33c4351193205-21">21</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33c4351193205-22">22</p><p class="crayon-num" data-line="crayon-56d5993cb33c4351193205-23">23</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33c4351193205-24">24</p><p class="crayon-num" data-line="crayon-56d5993cb33c4351193205-25">25</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33c4351193205-26">26</p><p class="crayon-num" data-line="crayon-56d5993cb33c4351193205-27">27</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33c4351193205-28">28</p><p class="crayon-num" data-line="crayon-56d5993cb33c4351193205-29">29</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33c4351193205-30">30</p><p class="crayon-num" data-line="crayon-56d5993cb33c4351193205-31">31</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33c4351193205-32">32</p><p class="crayon-num" data-line="crayon-56d5993cb33c4351193205-33">33</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33c4351193205-34">34</p><p class="crayon-num" data-line="crayon-56d5993cb33c4351193205-35">35</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33c4351193205-36">36</p><p class="crayon-num" data-line="crayon-56d5993cb33c4351193205-37">37</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33c4351193205-38">38</p><p class="crayon-num" data-line="crayon-56d5993cb33c4351193205-39">39</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33c4351193205-40">40</p><p class="crayon-num" data-line="crayon-56d5993cb33c4351193205-41">41</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33c4351193205-42">42</p><p class="crayon-num" data-line="crayon-56d5993cb33c4351193205-43">43</p><p class="crayon-num crayon-striped-num" data-line="crayon-56d5993cb33c4351193205-44">44</p></div>
				</td>
						<td class="crayon-code"><div class="crayon-pre"><p class="crayon-line" id="crayon-56d5993cb33c4351193205-1"><span class="crayon-v">kf</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">StratifiedKFold</span><span class="crayon-sy">(</span><span class="crayon-v">df</span><span class="crayon-sy">[</span><span class="crayon-s">'target'</span><span class="crayon-sy">]</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">n_folds</span><span class="crayon-o">=</span><span class="crayon-cn">10</span><span class="crayon-sy">)</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33c4351193205-2"><span class="crayon-v">mse</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-sy">]</span></p><p class="crayon-line" id="crayon-56d5993cb33c4351193205-3"><span class="crayon-v">fold_count</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-cn">0</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33c4351193205-4"><span class="crayon-st">for</span><span class="crayon-h"> </span><span class="crayon-v">train</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-e">test </span><span class="crayon-st">in</span><span class="crayon-h"> </span><span class="crayon-v">kf</span><span class="crayon-o">:</span></p><p class="crayon-line" id="crayon-56d5993cb33c4351193205-5"><span class="crayon-h">  </span><span class="crayon-e">print</span><span class="crayon-sy">(</span><span class="crayon-s">"Processing fold %s"</span><span class="crayon-h"> </span><span class="crayon-o">%</span><span class="crayon-h"> </span><span class="crayon-v">fold_count</span><span class="crayon-sy">)</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33c4351193205-6"><span class="crayon-h">  </span><span class="crayon-v">train_fold</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-v">df</span><span class="crayon-sy">.</span><span class="crayon-v">ix</span><span class="crayon-sy">[</span><span class="crayon-v">train</span><span class="crayon-sy">]</span></p><p class="crayon-line" id="crayon-56d5993cb33c4351193205-7"><span class="crayon-h">  </span><span class="crayon-v">test_fold</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-v">df</span><span class="crayon-sy">.</span><span class="crayon-v">ix</span><span class="crayon-sy">[</span><span class="crayon-v">test</span><span class="crayon-sy">]</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33c4351193205-8"> </p><p class="crayon-line" id="crayon-56d5993cb33c4351193205-9"><span class="crayon-h">  </span><span class="crayon-p"># find best features</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33c4351193205-10"><span class="crayon-h">  </span><span class="crayon-v">corr</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-v">train_fold</span><span class="crayon-sy">.</span><span class="crayon-e">corr</span><span class="crayon-sy">(</span><span class="crayon-sy">)</span><span class="crayon-sy">[</span><span class="crayon-s">'target'</span><span class="crayon-sy">]</span><span class="crayon-sy">[</span><span class="crayon-v">train_fold</span><span class="crayon-sy">.</span><span class="crayon-e">corr</span><span class="crayon-sy">(</span><span class="crayon-sy">)</span><span class="crayon-sy">[</span><span class="crayon-s">'target'</span><span class="crayon-sy">]</span><span class="crayon-h"> </span><span class="crayon-o">&lt;</span><span class="crayon-h"> </span><span class="crayon-cn">1</span><span class="crayon-sy">]</span><span class="crayon-sy">.</span><span class="crayon-e">abs</span><span class="crayon-sy">(</span><span class="crayon-sy">)</span></p><p class="crayon-line" id="crayon-56d5993cb33c4351193205-11"><span class="crayon-h">  </span><span class="crayon-v">corr</span><span class="crayon-sy">.</span><span class="crayon-e">sort</span><span class="crayon-sy">(</span><span class="crayon-v">ascending</span><span class="crayon-o">=</span><span class="crayon-t">False</span><span class="crayon-sy">)</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33c4351193205-12"><span class="crayon-h">  </span><span class="crayon-v">features</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-v">corr</span><span class="crayon-sy">.</span><span class="crayon-v">index</span><span class="crayon-sy">[</span><span class="crayon-sy">[</span><span class="crayon-cn">0</span><span class="crayon-sy">,</span><span class="crayon-cn">1</span><span class="crayon-sy">]</span><span class="crayon-sy">]</span><span class="crayon-sy">.</span><span class="crayon-v">values</span></p><p class="crayon-line" id="crayon-56d5993cb33c4351193205-13"> </p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33c4351193205-14"><span class="crayon-h">  </span><span class="crayon-p"># Get training examples</span></p><p class="crayon-line" id="crayon-56d5993cb33c4351193205-15"><span class="crayon-h">  </span><span class="crayon-v">train_fold_input</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-v">train_fold</span><span class="crayon-sy">[</span><span class="crayon-v">features</span><span class="crayon-sy">]</span><span class="crayon-sy">.</span><span class="crayon-e">values</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33c4351193205-16"><span class="crayon-e">  </span><span class="crayon-v">train_fold_output</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-v">train_fold</span><span class="crayon-sy">[</span><span class="crayon-s">'target'</span><span class="crayon-sy">]</span></p><p class="crayon-line" id="crayon-56d5993cb33c4351193205-17"> </p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33c4351193205-18"><span class="crayon-h">  </span><span class="crayon-p"># Fit logistic regression</span></p><p class="crayon-line" id="crayon-56d5993cb33c4351193205-19"><span class="crayon-h">  </span><span class="crayon-v">logreg</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">LogisticRegression</span><span class="crayon-sy">(</span><span class="crayon-sy">)</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33c4351193205-20"><span class="crayon-h">  </span><span class="crayon-v">logreg</span><span class="crayon-sy">.</span><span class="crayon-e">fit</span><span class="crayon-sy">(</span><span class="crayon-v">train_fold_input</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">train_fold_output</span><span class="crayon-sy">)</span></p><p class="crayon-line" id="crayon-56d5993cb33c4351193205-21"> </p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33c4351193205-22"><span class="crayon-h">  </span><span class="crayon-p"># Check MSE on test set</span></p><p class="crayon-line" id="crayon-56d5993cb33c4351193205-23"><span class="crayon-h">  </span><span class="crayon-v">pred</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-v">logreg</span><span class="crayon-sy">.</span><span class="crayon-e">predict</span><span class="crayon-sy">(</span><span class="crayon-v">test_fold</span><span class="crayon-sy">[</span><span class="crayon-v">features</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33c4351193205-24"><span class="crayon-h">  </span><span class="crayon-v">mse</span><span class="crayon-sy">.</span><span class="crayon-e">append</span><span class="crayon-sy">(</span><span class="crayon-e">mean_squared_error</span><span class="crayon-sy">(</span><span class="crayon-v">test_fold</span><span class="crayon-sy">.</span><span class="crayon-v">target</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">pred</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span></p><p class="crayon-line" id="crayon-56d5993cb33c4351193205-25"> </p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33c4351193205-26"><span class="crayon-h">  </span><span class="crayon-p"># Done with the fold</span></p><p class="crayon-line" id="crayon-56d5993cb33c4351193205-27"><span class="crayon-h">  </span><span class="crayon-v">fold_count</span><span class="crayon-h"> </span><span class="crayon-o">+=</span><span class="crayon-h"> </span><span class="crayon-cn">1</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33c4351193205-28"> </p><p class="crayon-line" id="crayon-56d5993cb33c4351193205-29"><span class="crayon-e">print</span><span class="crayon-sy">(</span><span class="crayon-e">DataFrame</span><span class="crayon-sy">(</span><span class="crayon-v">mse</span><span class="crayon-sy">)</span><span class="crayon-sy">.</span><span class="crayon-e">mean</span><span class="crayon-sy">(</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33c4351193205-30"> </p><p class="crayon-line" id="crayon-56d5993cb33c4351193205-31"><span class="crayon-p"># Processing fold 0</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33c4351193205-32"><span class="crayon-p"># Processing fold 1</span></p><p class="crayon-line" id="crayon-56d5993cb33c4351193205-33"><span class="crayon-p"># Processing fold 2</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33c4351193205-34"><span class="crayon-p"># Processing fold 3</span></p><p class="crayon-line" id="crayon-56d5993cb33c4351193205-35"><span class="crayon-p"># Processing fold 4</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33c4351193205-36"><span class="crayon-p"># Processing fold 5</span></p><p class="crayon-line" id="crayon-56d5993cb33c4351193205-37"><span class="crayon-p"># Processing fold 6</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33c4351193205-38"><span class="crayon-p"># Processing fold 7</span></p><p class="crayon-line" id="crayon-56d5993cb33c4351193205-39"><span class="crayon-p"># Processing fold 8</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33c4351193205-40"><span class="crayon-p"># Processing fold 9</span></p><p class="crayon-line" id="crayon-56d5993cb33c4351193205-41"> </p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33c4351193205-42"><span class="crayon-e">DataFrame</span><span class="crayon-sy">(</span><span class="crayon-v">mse</span><span class="crayon-sy">)</span><span class="crayon-sy">.</span><span class="crayon-e">mean</span><span class="crayon-sy">(</span><span class="crayon-sy">)</span></p><p class="crayon-line" id="crayon-56d5993cb33c4351193205-43"><span class="crayon-p"># 0 0.441212</span></p><p class="crayon-line crayon-striped-line" id="crayon-56d5993cb33c4351193205-44"><span class="crayon-p"># dtype: float64</span></p></div></td>
					</tr>
				</table>
			</div>
		</div>

<p/>
<h2>Conclusion</h2>
<p>We have seen how doing features selection at the wrong step can terribly bias the <span class="MathJax_Preview"><img src="http://www.alfredo.motta.name/wp-content/plugins/latex/cache/tex_a9fc1a03386ae38b64e06c8172994963.gif" class="tex" alt="MSE"/></span> estimate of your machine learning algorithm. We have also seen how to correctly apply cross validation by simply moving one step down the features selection such that the knowledge from the test data does not leak in our learning procedure.</p>
<p>If you want to make sure you don't leak info across the train and test set scikit learn gives you additional extra tools  like the <a href="http://scikit-learn.org/stable/auto_examples/feature_selection/feature_selection_pipeline.html">feature selection pipeline</a><span id="easy-footnote-5" class="easy-footnote-margin-adjust"/><span class="easy-footnote"><a href="#easy-footnote-bottom-5" title="&lt;a href=&quot;http://scikit-learn.org/stable/auto_examples/feature_selection/feature_selection_pipeline.html&quot;&gt;Scikit-learn feature selection pipeline&lt;/a&gt;"><sup>5</sup></a></span> and the classes inside the <a href="http://scikit-learn.org/stable/modules/feature_selection.html">feature selection module</a><span id="easy-footnote-6" class="easy-footnote-margin-adjust"/><span class="easy-footnote"><a href="#easy-footnote-bottom-6" title="&lt;a href=&quot;http://scikit-learn.org/stable/modules/feature_selection.html&quot;&gt;Scikit-learn feature selection modules&lt;/a&gt;"><sup>6</sup></a></span>.</p>
<p>Finally, if you want know more about cross validation and its tradeoffs both R. Kohavi<span id="easy-footnote-7" class="easy-footnote-margin-adjust"/><span class="easy-footnote"><a href="#easy-footnote-bottom-7" title="&lt;a href=&quot;http://dl.acm.org/citation.cfm?id=1643047&quot;&gt;R. Kohavi. A study of cross-validation and bootstrap for accuracy estimation and model selection&lt;/a&gt;"><sup>7</sup></a></span> and Y. Bengio with Y. Grandvalet<span id="easy-footnote-8" class="easy-footnote-margin-adjust"/><span class="easy-footnote"><a href="#easy-footnote-bottom-8" title="&lt;a href=&quot;http://dl.acm.org/citation.cfm?id=1044695&quot;&gt;Y. Bengio and Y. Grandvalet. No unbiased estimator of the variance of k-fold cross-validation&lt;/a&gt;"><sup>8</sup></a></span> wrote on this topic.</p>
<p>If you liked this post you should consider <a href="https://twitter.com/mottalrd">following me</a> on twitter.<br/>
Let me know your comments!</p>
<h3>References</h3>
<ol class="easy-footnotes-wrapper"><li class="easy-footnote-single"><span id="easy-footnote-bottom-1" class="easy-footnote-margin-adjust"/><a href="https://www.youtube.com/watch?v=nZAM5OXrktY">Lecture 1 on cross validation - Statistical Learning @ Stanford</a><a class="easy-footnote-to-top" href="#easy-footnote-1"/></li><li class="easy-footnote-single"><span id="easy-footnote-bottom-2" class="easy-footnote-margin-adjust"/><a href="http://scikit-learn.org/stable/">Scikit-learn framework</a><a/><a class="easy-footnote-to-top" href="#easy-footnote-2"/></li><li class="easy-footnote-single"><span id="easy-footnote-bottom-3" class="easy-footnote-margin-adjust"/><a href="http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedKFold.html">Stratified KFold Documentation</a><a class="easy-footnote-to-top" href="#easy-footnote-3"/></li><li class="easy-footnote-single"><span id="easy-footnote-bottom-4" class="easy-footnote-margin-adjust"/><a href="https://www.youtube.com/watch?v=S06JpVoNaA0">Lecture 2 on cross validation - Statistical Learning @ Stanford</a><a class="easy-footnote-to-top" href="#easy-footnote-4"/></li><li class="easy-footnote-single"><span id="easy-footnote-bottom-5" class="easy-footnote-margin-adjust"/><a href="http://scikit-learn.org/stable/auto_examples/feature_selection/feature_selection_pipeline.html">Scikit-learn feature selection pipeline</a><a class="easy-footnote-to-top" href="#easy-footnote-5"/></li><li class="easy-footnote-single"><span id="easy-footnote-bottom-6" class="easy-footnote-margin-adjust"/><a href="http://scikit-learn.org/stable/modules/feature_selection.html">Scikit-learn feature selection modules</a><a class="easy-footnote-to-top" href="#easy-footnote-6"/></li><li class="easy-footnote-single"><span id="easy-footnote-bottom-7" class="easy-footnote-margin-adjust"/><a href="http://dl.acm.org/citation.cfm?id=1643047">R. Kohavi. A study of cross-validation and bootstrap for accuracy estimation and model selection</a><a class="easy-footnote-to-top" href="#easy-footnote-7"/></li><li class="easy-footnote-single"><span id="easy-footnote-bottom-8" class="easy-footnote-margin-adjust"/><a href="http://dl.acm.org/citation.cfm?id=1044695">Y. Bengio and Y. Grandvalet. No unbiased estimator of the variance of k-fold cross-validation</a><a class="easy-footnote-to-top" href="#easy-footnote-8"/></li></ol>			</div>

	</div></body></html>