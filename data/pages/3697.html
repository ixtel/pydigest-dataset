<html><body><div><div itemprop="articleBody" class="wd-jnl-art-full-text article-text">
<h2 class="header-anchor" id="csd509524s1" name="csd509524s1">1. Introduction</h2><div class="article-text" data-mobile-collapse=""><p>The Python language is growing in popularity as a language for scientific computing, thanks to syntax, a high level standard library and several scientific packages. The combination of a relatively efficient array package –<tt>numpy</tt>–, a library for scientific computing –<tt>scipy</tt>–, and enhanced interactive console –<tt>ipython</tt>–and a comprehensive 2D plotting package –<tt>matplotlib</tt>–  makes it a relevant candidate for many daily scientific tasks.</p><p>However, the overhead of running a scientific application written in Python compared to the same algorithm written in a statically compiled language such as C is high. It is partly due to the interpretation cost, but also to Python's dynamic binding. Additionally, the Python compiler does not perform any optimization on the byte code, while scientific applications are first-class candidates for many of them. The ratio between an efficient C application and its Python counterpart can range from a factor of two, when the Python code only calls C routine, to a factor of one hundred or more when the Python code makes explicit array accesses.</p><p>Given the increasing amount of data that scientists may need to process for their experiments, even two can be a limiting factor for the adoption of Python for computation intensive experiments. But since scientific applications are often told to spend 90% of their time in 10% of the code, one can focus on those computation-intensive pieces of code. The goal of an optimizing compiler may not be to optimize the full Python application, but rather a subset of the application.</p><p>Several tools have been developed by an active community to fill the performance gap encountered when running these computation-intensive piece of code, either through static compilation or just in time (JIT) compilation.</p><p>One approach, used by Cython [<a xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:encoder="xalan://java.net.URLEncoder" xmlns:stringescapeutils="xalan://org.apache.commons.lang.StringEscapeUtils" class="cite hastip" href="#csd509524bib2" id="fnref-csd509524bib2">2</a>] is to suppress the interpretation overhead by translating Python Programs to C programs calling the Python C API [<a xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:encoder="xalan://java.net.URLEncoder" xmlns:stringescapeutils="xalan://org.apache.commons.lang.StringEscapeUtils" class="cite hastip" href="#csd509524bib13" id="fnref-csd509524bib13">13</a>]. More recently, Nuitka [<a xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:encoder="xalan://java.net.URLEncoder" xmlns:stringescapeutils="xalan://org.apache.commons.lang.StringEscapeUtils" class="cite hastip" href="#csd509524bib8" id="fnref-csd509524bib8">8</a>] has taken the same approach using C++ as a back-end. Going a step further Cython also provides an hybrid C/Python language that can efficiently be translated into C code. It relies both on the Python C API and on plain C. ShedSkin [<a xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:encoder="xalan://java.net.URLEncoder" xmlns:stringescapeutils="xalan://org.apache.commons.lang.StringEscapeUtils" class="cite hastip" href="#csd509524bib4" id="fnref-csd509524bib4">4</a>] translates implicitly strongly typed Python programs into C++, without any call to the Python C API, but fails to compile dynamic codes or codes for which static type inference is impossible.</p><p>The alternate approach consists in writing a JIT compiler, embedded into the interpreter, to dynamically turn the computation intensive parts into native code. This is what the <tt>numexpr</tt> module [<a xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:encoder="xalan://java.net.URLEncoder" xmlns:stringescapeutils="xalan://org.apache.commons.lang.StringEscapeUtils" class="cite hastip" href="#csd509524bib10" id="fnref-csd509524bib10">10</a>] does for Numpy expressions, JIT-compiling them from a string representation to native code. Numba [<a xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:encoder="xalan://java.net.URLEncoder" xmlns:stringescapeutils="xalan://org.apache.commons.lang.StringEscapeUtils" class="cite hastip" href="#csd509524bib9" id="fnref-csd509524bib9">9</a>] and Parakeet [<a xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:encoder="xalan://java.net.URLEncoder" xmlns:stringescapeutils="xalan://org.apache.commons.lang.StringEscapeUtils" class="cite hastip" href="#csd509524bib12" id="fnref-csd509524bib12">12</a>] extend this approach to Numpy-centric applications while PyPy [<a xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:encoder="xalan://java.net.URLEncoder" xmlns:stringescapeutils="xalan://org.apache.commons.lang.StringEscapeUtils" class="cite hastip" href="#csd509524bib3" id="fnref-csd509524bib3">3</a>] applies it to the whole language—including the Numpy module through the in-progress <tt>numpypy</tt> branch.</p><p>Except for PyPy, none of these compilers apply any of the static optimization techniques that have been known and successfully applied for decades to statically compiled language such as C or C++. Translators to statically compiled languages do take advantage of them indirectly, but the quality of the generated code may prevent advanced optimizations. For instance GCC hardly vectorizes the output of Cython, while the array semantic is available at the Python level. Taking into account the specificities of the Python language can unlock many new transformations. For instance, PyPy automatically converts the <tt>range</tt> builtin into <tt>xrange</tt> by using a dedicated structure called <tt>range-list</tt>.</p><p>This article presents Pythran, an optimizing compiler for a subset of the Python language that turns implicitly statically typed modules into parametric C++ code. It supports many high-level constructs of the 2.7 version of the Python language such as list comprehension, set comprehension, dict comprehension, generator expression, lambda functions, nested functions, polymorphic functions and global variables. It does <strong>not</strong> support user classes or any dynamic feature such as introspection or polymorphic variables.</p><p>Unlike existing alternatives, Pythran does not solely perform static typing of Python programs. It also performs various compiler optimizations such as detection of pure functions, temporary allocation removal or constant folding. These transformations are backed up by code analyses such as aliasing, inter-procedural memory effect computations or use-def chains.</p><p>The article is structured as follows: section <a xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:encoder="xalan://java.net.URLEncoder" xmlns:stringescapeutils="xalan://org.apache.commons.lang.StringEscapeUtils" class="secref" href="#csd509524s2">2</a> introduces the Pythran compiler compilation flow and internal representation (IR). Section <a xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:encoder="xalan://java.net.URLEncoder" xmlns:stringescapeutils="xalan://org.apache.commons.lang.StringEscapeUtils" class="secref" href="#csd509524s3">3</a> presents several code analyses while section <a xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:encoder="xalan://java.net.URLEncoder" xmlns:stringescapeutils="xalan://org.apache.commons.lang.StringEscapeUtils" class="secref" href="#csd509524s4">4</a> focuses on code optimizations. Section <a xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:encoder="xalan://java.net.URLEncoder" xmlns:stringescapeutils="xalan://org.apache.commons.lang.StringEscapeUtils" class="secref" href="#csd509524s5">5</a> presents back-end optimizations for Numpy expressions. Section <a xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:encoder="xalan://java.net.URLEncoder" xmlns:stringescapeutils="xalan://org.apache.commons.lang.StringEscapeUtils" class="secref" href="#csd509524s6">6</a> briefly introduces OpenMP-like annotations for explicit parallelization of Python programs and section <a xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:encoder="xalan://java.net.URLEncoder" xmlns:stringescapeutils="xalan://org.apache.commons.lang.StringEscapeUtils" class="secref" href="#csd509524s7">7</a> presents the performance obtained on a few synthetic benchmarks and concludes.</p></div>
<h2 class="header-anchor" id="csd509524s2" name="csd509524s2">2. Pythran compiler infrastructure</h2><div class="article-text" data-mobile-collapse=""><p>Pythran is a compiler for a subset of the Python language. In this paper, the name <strong>Pythran</strong> will be used indifferently to refer to the language or the associated compiler. The input of the Pythran compiler is a Python module—not a Python program—meant to be turned into a native module. Typically, computation-intensive parts of the program are moved to a module fed to Pythran. The module is then translated to polymorphic C++ code through the generation of templated classes in place of Python functions. These classes are either used from regular C++ applications, or instantiated for given types to generate a native Python module.</p><p>It is a strong requirement for Pythran to maintain backward compatibility with Python, that is, unlike Cython, all code compilable by Pythran is still executable by a standard-conforming Python interpreter. For this reason, in addition to language restrictions detailed in the following, Pythran understands special comments such as:</p><p># <tt>pythran export foo(int list, float)</tt>
</p><p>as an optional module signature. One does not need to list all the module functions in an <tt>export</tt> directive, only the functions meant to be used outside of the module, i.e imported by another module. Polymorphic functions can be listed several times with different types. These type annotations are only used for native code generation.</p><p>The Pythran compiler is built as a traditional static compiler: a front-end turns Python code into an IR, a middle-end performs various code optimizations on this IR, and a back-end turns the IR into C++ code. The front-end performs two steps:</p><div class="listBox"><table border="0" cellpadding="0" cellspacing="0"><tbody><tr valign="top"><td class="first-col">(1)  </td><td align="left">turn Python code into Python Abstract Syntax Tree (AST) by using the <tt>ast</tt> module from the standard library;</td></tr><tr valign="top"><td class="first-col">(2)  </td><td align="left">turn the Python AST into a type-agnostic Pythran IR, which remains a subset of the Python AST.</td></tr></tbody></table></div><p>
</p><p>Pythran IR is similar to the Python AST, as defined in the <tt>ast</tt> module, except that several nodes are forbidden (for instance because Pythran does not support user-defined classes, or the <tt>exec</tt> instruction), and some nodes are converted to others to form a simpler and easier to deal with AST for further analyses and optimizations. The transformations applied by Pythran on Python AST are the following:</p><ul>
<li>list/set/dict comprehension are expanded into loops wrapped into a function call;</li>
<li>destructuring is expanded into several variable assignments;</li>
<li>lambda functions are turned into named nested functions;</li>
<li>the closure of nested functions is statically computed to turn nested functions into global functions taking the closure elements as parameters. The initial definition is replaced by a <em>partial function</em> creation, using the <tt>partial</tt> type from the standard <tt>functools</tt> module;</li>
<li>implicit <tt>return None</tt> are made explicit;</li>
<li>all imports are fully expanded to make functions and global variables access paths explicit,</li>
<li>method calls are turned into function calls;</li>
<li>implicit __<tt>builtin</tt>__ function calls are made explicit;</li>
<li>
<tt>try ... finally</tt> instructions are turned into nested <tt>try ... except</tt> instructions;</li>
<li>identifiers whose name may be in conflict with C++ keywords are renamed.</li>
</ul><p>
</p><p>The back-end operates in three steps:</p><div class="listBox"><table border="0" cellpadding="0" cellspacing="0"><tbody><tr valign="top"><td class="first-col">(1)  </td><td align="left">turning Pythran IR into parametric C++ code;</td></tr><tr valign="top"><td class="first-col">(2)  </td><td align="left">instantiating the C++ code for the desired types;</td></tr><tr valign="top"><td class="first-col">(3)  </td><td align="left">compiling the generated C++ code into native code.</td></tr></tbody></table></div><p>
</p><p>The first step requires to map polymorphic variables and polymorphic functions from the Python world to C++. Pythran only supports polymorphic variables for functions, i.e. a variable can hold several function objects during its lifetime, but it cannot be assigned to a string if it has already been assigned to an integer. As shown later, it is possible to detect several false variable polymorphism cases using use-def chains. Function polymorphism is achieved through template parameters: a template function can be applied to several types as long as an implicit structural typing is respected. This technique is very similar to Python's duck typing, except that it is checked at compile time, as illustrated by the following implementation of a generic dot product in Python:</p><p>
</p><div class="boxout boxout-bdr-grey"><a class="table" name="" id=""/><table cellpadding="0" cellspacing="0" border="0" data-toolbar-link="" data-toolbar-img="/1749-4699/8/1/014001/suppdata/" data-toolbar-type="table"><colgroup><col align="left"/></colgroup><tbody>
<tr valign="top">
<td colspan="1" rowspan="1" align="left"><strong>def</strong> dot(L0, L1):</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  <strong>return</strong> <tt>sum(x*y</tt> <strong>for</strong> <tt>x,y</tt> <strong>in</strong> <tt>zip(L0,L1))</tt>
</td>
</tr>
</tbody></table></div>
<p>and in C++:</p><p>
</p><div class="boxout boxout-bdr-grey"><a class="table" name="" id=""/><table cellpadding="0" cellspacing="0" border="0" data-toolbar-link="" data-toolbar-img="/1749-4699/8/1/014001/suppdata/" data-toolbar-type="table"><colgroup><col align="left"/></colgroup><tbody>
<tr valign="top">
<td colspan="1" rowspan="1" align="left"><strong>template</strong> <span xmlns:xlink="http://www.w3.org/1999/xlink" class="inline-eqn"><span class="tex"><span class="texImage"><img src="http://cdn.iopscience.com/images/1749-4699/8/1/014001/Full/csd509524ieqn1.gif" alt="" align="top"/></span></span></span> <strong>class</strong> <tt>T0</tt>, <strong>class</strong> <tt>T1</tt> <span xmlns:xlink="http://www.w3.org/1999/xlink" class="inline-eqn"><span class="tex"><span class="texImage"><img src="http://cdn.iopscience.com/images/1749-4699/8/1/014001/Full/csd509524ieqn2.gif" alt="" align="top"/></span></span></span>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  <strong>auto</strong>   <tt>dot</tt>(<tt>T0</tt>&amp;&amp; <tt>L0</tt>, <tt>T1</tt>&amp;&amp; <tt>L1</tt>) <span xmlns:xlink="http://www.w3.org/1999/xlink" class="inline-eqn"><span class="tex"><span class="texImage"><img src="http://cdn.iopscience.com/images/1749-4699/8/1/014001/Full/csd509524ieqn3.gif" alt="" align="top"/></span></span></span> <tt>decltype(/* skipped */)</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  {</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">     <strong>return</strong> <tt>pythonic::sum</tt>(</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">       <tt>pythonic::map</tt>(</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">        <tt>operator</tt>_<tt>::multiply</tt>(),</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">        <tt>pythonic::zip</tt>(</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">         <tt>std::forward</tt> <span xmlns:xlink="http://www.w3.org/1999/xlink" class="inline-eqn"><span class="tex"><span class="texImage"><img src="http://cdn.iopscience.com/images/1749-4699/8/1/014001/Full/csd509524ieqn4.gif" alt="" align="top"/></span></span></span>
<tt>T0</tt> <span xmlns:xlink="http://www.w3.org/1999/xlink" class="inline-eqn"><span class="tex"><span class="texImage"><img src="http://cdn.iopscience.com/images/1749-4699/8/1/014001/Full/csd509524ieqn5.gif" alt="" align="top"/></span></span></span>
<tt>(L0)</tt>,</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">         <tt>std::forward</tt> <span xmlns:xlink="http://www.w3.org/1999/xlink" class="inline-eqn"><span class="tex"><span class="texImage"><img src="http://cdn.iopscience.com/images/1749-4699/8/1/014001/Full/csd509524ieqn6.gif" alt="" align="top"/></span></span></span>
<tt>T1</tt> <span xmlns:xlink="http://www.w3.org/1999/xlink" class="inline-eqn"><span class="tex"><span class="texImage"><img src="http://cdn.iopscience.com/images/1749-4699/8/1/014001/Full/csd509524ieqn7.gif" alt="" align="top"/></span></span></span>
<tt>(L1)</tt>)</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">      )</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">    );</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  }</td>
</tr>
</tbody></table></div>
<p>Although more verbose than the Python version, the C++ version also uses some form of structural typing: the only assumptions these two versions make are that <tt>L0</tt> and <tt>L1</tt> are iterable, their content can be multiplied and that the result of the multiplication is accumulatable.</p><p>Finally, Pythran computes the set of functions and types used for the processed module, and generates the minimal set of headers that provide the definitions needed to compile the generated C++ code. This step is critical to prevent typical slowdown during C++ compilation.</p><p>The second step only consists in the instantiation of the top-level functions of the module, using user-provided signatures. Template instantiation then triggers the different correctly typed instantiations for all functions called in the module. Note that the user only needs to provide the type of the functions exported outside the module. The possible types of all internal functions are then inferred from the call sites. This complex step leverage the C++11 compiler capabilities: Pythran generates only one line using user-provided signature to trigger the template instanciation by the C++ compiler.</p><p>The last step involves a template library, called <tt>pythonic</tt> that contains a polymorphic implementation of many functions of the Python standard library in the form of C++ template functions. Several optimizations, most notably using expression templates, are delegated to this library. Pythran relies on the C++11 [<a xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:encoder="xalan://java.net.URLEncoder" xmlns:stringescapeutils="xalan://org.apache.commons.lang.StringEscapeUtils" class="cite hastip" href="#csd509524bib7" id="fnref-csd509524bib7">7</a>] language and leverages recent features such as move semantics, type inference through <tt>decltype(...)</tt> and variadic templates. As a consequence it requires a C++11 compatible compiler for the native code generation. Boost.Python [<a xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:encoder="xalan://java.net.URLEncoder" xmlns:stringescapeutils="xalan://org.apache.commons.lang.StringEscapeUtils" class="cite hastip" href="#csd509524bib1" id="fnref-csd509524bib1">1</a>] is used for the automatic Python-to-C++ boilerplate. Generated C++ code is compatible starting with g++ 4.8.2 and clang++ 3.3.</p><p>It is important to note that all Pythran analyses and optimizations are type-agnostic, i.e. they do not assume any type for the variables manipulated by the program. The specialization of the generated code occurs only by generating the template instanciation. In other words, the Pythran compiler analyzes polymorphic functions and turns Python modules into C++ meta-programs.</p><p>Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#csd509524f1">1</a> summarizes the compilation flow and the involved tools.</p><figure xmlns:xlink="http://www.w3.org/1999/xlink" role="group" class="boxout boxout-bdr-grey" data-toolbar-type="figure" data-toolbar-link="csd509524f1" data-toolbar-img="http://cdn.iopscience.com/images/1749-4699/8/1/014001/Full/csd509524f1_online.jpg"><a name="csd509524f1" id="csd509524f1"/><figure><div class="panzoom-container"><p class="buttons zoom-tools"><button class="zoom-in"><span class="icon-zoomin"/>
	                 Zoom In
				</button><button class="zoom-out"><span class="icon-zoomout"/>
	                 Zoom Out
				</button><span class="mobile-block"><button class="reset"><span class="icon-loop"/>
	                     Reset image size
					</button></span></p></div><figcaption><div class="article-text figure-caption"><p><strong>Figure 1.</strong> Pythran compilation flow.</p></div><p class="mb-05 print-hide">
           		Download figure:
				<span class="btn-multi-block"><a class="btn btn-primary fig-dwnld-std-img" id="wd-jnl-art-btn-std-img-csd509524f1" href="/1749-4699/8/1/014001/downloadFigure/figure/csd509524f1"><span class="icon-image"/>
		                     Standard
						</a><a class="btn btn-primary fig-dwnld-hi-img" id="wd-jnl-art-btn-hires-img-csd509524f1" href="/1749-4699/8/1/014001/downloadHRFigure/figure/csd509524f1"><span class="icon-image"/>
		                     High-resolution
						</a><a class="btn btn-primary fig-dwnld-ppoint" id="wd-jnl-art-btn-ppt-csd509524f1" href="/1749-4699/8/1/014001/powerpoint/figure/csd509524f1"><span class="icon-file-powerpoint"/>
								 Export PowerPoint slide
						</a></span></p></figcaption></figure></figure></div>
<h2 class="header-anchor" id="csd509524s3" name="csd509524s3">3. Code analyses</h2><div class="article-text" data-mobile-collapse=""><p>A code analysis is a function that takes a part of the IR (or the whole module's IR) as an input and returns aggregated high-level information. For instance, a simple Pythran analysis called <tt>Identifiers</tt> gathers the set of all identifiers used throughout the program. This information is later used when creating new identifiers so that no conflict occurs with already existing ones.</p><p>One of the most important analysis in Pythran is the <strong>alias analysis</strong>, sometimes referred as <strong>points-to</strong> analysis. For each identifier, it computes an approximation of the set of locations this identifier may points to. For instance, let us consider the polymorphic function <tt>foo</tt> defined as follows:</p><p>
</p><div class="boxout boxout-bdr-grey"><a class="table" name="" id=""/><table cellpadding="0" cellspacing="0" border="0" data-toolbar-link="" data-toolbar-img="/1749-4699/8/1/014001/suppdata/" data-toolbar-type="table"><colgroup><col align="left"/></colgroup><tbody>
<tr valign="top">
<td colspan="1" rowspan="1" align="left"><strong>def</strong> <tt>foo(a,b)</tt>:</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  c = <tt>a</tt> <strong>or</strong> <tt>b</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  <strong>return</strong> <tt>c * 2</tt>
</td>
</tr>
</tbody></table></div>
<p>The identifier <tt>c</tt> involved in the multiplication may refer to</p><ul>
<li>a fresh location if <tt>a</tt> and <tt>b</tt> are scalars</li>
<li>the same location as <tt>a</tt> if <tt>a</tt> evaluates to <tt>True</tt>
</li>
<li>the same location as <tt>b</tt> otherwise.</li>
</ul><p>
</p><p>As we do not specialize the analysis for different types and the truth value of <tt>a</tt> is unknown at compilation time, the alias analysis yields the approximated result that <tt>c</tt> may point to a fresh location, <tt>a</tt> or <tt>b</tt>.</p><p>Without this kind of information, even a simple instruction like <tt>sum(a)</tt> would yield very few information as there is no guarantee that the <tt>sum</tt> identifiers point to the <tt>sum</tt> built-in, as it could have been previously stated that <tt>sum=len</tt>.</p><p>When turning Python AST into Pythran IR, nested functions are turned into global functions taking their expanded closure as parameter. This closure is computed using the information provided by the <tt>Globals</tt> and <tt>ImportedIds</tt> analysis. The former statically computes the state of the dictionary of globals, and the latter computes the set of identifiers used by an instruction but not declared in this instruction. For instance in the following snippet:</p><p>
</p><div class="boxout boxout-bdr-grey"><a class="table" name="" id=""/><table cellpadding="0" cellspacing="0" border="0" data-toolbar-link="" data-toolbar-img="/1749-4699/8/1/014001/suppdata/" data-toolbar-type="table"><colgroup><col align="left"/></colgroup><tbody>
<tr valign="top">
<td colspan="1" rowspan="1" align="left"><strong>def</strong> <tt>outer(outer</tt>_<tt>argument)</tt>:</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  <strong>def</strong> <tt>inner(inner</tt>_<tt>argument)</tt>:</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">    <strong>return</strong> <tt>cos(outer</tt>_<tt>argument) + inner</tt>_<tt>argument</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  <strong>return</strong> <tt>inner(3)</tt>
</td>
</tr>
</tbody></table></div>
<p>The <tt>Globals</tt> analysis called on the <tt>inner</tt> function definition marks <tt>cos</tt> as a global variable, and <tt>ImportedIds</tt> marks <tt>outer</tt>_<tt>argument</tt> and <tt>cos</tt> as imported identifiers. The result of the transformation of nested function into global functions on that particular example is:</p><p>
</p><div class="boxout boxout-bdr-grey"><a class="table" name="" id=""/><table cellpadding="0" cellspacing="0" border="0" data-toolbar-link="" data-toolbar-img="/1749-4699/8/1/014001/suppdata/" data-toolbar-type="table"><colgroup><col align="left"/></colgroup><tbody>
<tr valign="top">
<td colspan="1" rowspan="1" align="left"><strong>import</strong> <tt>functools</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left"> </td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left"><strong>def</strong> _<tt>inner(outer</tt>_<tt>argument, inner</tt>_<tt>argument):</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  <strong>return</strong> <tt>cos(outer</tt>_<tt>argument) + inner</tt>_<tt>argument</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left"> </td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left"><strong>def</strong> <tt>outer(outer</tt>_<tt>argument):</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  <tt>inner</tt> <tt>=</tt> <tt>functools.partial(</tt>_<tt>inner, outer</tt>_<tt>argument)</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  <strong>return</strong> <tt>inner(3)</tt>
</td>
</tr>
</tbody></table></div>
<p>A rather high-level analysis is the <tt>PureFunctions</tt> analysis, that computes the set of functions declared in the module that are pure, i.e. whose return value only depends from the value of their arguments and do not change any global state. This analysis depends on two other analyses, namely <tt>GlobalEffects</tt> that computes for each function whether it modifies the global state (including I/O, random generators, etc) and <tt>ArgumentEffects</tt> that computes for each argument of each function whether or not this argument may be updated in the function body. These three analyses work inter-procedurally, as illustrated in the following example:</p><p>
</p><div class="boxout boxout-bdr-grey"><a class="table" name="" id=""/><table cellpadding="0" cellspacing="0" border="0" data-toolbar-link="" data-toolbar-img="/1749-4699/8/1/014001/suppdata/" data-toolbar-type="table"><colgroup><col align="left"/></colgroup><tbody>
<tr valign="top">
<td colspan="1" rowspan="1" align="left"><strong>def</strong>  <tt>fibo(n):</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  <strong>return</strong> <tt>n</tt> <strong>if</strong> <tt>n</tt> <span xmlns:xlink="http://www.w3.org/1999/xlink" class="inline-eqn"><span class="tex"><span class="texImage"><img src="http://cdn.iopscience.com/images/1749-4699/8/1/014001/Full/csd509524ieqn8.gif" alt="" align="top"/></span></span></span> <tt>2</tt> <strong>else</strong> <tt>fibo(n --- 1) + fibo(n --- 2)</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left"> </td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left"><strong>def</strong>  <tt>bar(l):</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  <strong>return</strong> <tt>map(fibo, l)</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left"> </td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left"><strong>def</strong>  <tt>foo(l):</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  <strong>return</strong> <tt>map(fibo, random.sample(l, 3))</tt>
</td>
</tr>
</tbody></table></div>
<p>The <tt>fibo</tt> function is pure as it has no global effect or argument effect and only calls itself. As consequence the <tt>bar</tt> function is also pure since the <tt>map</tt> intrinsic is pure when its first argument is pure. However the <tt>foo</tt> function is not pure since it calls the <tt>sample</tt> function from the <tt>random</tt> module, which has a global effect (on the underlying random number generator internal state).</p><p>Several analyses depend on the <tt>PureFunctions</tt> analysis. <tt>ParallelMaps</tt> uses aliasing information to check if an identifier points to the <tt>map</tt> intrinsic, and checks if the first argument is a pure function using <tt>PureFunctions</tt>. In that case the <tt>map</tt> is added to the set of parallel maps, because it can be executed in any order. This is the case for the first <tt>map</tt> in the following snippet, but not for the second because <tt>print b</tt> involves an <strong>I/O</strong>.</p><p>
</p><div class="boxout boxout-bdr-grey"><a class="table" name="" id=""/><table cellpadding="0" cellspacing="0" border="0" data-toolbar-link="" data-toolbar-img="/1749-4699/8/1/014001/suppdata/" data-toolbar-type="table"><colgroup><col align="left"/></colgroup><tbody>
<tr valign="top">
<td colspan="1" rowspan="1" align="left"><strong>def</strong>  <tt>pure(a):</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  <strong>return</strong> <tt>a</tt> ** <tt>2</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left"><strong>def</strong>  <tt>guilty(a):</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  <tt>b=pure(a)</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  <strong>print</strong> <tt>b</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  <strong>return</strong> <tt>b</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left"> </td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">
<tt>l=list(...)</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">
<tt>map(pure, l)</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">
<tt>map(guilty, l)</tt>
</td>
</tr>
</tbody></table></div>
<p>
<tt>ConstantExpressions</tt> uses function purity to decide whether or not a given expression is constant, i.e. its value only depends on literals. For instance the expression <tt>fibo(12)</tt> is a constant expression because <tt>fibo</tt> is pure and its argument is a literal.</p><p>
<tt>UseDefChains</tt> is a classical analysis from the static compilation world. For each variable defined in a function, it computes the chain of <strong>use</strong> and <strong>def</strong>. The result can be used to drive various code transformations, for instance to remove dead code, such as a <strong>def</strong> followed by either nothing or a <strong>def</strong> is useless. It is used in Pythran to avoid false polymorphism. An intuitive way to represent use-def chains is illustrated on the next code snippet:</p><p>
</p><div class="boxout boxout-bdr-grey"><a class="table" name="" id=""/><table cellpadding="0" cellspacing="0" border="0" data-toolbar-link="" data-toolbar-img="/1749-4699/8/1/014001/suppdata/" data-toolbar-type="table"><colgroup><col align="left"/></colgroup><tbody>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">
<tt>a</tt> <tt>=</tt> <tt>1</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left"><strong>if</strong>   <tt>cond:</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">   <tt>a</tt> <tt>=</tt> <tt>a + 2</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left"><strong>else</strong> :</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">   <tt>a</tt> <tt>=</tt> <tt>3</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left"><strong>print</strong> <tt>a</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">
<tt>a</tt> <tt>=</tt> <tt>4</tt>
</td>
</tr>
</tbody></table></div>
<p>In this example, there are two possible chains starting from the first assignment. Using <tt>U</tt> to denote <strong>use</strong> and <tt>D</tt> to denote <strong>def</strong>, one gets: </p><p>
</p><p>The fact that all paths finish by a <strong>def</strong> indicates that the last assignment can be removed (but not necessarily its right-hand part that could have a side-effect). In the use-def Chains, we can gather all nodes tied with a <strong>use</strong> node or preceded by a common <strong>def</strong> node. All these nodes refere to the same variable and can safely be renamed from <tt>a</tt> to <tt>a</tt>' (thus performing scalar renaming). It avoid false polymorphism.</p><p>All the above analyses are used by Pythran developers to build code transformations that improve the execution time of the generated code.</p></div>
<h2 class="header-anchor" id="csd509524s4" name="csd509524s4">4. Code optimizations</h2><div class="article-text" data-mobile-collapse=""><p>One of the benefits of translating Python code to C++ code is that it removes most of the dynamic lookups. It also unveils all the optimizations available at C++ level. For instance, a function call is quite costly in Python, which advocates in favor of using inlining. This transformation comes at no cost when using C++ as the back-end language, as the C++ compiler performs this transformation triggered by fine tuned heuristics.</p><p>However, there are some information available at the Python level that cannot be recovered at the C++ level. For instance, Pythran uses functor with an internal state and a goto dispatch table to represent generators. This approach is not very efficient, especially for trivial cases. Such trivial cases appear when a generator expression is converted, in the front-end, to a looping generator. To avoid this extra cost, Pythran turns generator expressions into call to <tt>imap</tt> and <tt>ifilter</tt> from the <tt>itertools</tt> module whenever possible, removing the unnecessary goto dispatching table. This kind of transformation cannot be made by the C++ compiler. For instance, the one-liner <tt>len(set(vec[i]i + i for i in cols))</tt> extracted from the <tt>nqueens</tt> benchmarks from the Unladen Swallow project is rewritten as <tt>len(set(itertools.imap(lambda i: vec[i] + i,cols)))</tt>. This new form is less efficient in pure Python (it implies one extra function call per iteration), but can be compiled into C++ more efficiently than a generic generator.</p><p>A similar optimization consists in turning <tt>map</tt>, <tt>zip</tt> or <tt>filter</tt> into their equivalent version from the <tt>itertool</tt> module. The benefit is twofold: first it removes a temporary allocation, second it gives an opportunity to the compiler to replace list accesses by scalar accesses. This transformation is not always valid, nor profitable. It is not valid if the content of the output list is written later on, and not profitable if the content of the output list is read several times, as each read implies the (re) computation, as illustrated in the following code:</p><p>
</p><div class="boxout boxout-bdr-grey"><a class="table" name="" id=""/><table cellpadding="0" cellspacing="0" border="0" data-toolbar-link="" data-toolbar-img="/1749-4699/8/1/014001/suppdata/" data-toolbar-type="table"><colgroup><col align="left"/></colgroup><tbody>
<tr valign="top">
<td colspan="1" rowspan="1" align="left"><strong>def</strong>  <tt>valid</tt>_<tt>conversion(n):</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  # <tt>this map can be converted to imap</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  <tt>L</tt> <tt>=</tt> <tt>map(math.cos, range(n))</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  <strong>return</strong> <tt>sum(L) # sum iterates once on its input</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left"> </td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left"><strong>def</strong>  <tt>invalid</tt>_<tt>conversion(n):</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  # <tt>this map cannot be converted to imap</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  <tt>L</tt> <tt>=</tt> <tt>map(math.cos, range(n))</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  <tt>L[0]</tt> <tt>=</tt> <tt>1 # invalid assignment if converted to imap</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  <strong>return</strong> <tt>sum(L) + max(L) # two iterations over L prevents imap transformation</tt>
</td>
</tr>
</tbody></table></div>
<p>The information regarding constant expressions is used to perform a classical transformation called <tt>ConstantFolding</tt>, which consists in the compile-time evaluation of constant expressions. The validity is guaranteed by the <tt>ConstantExpressions</tt> analysis, and the evaluation relies on Python ability to compile an AST into byte code and run it, benefiting from the fact that Pythran IR is a subset of Python AST. A typical illustration is the initialization of a cache at compile-time:</p><p>
</p><div class="boxout boxout-bdr-grey"><a class="table" name="" id=""/><table cellpadding="0" cellspacing="0" border="0" data-toolbar-link="" data-toolbar-img="/1749-4699/8/1/014001/suppdata/" data-toolbar-type="table"><colgroup><col align="left"/></colgroup><tbody>
<tr valign="top">
<td colspan="1" rowspan="1" align="left"><strong>def</strong>  <tt>esieve(n):</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  <tt>candidates=range(2, n+1)</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  <strong>return</strong> <tt>sorted</tt>(</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">    <tt>set(candidates) - set(p</tt>*<tt>i</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">                <strong>for</strong> <tt>p</tt> <strong>in</strong> <tt>candidates</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">                <strong>for</strong> <tt>i</tt> <strong>in</strong> <tt>range(p, n+1)</tt>)</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">    )</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left"> </td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">
<tt>cache</tt> <tt>=</tt> <tt>esieve(100)</tt>
</td>
</tr>
</tbody></table></div>
<p>Pythran automatically detects that <tt>eseive</tt> is a pure function and evaluates the <tt>cache</tt> variable value at compile time.</p><p>
<tt>ConstantFolding</tt> may create a lot of candidates for <tt>LoopUnrolling</tt>. In Python, loops are used to iterate over sequences. If such a sequence is an explicit list, then Pythran unrolls the loop accordingly. Thanks to <tt>ConstantFolding</tt>, a call to <tt>range(4)</tt> will statically turn into <tt>[0, 1, 2, 3]</tt> (after a check that <tt>range</tt> actually points to __<tt>builtin</tt>__<tt>.range</tt>) making it possible to unroll the loop statically.</p><p>Sometimes, developers use the same variable in a function to represent value with different types, which leads to false polymorphism, as in:</p><p>
</p>
<p>These instructions cannot be translated to C++ directly because <tt>a</tt> would have both <tt>double</tt> and <tt>str</tt> type. However, using <tt>UseDefChains</tt> it is possible to assert the validity of the renaming of the instructions into:</p><p>
</p>
<p>that does not have the same typing issue.</p><p>In addition to these Python-level optimizations, the Pythran back end library, <tt>pythonic</tt>, uses several well known optimizations, especially for Numpy expressions.</p></div>
<h2 class="header-anchor" id="csd509524s5" name="csd509524s5">5. Library level optimizations</h2><div class="article-text" data-mobile-collapse=""><p>Using the proper library, the <span class="small-caps">C++</span> language provides an abstraction level close to what Python proposes. Pythran provides a wrapper library, <tt>pythonic</tt>, that leverage on the <span class="small-caps">C++</span> Standard Template Library (STL), the GNU Multiple Precision Arithmetic Library (GMP) and the Numerical Template Toolbox (NT2)<sup xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:encoder="xalan://java.net.URLEncoder" xmlns:stringescapeutils="xalan://org.apache.commons.lang.StringEscapeUtils" class="fnref"><a class="fnref" href="#csd509524fn1">1</a></sup>
 to emulate Python standard library. The STL is used to provide a typed version of the standard containers (<tt>list</tt>, <tt>set</tt>, <tt>dict</tt> and <tt>str</tt>), as well as reference-based memory management through <tt>shared</tt>_<tt>ptr</tt>. Generic algorithms such as <tt>accumulate</tt> are used when possible. GMP is the natural choice to represent Python's <tt>long</tt> in <span class="small-caps">C++</span>. NT2 provides a generic vector library called <tt>boost.simd</tt> [<a xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:encoder="xalan://java.net.URLEncoder" xmlns:stringescapeutils="xalan://org.apache.commons.lang.StringEscapeUtils" class="cite hastip" href="#csd509524bib5" id="fnref-csd509524bib5">5</a>] that enables the use of vector instruction units of modern processors in a generic way. It is used to efficiently compile Numpy expressions.</p><p>Numpy expressions are the perfect candidates for library level optimizations. Pythran implements three optimizations on such expressions:</p><div class="listBox"><table border="0" cellpadding="0" cellspacing="0"><tbody><tr valign="top"><td class="first-col">(1)  </td><td align="left">Expression templates [<a xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:encoder="xalan://java.net.URLEncoder" xmlns:stringescapeutils="xalan://org.apache.commons.lang.StringEscapeUtils" class="cite hastip" href="#csd509524bib14" id="fnref-csd509524bib14">14</a>] are used to avoid multiple iterations and the creation of intermediate arrays. Because they aggregate all <tt>ufunc</tt> into a single expression at compile time, they also increase the computation intensity of the loop body, and hence increase the impact of the two following optimizations.</td></tr><tr valign="top"><td class="first-col">(2)  </td><td align="left">Loop vectorization. All modern processors have vector instruction units capable of applying the same operation on a vector of data instead of a single data. For instance Intel Sandy Bridge can run eight single-precision additions per instruction. One can directly use the vector instruction set assembly to use these vector units, or use C/C++ intrinsics. Pythran relies on <tt>boost.simd</tt> from NT2 that offers a generic vector implementation of all standard math functions to generate a vectorized version of Numpy expressions. Again, the aggregation of operators performed by the expression templates proves to be beneficial, as it reduces the number of (costly) loads from the main memory to the vector unit.</td></tr><tr valign="top"><td class="first-col">(3)  </td><td align="left">Loop parallelization through OpenMP [<a xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:encoder="xalan://java.net.URLEncoder" xmlns:stringescapeutils="xalan://org.apache.commons.lang.StringEscapeUtils" class="cite hastip" href="#csd509524bib11" id="fnref-csd509524bib11">11</a>]. Numpy expression computation do not carry any loop-dependency. They are perfect candidates for loop parallelization, especially after the expression templates aggregation, as OpenMP generally performs better on loops with higher computation intensity that masks the scheduling overhead.</td></tr></tbody></table></div><p>
</p><p>To illustrate the benefits of these three optimizations combined, let us consider the simple Numpy expression:</p><p>
<tt>d</tt> <tt>=</tt> <tt>numpy.sqrt(b*b+c*c)</tt>
</p><p>When benchmarked with the <tt>timeit</tt> module on an hyper-threaded quad-core i7, the pure Python execution yields:</p><p>
</p><div class="boxout boxout-bdr-grey"><a class="table" name="" id=""/><table cellpadding="0" cellspacing="0" border="0" data-toolbar-link="" data-toolbar-img="/1749-4699/8/1/014001/suppdata/" data-toolbar-type="table"><colgroup><col align="left"/></colgroup><tbody>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">
<span xmlns:xlink="http://www.w3.org/1999/xlink" class="inline-eqn"><span class="tex"><span class="texImage"><img src="http://cdn.iopscience.com/images/1749-4699/8/1/014001/Full/csd509524ieqn9.gif" alt="" align="top"/></span></span></span> % <tt>timeit np.sqrt(b*b+c*c)</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">
<tt>1000 loops, best of 3: 1.23 ms per loop</tt>
</td>
</tr>
</tbody></table></div>
<p>then after Pythran processing and using expression templates:</p><p>
</p><div class="boxout boxout-bdr-grey"><a class="table" name="" id=""/><table cellpadding="0" cellspacing="0" border="0" data-toolbar-link="" data-toolbar-img="/1749-4699/8/1/014001/suppdata/" data-toolbar-type="table"><colgroup><col align="left"/></colgroup><tbody>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">
<span xmlns:xlink="http://www.w3.org/1999/xlink" class="inline-eqn"><span class="tex"><span class="texImage"><img src="http://cdn.iopscience.com/images/1749-4699/8/1/014001/Full/csd509524ieqn10.gif" alt="" align="top"/></span></span></span> % <tt>timeit my.pythranized(b,c)</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">
<tt>1000 loops, best of 3: 621 us per loop</tt>
</td>
</tr>
</tbody></table></div>
<p>The speed-up is due to the fact that expression templates replace four temporary array creations and four loops by a single allocation and a single loop.</p><p>Going a step further and vectorizing the generated loop yields an extra performance boost:</p><p>
</p><div class="boxout boxout-bdr-grey"><a class="table" name="" id=""/><table cellpadding="0" cellspacing="0" border="0" data-toolbar-link="" data-toolbar-img="/1749-4699/8/1/014001/suppdata/" data-toolbar-type="table"><colgroup><col align="left"/></colgroup><tbody>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">
<span xmlns:xlink="http://www.w3.org/1999/xlink" class="inline-eqn"><span class="tex"><span class="texImage"><img src="http://cdn.iopscience.com/images/1749-4699/8/1/014001/Full/csd509524ieqn11.gif" alt="" align="top"/></span></span></span> % <tt>timeit my.pythranized(b,c)</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">
<tt>1000 loops, best of 3: 418 us per loop</tt>
</td>
</tr>
</tbody></table></div>
<p>Although the AVX instruction set makes it possible to store four double precision floats, one does not get a 4x speed up because of the (unaligned) memory transfers to and from vector registers.</p><p>Finally, using both expression templates, vectorization and OpenMP:</p><p>
</p><div class="boxout boxout-bdr-grey"><a class="table" name="" id=""/><table cellpadding="0" cellspacing="0" border="0" data-toolbar-link="" data-toolbar-img="/1749-4699/8/1/014001/suppdata/" data-toolbar-type="table"><colgroup><col align="left"/></colgroup><tbody>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">
<span xmlns:xlink="http://www.w3.org/1999/xlink" class="inline-eqn"><span class="tex"><span class="texImage"><img src="http://cdn.iopscience.com/images/1749-4699/8/1/014001/Full/csd509524ieqn12.gif" alt="" align="top"/></span></span></span> % <tt>timeit my.pythranized(b,c)</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">
<tt>1000 loops, best of 3: 105 us per loop</tt>
</td>
</tr>
</tbody></table></div>
<p>The four hyper-threaded cores give an extra performance boost. In the end, Pythran generates a native module that performs roughly <span xmlns:xlink="http://www.w3.org/1999/xlink" class="inline-eqn"><span class="tex"><span class="texImage"><img src="http://cdn.iopscience.com/images/1749-4699/8/1/014001/Full/csd509524ieqn13.gif" alt="" align="top"/></span></span></span> faster than the original version.</p><p>As a reference, the <tt>numexpr</tt> module that performs JIT optimization of the expression yields the following timing:</p><p>
</p><div class="boxout boxout-bdr-grey"><a class="table" name="" id=""/><table cellpadding="0" cellspacing="0" border="0" data-toolbar-link="" data-toolbar-img="/1749-4699/8/1/014001/suppdata/" data-toolbar-type="table"><colgroup><col align="left"/></colgroup><tbody>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">
<span xmlns:xlink="http://www.w3.org/1999/xlink" class="inline-eqn"><span class="tex"><span class="texImage"><img src="http://cdn.iopscience.com/images/1749-4699/8/1/014001/Full/csd509524ieqn14.gif" alt="" align="top"/></span></span></span> % <tt>timeit numexpr.evaluate('sqrt(b*b+c*c)')</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">
<tt>1000 loops, best of 3: 395 us per loop</tt>
</td>
</tr>
</tbody></table></div>
<p>Pythran still performs almost four times faster.</p></div>
<h2 class="header-anchor" id="csd509524s6" name="csd509524s6">6. Explicit parallelization</h2><div class="article-text" data-mobile-collapse=""><p>Many scientific applications can benefit from the parallel execution of their Kernels. As modern computers generally feature several processors and several cores per processor, it is critical for the scientific application developers to be able to take advantage of them.</p><p>As explained in the previous section, Pythran takes advantage of multiple cores when compiling Numpy expressions. However, when possible, it is often more profitable to parallelize the outermost loops rather than the inner loops—the Numpy expressions—because it avoids the synchronization barrier at the end of each parallel section, and generally offers more computation intensive computations.</p><p>The OpenMP standard [<a xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:encoder="xalan://java.net.URLEncoder" xmlns:stringescapeutils="xalan://org.apache.commons.lang.StringEscapeUtils" class="cite hastip" href="#csd509524bib11" id="fnref-csd509524bib11">11</a>] is a widely used solution for Fortran, C and C++ to describe loop-based and task-based parallelism. It consists of a few directives attached to the code, directives that describe parallel loops and parallel code sections in a shared memory model.</p><p>Pythran makes this directives available at the Python level through string instructions. The semantic is roughly similar to the semantics defined by the standard, assuming that all variables have function-level scope.</p><p>The following listing gives a simple example of explicit loop-based parallelism (OpenMP 3.0 task-based parallelism form is also supported, the interested reader may refer to [<a xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:encoder="xalan://java.net.URLEncoder" xmlns:stringescapeutils="xalan://org.apache.commons.lang.StringEscapeUtils" class="cite hastip" href="#csd509524bib6" id="fnref-csd509524bib6">6</a>] for more details). the <tt>Kernel</tt> function is pure and computation intensive, making the loop a perfect candidate for parallelization.</p><p>
</p><div class="boxout boxout-bdr-grey"><a class="table" name="" id=""/><table cellpadding="0" cellspacing="0" border="0" data-toolbar-link="" data-toolbar-img="/1749-4699/8/1/014001/suppdata/" data-toolbar-type="table"><colgroup><col align="left"/></colgroup><tbody>
<tr valign="top">
<td colspan="1" rowspan="1" align="left"><strong>def</strong>  <tt>compute</tt>_<tt>julia(cr, ci, N, bound</tt> <tt>=</tt> <tt>1.5, lim</tt> <tt>=</tt> <tt>1000., cutoff</tt> <tt>=</tt> <tt>1e6):</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  <tt>julia</tt> <tt>=</tt> <tt>np.empty((N, N), np.uint32)</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  <tt>grid</tt>_<tt>x</tt> <tt>=</tt> <tt>np.linspace(-bound, bound, N)</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  #<tt>omp parallel for</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">   <strong>for</strong> <tt>i, x</tt> <strong>in</strong> <tt>enumerate(grid</tt>_<tt>x):</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">    <strong>for</strong> <tt>j, y</tt> <strong>in</strong> <tt>enumerate(grid</tt>_<tt>x):</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">       <tt>julia[i,j]=Kernel(x, y, cr, ci, lim, cutoff)</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">  <strong>return</strong> <tt>julia</tt>
</td>
</tr>
</tbody></table></div>
<p>As in in standard OpenMP, loop indices of the parallel loop are automatically made private, and Pythran extends the concept to any random access iterator, even after type destructuring. In the example, both <tt>i</tt> and <tt>x</tt> are private. A scope analysis is used to prove that <tt>j</tt> and <tt>y</tt> are also local to the loop. In the end, the whole computation achieves a perfect speedup of <span xmlns:xlink="http://www.w3.org/1999/xlink" class="inline-eqn"><span class="tex"><span class="texImage"><img src="http://cdn.iopscience.com/images/1749-4699/8/1/014001/Full/csd509524ieqn15.gif" alt="" align="top"/></span></span></span>.</p><p>Next section performs an in-depth comparison of Pythran with three other Python optimizers: CPython, Cython and PyPy.</p></div>
<h2 class="header-anchor" id="csd509524s7" name="csd509524s7">7. Benchmarks</h2><div class="article-text" data-mobile-collapse=""><p>All benchmarks presented in this section are run on an hyper-threaded quad-core i7, using examples shipped along with Pythran sources, available at <a xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:encoder="xalan://java.net.URLEncoder" xmlns:stringescapeutils="xalan://org.apache.commons.lang.StringEscapeUtils" class="webref" target="_blank" href="https://github.com/serge-sans-paille/pythran">https://github.com/serge-sans-paille/pythran</a> in the <tt>pythran/test/cases</tt> directory. The Pythran version used is the <tt>HEAD</tt> of the <tt>iop2014</tt> branch. The other tools are: PyPy 2.0.2 compiled with the <tt>-jit</tt> flag, CPython 2.7.3 and Cython 0.20. All timings are made using the <tt>timeit</tt> module, taking the best of all runs. All C++ codes are compiled with g++ 4.8.2, using the tool default compiler option, generally <tt>-O3</tt> plus a few optimizing flags depending on the target. To make a fair comparison, no parallelism annotations are added to any code.</p><p>Provided each module has been properly compiled, each experiment is run using the following command from a shell:</p><p>
<tt>python -m timeit -s '</tt>${<tt>SETUP</tt>}<tt>' '</tt>${<tt>RUN</tt>}<tt>'</tt>
</p><p>where $<tt>SETUP</tt> and $<tt>RUN</tt> depend on the benchmark and <tt>pypy</tt> is substituted to <tt>python</tt> when needed. The $<tt>SETUP</tt> and $<tt>RUN</tt> values are available to the reader on the Git repository in the <tt>doc/papers/iop/xp/run</tt>_<tt>benchamrks.sh</tt> file with all the experimental setup.</p><p>The same source is used for each benchmark, to the exception of Cython sources which needed a manual conversion. For these sources, bound checking and wrap around are always disable, all variables are declared using the proper C or C++ type and all mathematical functions in use come from <tt>libm</tt>. Some calls to the <tt>zeros</tt>_<tt>like</tt> function also have been replaced by calls to <tt>zeros</tt> for the PyPy version.</p><h3 id="csd509524s7-1" name="csd509524s7-1">7.1. NQueens</h3><div class="article-text"><p>'Nqueens' is a benchmark extracted from the former Unladen Swallow<sup xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:encoder="xalan://java.net.URLEncoder" xmlns:stringescapeutils="xalan://org.apache.commons.lang.StringEscapeUtils" class="fnref"><a class="fnref" href="#csd509524fn2">2</a></sup>
 project. It is particularly interesting as it makes an intensive use of non-trivial generator expressions and integer sets.</p><p>Table <a xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:encoder="xalan://java.net.URLEncoder" xmlns:stringescapeutils="xalan://org.apache.commons.lang.StringEscapeUtils" class="tabref" href="#csd509524t1">1</a> illustrates the benchmark results for CPython, Cython, PyPy and Pythran. It showcases the fact that Pythran benefits a lot from high-level transformations, for instance <em>lazy evaluation</em>, which prevents the many allocation/deallocation triggered by Cython and PyPy runs. A deeper look at the Pythran profiling trace shows that more than half of the execution time is spent allocating and deallocating a <tt>set</tt> used in the internal loop. There is a memory allocation invariant that could be taken advantage of there, but none of the compiler does so.</p><div class="boxout boxout-bdr-grey"><a class="table" name="csd509524t1" id="csd509524t1"/><p><b>Table 1.</b> 
Benchmarking result on the NQueen program.
</p><table cellpadding="0" cellspacing="0" border="0" data-toolbar-link="csd509524t1" data-toolbar-img="/1749-4699/8/1/014001/suppdata/" data-toolbar-type="table"><colgroup><col align="left"/><col align="left"/><col align="left"/><col align="left"/><col align="left"/></colgroup><thead>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">Tool</td>
<td colspan="1" rowspan="1" align="center">CPython</td>
<td colspan="1" rowspan="1" align="center">Cython</td>
<td colspan="1" rowspan="1" align="center">Pythran</td>
<td colspan="1" rowspan="1" align="center">PyPy</td>
</tr>
</thead><tbody>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">Timing</td>
<td colspan="1" rowspan="1" align="center">181 ms</td>
<td colspan="1" rowspan="1" align="center">42.3 ms</td>
<td colspan="1" rowspan="1" align="center"><strong>22.5 ms</strong>
</td>
<td colspan="1" rowspan="1" align="center">51.3 ms</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">Speedup</td>
<td colspan="1" rowspan="1" align="center"><b>×</b>1</td>
<td colspan="1" rowspan="1" align="center"><b>×</b>4.27</td>
<td colspan="1" rowspan="1" align="center"><strong><b>×</b>8.04</strong>
</td>
<td colspan="1" rowspan="1" align="center"><b>×</b>3.52</td>
</tr>
</tbody></table></div></div><h3 id="csd509524s7-2" name="csd509524s7-2">7.2. Rosen der</h3><div class="article-text"><p>The 'rosen der' Kernel computes a complex numpy expression that accesses data through a one-dimensional slice. It is interesting to evaluate how much of the expression can be compiled to regular, efficient loop while avoiding temporaries. Table <a xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:encoder="xalan://java.net.URLEncoder" xmlns:stringescapeutils="xalan://org.apache.commons.lang.StringEscapeUtils" class="tabref" href="#csd509524t2">2</a> illustrate the benchmark results for CPython, Cython, PyPy and Pythran. Cython slightly outperforms Pythran on that test case, but Cython code had to be manually transformed into an explicit loop. Moreover, activating automatic parallelization for Pythran yields an additional <span xmlns:xlink="http://www.w3.org/1999/xlink" class="inline-eqn"><span class="tex"><span class="texImage"><img src="http://cdn.iopscience.com/images/1749-4699/8/1/014001/Full/csd509524ieqn16.gif" alt="" align="top"/></span></span></span> speedup without a change to the input, while Cython needs manual code annotation to achieve a similar result. It seems that PyPy support for numpy expression is still not mature for numpy expression, but there is a strong on-going effort of the community to fix this.</p><div class="boxout boxout-bdr-grey"><a class="table" name="csd509524t2" id="csd509524t2"/><p><b>Table 2.</b> 
Benchmarking result on the Rosen der program.
</p><table cellpadding="0" cellspacing="0" border="0" data-toolbar-link="csd509524t2" data-toolbar-img="/1749-4699/8/1/014001/suppdata/" data-toolbar-type="table"><colgroup><col align="left"/><col align="left"/><col align="left"/><col align="left"/><col align="left"/></colgroup><thead>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">Tool</td>
<td colspan="1" rowspan="1" align="center">CPython</td>
<td colspan="1" rowspan="1" align="center">Cython</td>
<td colspan="1" rowspan="1" align="center">Pythran</td>
<td colspan="1" rowspan="1" align="center">PyPy</td>
</tr>
</thead><tbody>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">Timing</td>
<td colspan="1" rowspan="1" align="center">22.3 ms</td>
<td colspan="1" rowspan="1" align="center"><strong>3.39 ms</strong>
</td>
<td colspan="1" rowspan="1" align="center">3.81 ms</td>
<td colspan="1" rowspan="1" align="center">51.3 ms</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">Speedup</td>
<td colspan="1" rowspan="1" align="center"><b>×</b>1</td>
<td colspan="1" rowspan="1" align="center"><strong><b>×</b>6.57</strong>
</td>
<td colspan="1" rowspan="1" align="center"><b>×</b>5.85</td>
<td colspan="1" rowspan="1" align="center"><b>×</b>0.43</td>
</tr>
</tbody></table></div></div><h3 id="csd509524s7-3" name="csd509524s7-3">7.3. Growcut</h3><div class="article-text"><p>The 'growcut' application is an image processing code where all array indexing is explicit. In that sense, this benchmark is complementary to the previous one, as it evaluates how well explicit array accesses are compiled. Table <a xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:encoder="xalan://java.net.URLEncoder" xmlns:stringescapeutils="xalan://org.apache.commons.lang.StringEscapeUtils" class="tabref" href="#csd509524t3">3</a> summarizes the results. All compilers yield impressive speedups on that benchmark, but this is is merely due to the fact that accessing individual elements is extremely slow in numpy, and the direct C translation of the Python code is very close to what a C developer would write. Pythran appears to outperforms Cython once all accesses are explicit, most certainly due to cleaner code generation.</p><div class="boxout boxout-bdr-grey"><a class="table" name="csd509524t3" id="csd509524t3"/><p><b>Table 3.</b> 
Benchmarking result on the growcut program.
</p><table cellpadding="0" cellspacing="0" border="0" data-toolbar-link="csd509524t3" data-toolbar-img="/1749-4699/8/1/014001/suppdata/" data-toolbar-type="table"><colgroup><col align="left"/><col align="left"/><col align="left"/><col align="left"/><col align="left"/></colgroup><thead>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">Tool</td>
<td colspan="1" rowspan="1" align="center">CPython</td>
<td colspan="1" rowspan="1" align="center">Cython</td>
<td colspan="1" rowspan="1" align="center">Pythran</td>
<td colspan="1" rowspan="1" align="center">PyPy</td>
</tr>
</thead><tbody>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">Timing</td>
<td colspan="1" rowspan="1" align="center">398 ms</td>
<td colspan="1" rowspan="1" align="center">991 us</td>
<td colspan="1" rowspan="1" align="center"><strong>526 us</strong>
</td>
<td colspan="1" rowspan="1" align="center">5.54 ms</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">Speedup</td>
<td colspan="1" rowspan="1" align="center"><b>×</b>1</td>
<td colspan="1" rowspan="1" align="center"><b>×</b>401.6</td>
<td colspan="1" rowspan="1" align="center"><b>×</b>756.7</td>
<td colspan="1" rowspan="1" align="center"><b>×</b>71</td>
</tr>
</tbody></table></div></div><h3 id="csd509524s7-4" name="csd509524s7-4">7.4. Blacksholes</h3><div class="article-text"><p>The 'blacksholes' application is a benchmarks that showcases the translation cost from the Python space to native space. Indeed we forced the use of a <tt>list</tt> as input of the Kernel instead of a numpy array. As a consequence there is a costly translation step from a structure that contains an array of pointers to generic Python objects to a dense structure involved. Results are shown in table <a xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:encoder="xalan://java.net.URLEncoder" xmlns:stringescapeutils="xalan://org.apache.commons.lang.StringEscapeUtils" class="tabref" href="#csd509524t4">4</a>. They show that conversion code is less efficient in Cython than in Pythran. PyPy also outperforms Cython, because it does not suffer from a similar conversion penalty.</p><div class="boxout boxout-bdr-grey"><a class="table" name="csd509524t4" id="csd509524t4"/><p><b>Table 4.</b> 
Benchmarking result on the blacksholes program.
</p><table cellpadding="0" cellspacing="0" border="0" data-toolbar-link="csd509524t4" data-toolbar-img="/1749-4699/8/1/014001/suppdata/" data-toolbar-type="table"><colgroup><col align="left"/><col align="left"/><col align="left"/><col align="left"/><col align="left"/></colgroup><thead>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">Tool</td>
<td colspan="1" rowspan="1" align="center">CPython</td>
<td colspan="1" rowspan="1" align="center">Cython</td>
<td colspan="1" rowspan="1" align="center">Pythran</td>
<td colspan="1" rowspan="1" align="center">PyPy</td>
</tr>
</thead><tbody>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">Timing</td>
<td colspan="1" rowspan="1" align="center">490 us</td>
<td colspan="1" rowspan="1" align="center">74.7 us</td>
<td colspan="1" rowspan="1" align="center"><strong>34.8 us</strong>
</td>
<td colspan="1" rowspan="1" align="center">37.3 us</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">Speedup</td>
<td colspan="1" rowspan="1" align="center"><b>×</b>1</td>
<td colspan="1" rowspan="1" align="center"><b>×</b>6.55</td>
<td colspan="1" rowspan="1" align="center"><b>×</b>14.1</td>
<td colspan="1" rowspan="1" align="center"><b>×</b>13.1</td>
</tr>
</tbody></table></div></div></div>
<h2 class="header-anchor" id="csd509524s8" name="csd509524s8">8. Integration into existing applications</h2><div class="article-text" data-mobile-collapse=""><p>The <tt>distutils</tt> is the standard way to package applications. In addition to its command line interface, Pythran provides a <tt>distutils</tt> extension to integrate Pythran extension into existing applications. Its use is similar to the <tt>Extension</tt> class:</p><p>
</p><div class="boxout boxout-bdr-grey"><a class="table" name="" id=""/><table cellpadding="0" cellspacing="0" border="0" data-toolbar-link="" data-toolbar-img="/1749-4699/8/1/014001/suppdata/" data-toolbar-type="table"><colgroup><col align="left"/></colgroup><tbody>
<tr valign="top">
<td colspan="1" rowspan="1" align="left"><strong>from</strong> <tt>distutils.core</tt> <strong>import</strong> <tt>setup</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left"><strong>from</strong> <tt>pythran.dist</tt> <strong>import</strong> <tt>PythranExtension</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">
<tt>module1</tt> <tt>=</tt> <tt>PythranExtension('demo', sources</tt> <tt>=</tt> <tt>['a.py'])</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left"> </td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">
<tt>setup(name='demo',</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">     <tt>version</tt> <tt>=</tt> <tt>'1.0',</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">     <tt>description</tt> <tt>=</tt> <tt>'This is a demo package',</tt>
</td>
</tr>
<tr valign="top">
<td colspan="1" rowspan="1" align="left">     <tt>ext</tt>_<tt>modules</tt> <tt>=</tt> <tt>[module1]</tt>)</td>
</tr>
</tbody></table></div>
<p>As Pythran input language is a subset of Python, it is still possible to write the <tt>setup.py</tt> so that the Python code is used as a regular module if the Pythran import fails.</p><p>Pythran is supported on Linux and MacOSX systems. It is available in source code form on the git repository <a xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:encoder="xalan://java.net.URLEncoder" xmlns:stringescapeutils="xalan://org.apache.commons.lang.StringEscapeUtils" class="webref" target="_blank" href="https://github.com/serge-sans-paille/pythran">https://github.com/serge-sans-paille/pythran</a>, and on the cheese-shop <a xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:encoder="xalan://java.net.URLEncoder" xmlns:stringescapeutils="xalan://org.apache.commons.lang.StringEscapeUtils" class="webref" target="_blank" href="https://pypi.python.org/pypi/pythran">https://pypi.python.org/pypi/pythran</a>.</p></div>
<h2 class="header-anchor" id="csd509524s9" name="csd509524s9">9. Future work</h2><div class="article-text" data-mobile-collapse=""><p>Although Pythran focuses on a subset of Python and its standard library, many optimization opportunities are still available. Using a Domain Specific Language (DSL) approach, one could use rewriting rules to optimize several Python idioms. For instance, <tt>len(set(x))</tt> could lead to an optimized <tt>count</tt>_<tt>uniq</tt> that would iterate only once on the input sequence.</p><p>There is naturally more work to be done at the Numpy level, for instance to support more functions from the original module. The extraction of Numpy expressions from <tt>for</tt> loops is also a natural optimization candidate, which shares similarities with code refactoring.</p><p>Numpy expressions also fit perfectly well in the polyhedral model. Exploring the coupling of polyhedral tools with the code generated from Pythran offers enthusiastic perspectives.</p></div>
<h2 class="header-anchor" id="csd509524s10" name="csd509524s10">10. Conclusion</h2><div class="article-text" data-mobile-collapse=""><p>This paper presents the Pythran compiler, a translator and an optimizer that converts Python into C++. Unlike existing static Python compilers, Pythran leverages several function-level or module-level analyses to provide generic or Python-centric code optimizations. Additionally, it uses a C++ library that makes heavy usage of template programming to provide an efficient API similar to a subset of Python standard library. This library takes advantage of modern hardware capabilities—vector instruction units and multiple-cores—in its implementation of parts of the <tt>numpy</tt> package.</p><p>This paper gives an overview of the compilation flow, the analyses involved and the optimizations used. It also compares the performance of compiled Pythran modules against CPython and other optimizers like Cython and PyPy.</p><p>To conclude, limiting Python to a statically typed subset does not hinders the expressiveness when it comes to scientific or mathematic computations, but makes it possible to use a wide variety of classical optimizations to help Python match the performances of statically compiled language. Moreover, one can use high level information to generate efficient code that would be difficult to write for the average programmer. In the end, the scientific community can benefit from a high-level language to quickly write their experiments, and still run them at decent speed.</p></div> <h2 class="header-anchor" name="acknowledgments" id="acknowledgements">Acknowledgments</h2><div class="article-text" data-mobile-collapse=""><p>This project has been partially funded by the CARP Project and the SILKAN Company. The authors would like to thank Yuancheng Peng and Eliott Coyac for their contributions to the development of the Pythran compiler.</p></div></div>
	

				

	
























	
	
		</div></body></html>