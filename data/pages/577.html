<html><body><div><div dir="ltr" trbidi="on">
<p>If you haven't seen it yet, Glyph wrote a great blog post on concurrency called <a href="https://glyph.twistedmatrix.com/2014/02/unyielding.html">Unyielding</a>. This blog post is a response to that blog post.</p>

<p>Over the years, I've tried each of the approaches he talks about while working at various companies. I've known all of the arguments for years. However, I think his blog post is the best blog post I've read for the arguments he is trying to make. Nice job, Glyph!</p>

<p>In particular, I agree with his statements:</p>

<blockquote>What I hope I’ve demonstrated is that if you agree with me that threading has problematic semantics, and is difficult to reason about, then there’s no particular advantage to using microthreads, beyond potentially optimizing your multithreaded code for a very specific I/O bound workload.</blockquote>

<blockquote>There are no shortcuts to making single-tasking code concurrent. It's just a hard problem, and some of that hard problem is reflected in the difficulty of typing a bunch of new concurrency-specific code.</blockquote>

<p>In this blog post, I'm not really disputing his core message. Rather, I'm just pointing out some details and distinctions.</p>

<p>First of all, it threw me off when he mentioned JavaScript since JavaScript doesn't have threads. In the browser, it has web workers which are like processes, and in Node, it has a mix of callbacks, deferreds, and yield. However, reading his post a second time, all he said was that JavaScript had "global shared mutable state". He never said that it had threads.</p>

<p>The next thing I'd like to point out is that there are some real readability differences between the different approaches. Glyph did a good job of arguing that it's difficult to <i>reason</i> about concurrency when you use threads. However, if you ignore race conditions for a moment: I think it's certainly true that threads, explicit coroutines, and green threads are easier to read than callbacks and deferreds. That's because they let you write code in a more traditional, linear fashion. Even though I can do it, using callbacks and deferreds always cause my brain to hurt ;) Perhaps I just need more practice.</p>

<p>Another thing to note is that the type of application matters a lot when you need to address concurrency concerns. For instance, if you're building a UI, you don't want any computationally heavy work to be done on the UI thread. For instance, in Android, you do as little CPU heavy and IO heavy work as possible on the UI thread, and instead push that work off into other threads.</p>

<p>Other things to consider are IO bound vs. CPU bound, stateful vs. stateless.</p>

<p>Threads are fine, if all of the following are true:</p>

<ul>
<li>You're building a stateless web app.</li>
<li>You're IO bound.</li>
<li>All mutable data is stored in a per-request context object, in per-request instances, or in thread-local storage.</li>
<li>You have no module-level or class-level mutable data.</li>
<li>You're not doing things like creating new classes or modules on the fly.</li>
<li>In general, threads don't interact with each other.</li>
<li>You keep your application state in a database.</li>
</ul>

<p>Sure there's always going to be some global, shared, mutable data such as sys.modules, but in practice Python itself protects that using the GIL.</p>

<p>I've built apps such as the above in a multithreaded way for years, and I've never run into any race conditions. The difference between this sort of app and the app that lead to Glyph's "buggiest bug" is that he was writing a very stateful application server.</p>

<p>I'd also like to point out that it's important to not overlook the utility of UNIX processes. Everyone knows how useful the multiprocessing module is and that processes are the best approach in Python for dealing with CPU bound workloads (because you don't have to worry about the GIL).</p>

<p>However, using a pre-fork model is also a great way of building stateless web applications. If you have to handle a lot of requests, but you don't have to handle a very large number simultaneously, pre-forked processes are fine. The upside is that the code is both easy to read (because it doesn't use callbacks or deferreds), and it's easy to reason about (because you don't have the race conditions that threads have). Hence, a pre-fork model is great for programmer productivity. The downside is that each process can eat up a lot of memory. Of course, if your company makes it to the point where hardware efficiency costs start outweighing programmer efficiency costs, you have what I like to call a "nice to have problem". PHP and Ruby on Rails have both traditionally used a pre-fork approach.</p>

<p>I'm also a huge fan of approaches such as Erlang that give you what is conceptually a process, without the overhead of real UNIX processes.</p>

<p>As Glyph hinted at, this is a really polarizing issue, and there really are no easy, perfect-in-every-way solutions. Any time concurrency is involved, there are always going to be some things you need to worry about regardless of which approach to concurrency you take. That's why we have things like databases, transactions, etc. It's really easy to fall into religious arguments about the best approach to concurrency at an application level. I think it's really helpful to be frank and honest about the pros and cons of each approach.</p>

<p>That being said, I do look forward to one day trying out Guido's asyncio module.</p>

<p>See also:</p>


</div>
</div></body></html>