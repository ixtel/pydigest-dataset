<html><body><div><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-draw" class="anchor" href="#draw" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>draw</h1>

<p>TensorFlow implementation of <a href="http://arxiv.org/pdf/1502.04623.pdf">DRAW: A Recurrent Neural Network For Image Generation</a> on the MNIST generation task.</p>

<table><thead>
<tr>
<th>With Attention</th>
<th>Without Attention</th>
</tr>
</thead><tbody>
<tr>
<td><a href="https://camo.githubusercontent.com/3deb38f6b574f530fc27eefe67339f167ed4127a/687474703a2f2f692e696d6775722e636f6d2f5866416b5850772e676966" target="_blank"><img src="https://camo.githubusercontent.com/3deb38f6b574f530fc27eefe67339f167ed4127a/687474703a2f2f692e696d6775722e636f6d2f5866416b5850772e676966" data-canonical-src="http://i.imgur.com/XfAkXPw.gif"/></a></td>
<td><a href="https://camo.githubusercontent.com/efbf496788953d0d2616ab818ca5cf200db254de/687474703a2f2f692e696d6775722e636f6d2f715155546f4f792e676966" target="_blank"><img src="https://camo.githubusercontent.com/efbf496788953d0d2616ab818ca5cf200db254de/687474703a2f2f692e696d6775722e636f6d2f715155546f4f792e676966" data-canonical-src="http://i.imgur.com/qQUToOy.gif"/></a></td>
</tr>
</tbody></table>

<p>Although open-source implementations of this paper already exist (see links below), this implementation focuses on simplicity and ease of understanding. I tried to make the code resemble the raw equations as closely as posible.</p>

<p>For a gentle walkthrough through the paper and implementation, see the writeup here: <a href="http://evjang.com/articles/draw">https://evjang.com/articles/draw</a>.</p>

<h2><a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Usage</h2>

<p><code>python draw.py --data_dir=/tmp/draw</code> downloads the binarized MNIST dataset to /tmp/draw/mnist and trains the DRAW model with attention enabled for both reading and writing. After training, output data is written to <code>/tmp/draw/draw_data.npy</code></p>

<p>You can visualize the results by running the script <code>python plot_data.py &lt;prefix&gt; &lt;output_data&gt;</code></p>

<p>For example, </p>

<p><code>python myattn /tmp/draw/draw_data.npy</code></p>

<p>To run training without attention, do:</p>

<p><code>python draw.py --working_dir=/tmp/draw --read_attn=False --write_attn=False</code></p>

<h2><a id="user-content-restoring-from-pre-trained-model" class="anchor" href="#restoring-from-pre-trained-model" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Restoring from Pre-trained Model</h2>

<p>Instead of training from scratch, you can load pre-trained weights by uncommenting the following line in <code>draw.py</code> and editing the path to your checkpoint file as needed. Save electricity! </p>

<div class="highlight highlight-source-python"><pre>saver.restore(sess, <span class="pl-s"><span class="pl-pds">"</span>/tmp/draw/drawmodel.ckpt<span class="pl-pds">"</span></span>)</pre></div>

<p>This git repository contains the following pre-trained in the <code>data/</code> folder:</p>

<table><thead>
<tr>
<th>Filename</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>draw_data_attn.npy</td>
<td>Training outputs for DRAW with attention</td>
</tr>
<tr>
<td>drawmodel_attn.ckpt</td>
<td>Saved weights for DRAW with attention</td>
</tr>
<tr>
<td>draw_data_noattn.npy</td>
<td>Training outputs for DRAW without attention</td>
</tr>
<tr>
<td>drawmodel_noattn.ckpt</td>
<td>Saved weights for DRAW without attention</td>
</tr>
</tbody></table>

<p>These were trained for 10000 iterations with minibatch size=100 on a GTX 970 GPU.</p>

<h2><a id="user-content-useful-resources" class="anchor" href="#useful-resources" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Useful Resources</h2>


</article>
  </div></body></html>