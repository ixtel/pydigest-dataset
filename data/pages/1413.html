<html><body><div><div class="post">
    <h1 class="post-title">Parallel Python on a GPU with OpenCL</h1>
    <span class="post-date">06 Sep 2014</span>
    
<h2 id="run-code-on-the-what">Run code on the what?</h2>

<p>I had a Wordpress blog in a previous life but I deleted it the other day, right
after I made this site. I had only one post on that blog that attracted any
attention. At the time I was a student working with time-series data obtained
from various telescopes in
<a href="http://en.wikipedia.org/wiki/Sutherland,_Northern_Cape">Sutherland</a>, in South
Africa. Some of these datasets were rather a bit larger than I was used to at
the time and the running times of some of my programs was slightly obscene.</p>

<p>Then I received a <strong>QUAD CORE</strong> machine to use at the Observatory. It
was great! I thought I’m going to write the worlds fastest parallel code and
everything will be awesome. Turns out writing parallel code is hard. Really
hard. I also learned that Python can basically not do parallel computations (or
so I thought, Global Interpreter Lock what?)</p>

<p>Being an astronomy student, I was exposed to Fortran. And not the ‘nice’
Fortran 95, nope, the old kind with no code in the first 6 columns and
everything in CAPS and variable names shorter than 6 characters O_o One gets
used to that stuff and once you do you can write some fairly understandable
stuff that runs like greased lightning. But you don’t really want to if you
know Python. So I learned the concept of profiling my code and rewriting only
the slow parts in a compiled language and it is during this time that I
figured out not only how to call Fortran code from Python, but also how to use
OpenMP to parallelize it. I wrote what I learned on that old Wordpress site and
that was the only one that got any attention from the outside world, and not a
lot of it. Now it just so happens that I learned something new again. I’m not a
student anymore and I don’t get to work with science data nearly as much as I
would have liked to but I still like tinkering with computers and writing code.</p>

<p>I learned how to take that code that takes too long to run, even if you run it
on 8 threads on an overclocked i7, and make it run on 1000+ cores on a graphics
card. Yep. </p>

<h2 id="lets-make-something-then-make-it-fast">Let’s make something then make it fast.</h2>

<p>So the idea is to show a simple calculation that is easily done in just a few
lines of python+numpy and then show how much faster (or slower) that code runs
if you go through the effort of redoing it in another language or using another
tool.</p>

<p>The one thing I used a lot was the <em>Deeming</em> periodogram. It is basically a
<a href="http://en.wikipedia.org/wiki/Discrete_Fourier_transform">Discrete Fourier
Transform [DFT]</a>. In
astronomy it is used to find periodic signals in time-series datasets and this
is exactly what I used it for. I just got bored with waiting for my computer to
finish calculating them for me.</p>

<p><em>Wait, you calculated the DFT from its
definition? Why not a <a href="http://en.wikipedia.org/wiki/Fast_Fourier_transform">Fast Fourier Transform (FFT)</a>?</em></p>

<p>Here’s the problem. The FFT algorithm assumes that the samples are regularly
spaced (in time, say) and the DFT doesn’t make this assumption and in astronomy
the observations are almost NEVER done exactly <em>N</em> seconds apart. So I had to
use the slow algorithm OR I had resample the dataset into one that was evenly
sampled in time. I chose the former because thats what was done by everyone
else (maybe a topic for another post).</p>

<p>OK, so here’s what the Python code looks like for calculating a Deeming Periodogram:</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">periodogram_numpy</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">freqs</span><span class="p">):</span>
    <span class="sd">''' Calculate the Deeming periodogram using numpy</span>
<span class="sd">    '''</span>
    <span class="n">amps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">freqs</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s">'float'</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">freqs</span><span class="p">):</span>
        <span class="n">real</span> <span class="o">=</span> <span class="p">(</span><span class="n">m</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">f</span><span class="o">*</span><span class="n">t</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">imag</span> <span class="o">=</span> <span class="p">(</span><span class="n">m</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">f</span><span class="o">*</span><span class="n">t</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">amps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">real</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">imag</span><span class="o">**</span><span class="mi">2</span>

    <span class="n">amps</span> <span class="o">=</span> <span class="mf">2.0</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">amps</span><span class="p">)</span><span class="o">/</span><span class="n">t</span><span class="o">.</span><span class="n">size</span>
    <span class="k">return</span> <span class="n">amps</span></code></pre></div>

<p>It’s super simple and for small amounts of data it runs in less than a second
and that’s all fine. Simple is good and fast enough is good. But I didn’t
always have small amounts of data. I also had an extra complication that has to
do with the mathematics of Fourier Transforms. In the above code you can see
that there is one loop that runs in Python and it loops over all the
frequencies given. There is another loop inside that but you don’t see it in
this code because it runs inside of numpy behind the scenes. The invisible loop
runs over every data point. So this function touches every frequency once, and
for every frequency, it touches every data point. So the number of operations
can be estimated by multiplying those two together i.e.:</p>

<p>
  Operations = N<sub>frequencies</sub> ×  M<sub>data points</sub>
</p>

<p>There’s a couple more operations hidden away in there but we’ll ignore them for
now.  That number can climb quite rapidly and this kind of algorithm is said to
have <em>O(N²)</em> (of the Order N×N) complexity. So the more data you have the
longer (and then some) it takes to do. So how do you make it go faster? The
correct answer is: <strong>DO LESS</strong>, which really means, in computer science terms,
to choose an algorithm that can do what you want to do, in fewer operations.
Sometimes this is just not a possibility and even after selecting the best
algorithm you still have so much calculations that you can go to the bar for a
few hours while your computer sits and chugs away at the numbers. (Hey why
optimize at all?)</p>

<p>One thing to notice is that this algorithm can be done in <em>parallel</em>. You do
not need to know the result from the previous frequency bin to obtain the next
one, in this instance. So nothing stops you from calculating all the frequency
loops simultaneously. This is where my old blog post came in and where this one
starts and takes it a bit further. We are going to do those loops in parallel
and we are going to use a time-tested language (because I wrote the code long
ago and it still works and I’m too lazy to do it in a modern language again).</p>

<h2 id="requirements-for-the-code">Requirements for the code</h2>

<p>I have a <code>Gist</code> that contains the code I ran to get all the results. You can
find it <a href="https://gist.github.com/ezietsman/226473">here</a>. It contains a script
(<code>build_deeming.sh</code>) that will build the necessary things and another
(<code>deeming.py</code>) that runs the whole benchmark. I’ve updated the script to work
with Python 3.4, it should run on python 2.7 too. Let me know if you’d like a
<code>pip requirements.txt</code> file for easy dependency installations. You’ll need at least the following:</p>

<pre><code>pip install numpy matplotlib cython pyopencl
</code></pre>

<p>as well as a <code>c</code> and <code>fortran</code> compiler. I used <code>gcc</code> and <code>gfortran</code>. You’ll
also need a working <code>opencl</code> installation. I’m unsure as to what that entails
but I HAVE installed the entire CUDA SDK, Nvidia drivers as well as <code>pyopencl</code>.</p>

<h2 id="the-results">The Results</h2>

<p>OK on to the fun stuff!</p>

<p>When <code>deeming.py</code> is run, it will compile the <code>opencl</code> kernel and the <code>cython</code>
versions the first time you run it, after that they are cached will just be
called. It will then proceed to calculate the deeming periodograms for 1000,
2000, 4000, 8000, 16000, 32000 and 64000 datapoints and frequencies. This takes
a good few minutes to complete on my computer. I have the following goodies:</p>

<ul>
  <li>64 Bit Manjaro (Arch) Linux</li>
  <li>Intel Core i7-920 @ 3.6GHz</li>
  <li>8GB RAM @ 1333MHz</li>
  <li>NVidia Geforce 760 GTX 2GB</li>
</ul>

<p>Here are the plots that are produced by <code>deeming.py</code>. The bars represent how
much faster each method is than the numpy version I posted earlier in the post.
In each case the numpy version will have a speedup of exactly 1.0. I plotted
the actual execution time in seconds on top of every bar. In the case of the
Fortran version (which is parallelised with <code>openmp</code>), the number next to the name
denotes the number of threads that was used in the calculation.</p>

<p>First up, 1000×1000. Nothing much to see here. For small amounts of data it
won’t take long to complete.</p>
<figure>
  <img src="/assets/images/opencl/1000x1000-barchart.jpg" alt="Barchart for 1000x1000 benchmarks"/>
  <figcaption>Speedups for 1000 frequencies on 1000 data points</figcaption>
</figure>

<p>Next up 2000×2000, everything still pretty fast.</p>

<figure>
  <img src="/assets/images/opencl/2000x2000-barchart.jpg" alt="Barchart for 2000x2000 benchmarks"/>
  <figcaption>Speedups for 2000 frequencies on 2000 data points</figcaption>
</figure>

<p>And from here onwards, things get silly. The 760 is starting to show its muscle.</p>

<figure>
  <img src="/assets/images/opencl/4000x4000-barchart.jpg" alt="Barchart for 4000x4000 benchmarks"/>
  <figcaption>Speedups for 4000 frequencies on 4000 data points</figcaption>
</figure>
<figure>
  <img src="/assets/images/opencl/8000x8000-barchart.jpg" alt="Barchart for 8000x8000 benchmarks"/>
  <figcaption>Speedups for 8000 frequencies on 8000 data points</figcaption>
</figure>
<figure>
  <img src="/assets/images/opencl/16000x16000-barchart.jpg" alt="Barchart for 16000x16000 benchmarks"/>
  <figcaption>Speedups for 16000 frequencies on 16000 data points</figcaption>
</figure>
<figure>
  <img src="/assets/images/opencl/32000x32000-barchart.jpg" alt="Barchart for 16000x16000 benchmarks"/>
  <figcaption>Speedups for 32000 frequencies on 32000 data points</figcaption>
</figure>

<p>For 64000×64000 operations, even the multithreaded fortran code takes minutes
to complete and the GPU is hardly breaking a sweat, completing the calculation
in less than 120th the time it takes the single threaded versions to do it and
almost 25 times faster than even the 8 thread Fortran version!</p>

<figure>
  <img src="/assets/images/opencl/64000x64000-barchart.jpg" alt="Barchart for 16000x16000 benchmarks"/>
  <figcaption>Speedups for 64000 frequencies on 64000 data points</figcaption>
</figure>

<p>The code needed to do this was not super hard to write but it wasn’t as easy as
the original numpy version was and while this increase in performance is
astonishing, most real-world applications won’t see these kinds of results due
to <a href="http://en.wikipedia.org/wiki/Amdahl's_law">Amdahl’s Law</a>. I still think it
is astonishing that I could make my numpy function go 120x faster with 6 times
the amount of code (and a light sprinkling of 1152 cores).</p>

<p>I hope this helps someone. Please leave a comment!</p>




<p id="comments"/>



</div>

</div></body></html>