<html><body><div><div class="entry-content clearfix">












<p><strong>In this article we’re going to build a scraper for an <em>actual</em> freelance gig where the client wants a Python program to scrape data from <a href="http://stackoverflow.com/questions?pagesize=50&amp;sort=newest">Stack Overflow</a> to grab new questions (question title and URL). Scraped data should then be stored in <a href="http://www.mongodb.org/">MongoDB</a>.</strong> It’s worth noting that Stack Overflow has an <a href="https://api.stackexchange.com/">API</a>, which can be used to access the <em>exact</em> same data. However, the client wanted a scraper, so a scraper is what he got.</p>




<p><strong>Updates:</strong></p>

<ol>
<li>01/03/2014 – Refactored the spider. Thanks, <a href="https://twitter.com/kissgyorgy">@kissgyorgy</a>.</li>
<li>02/18/2015 – Added <a href="https://realpython.com/blog/python/web-scraping-and-crawling-with-scrapy-and-mongodb/">Part 2</a>.</li>
<li>09/06/2015 – Updated to the latest version of Scrapy and PyMongo – cheers!</li>
</ol>





<blockquote><p>As always, be sure to review the site’s terms of use/service and respect the <em>robots.txt</em> file before starting any scraping job. Make sure to adhere to ethical scraping practices by not flooding the site with numerous requests over a short span of time. <em>Treat any site you scrape as if it were your own</em>.</p></blockquote>

<a name="Installation"/>
<h2>Installation</h2>

<p>We need the <a href="http://doc.scrapy.org/en/1.0/">Scrapy</a> library (v1.0.3) along with <a href="http://api.mongodb.org/python/3.0.3/">PyMongo</a> (v3.0.3) for storing the data in MongoDB. You need to install <a href="http://docs.mongodb.org/manual/installation/">MongoDB</a> as well (not covered).</p>

<a name="Scrapy"/>
<h3>Scrapy</h3>

<p>If you’re running OSX or a flavor of Linux, install Scrapy with pip (with your virtualenv activated):</p>

<figure class="code"><figcaption><span/></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"><span class="nv">$ </span>pip install <span class="nv">Scrapy</span><span class="o">==</span>1.0.3
</span><span class="line"><span class="nv">$ </span>pip freeze &gt; requirements.txt
</span></code></pre></td></tr></table></div></figure>


<p>If you are on Windows machine, you will need to manually install a number of dependencies. Please refer to the <a href="http://doc.scrapy.org/en/latest/intro/install.html">official documentation</a> for detailed instructions as well as <a href="https://www.youtube.com/watch?v=eEK2kmmvIdw">this Youtube video</a> that I created.</p>

<p>Once Scrapy is setup, verify your installation by running this command in the Python shell:</p>

<figure class="code"><figcaption><span/></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">&gt;&gt;&gt; import scrapy
</span><span class="line">&gt;&gt;&gt;
</span></code></pre></td></tr></table></div></figure>


<p>If you don’t get an error then you are good to go!</p>

<a name="PyMongo"/>
<h3>PyMongo</h3>

<p>Next, install PyMongo with pip:</p>

<figure class="code"><figcaption><span/></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"><span class="nv">$ </span>pip install pymongo
</span><span class="line"><span class="nv">$ </span>pip freeze &gt; requirements.txt
</span></code></pre></td></tr></table></div></figure>


<p>Now we can start building the crawler.</p>

<a name="Scrapy.Project"/>
<h2>Scrapy Project</h2>

<p>Let’s start a new Scrapy project:</p>

<figure class="code"><figcaption><span/></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"><span class="nv">$ </span>scrapy startproject stack
</span><span class="line">2015-09-05 20:56:40 <span class="o">[</span>scrapy<span class="o">]</span> INFO: Scrapy 1.0.3 started <span class="o">(</span>bot: scrapybot<span class="o">)</span>
</span><span class="line">2015-09-05 20:56:40 <span class="o">[</span>scrapy<span class="o">]</span> INFO: Optional features available: ssl, http11
</span><span class="line">2015-09-05 20:56:40 <span class="o">[</span>scrapy<span class="o">]</span> INFO: Overridden settings: <span class="o">{}</span>
</span><span class="line">New Scrapy project <span class="s1">'stack'</span> created in:
</span><span class="line">    /stack-spider/stack
</span><span class="line">
</span><span class="line">You can start your first spider with:
</span><span class="line">    <span class="nb">cd </span>stack
</span><span class="line">    scrapy genspider example example.com
</span></code></pre></td></tr></table></div></figure>


<p>This creates a number of files and folders that includes a basic boilerplate for you to get started quickly:</p>

<figure class="code"><figcaption><span/></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line">├── scrapy.cfg
</span><span class="line">└── stack
</span><span class="line">    ├── __init__.py
</span><span class="line">    ├── items.py
</span><span class="line">    ├── pipelines.py
</span><span class="line">    ├── settings.py
</span><span class="line">    └── spiders
</span><span class="line">        └── __init__.py
</span></code></pre></td></tr></table></div></figure>


<a name="Specify.Data"/>
<h3>Specify Data</h3>

<p>The <em>items.py</em> file is used to define storage “containers” for the data that we plan to scrape.</p>

<p>The <code>StackItem()</code> class inherits from <code>Item</code> (<a href="http://doc.scrapy.org/en/1.0/topics/items.html">docs</a>), which basically has a number of pre-defined objects that Scrapy has already built for us:</p>

<figure class="code"><figcaption><span/></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">scrapy</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">class</span> <span class="nc">StackItem</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
</span><span class="line">    <span class="c"># define the fields for your item here like:</span>
</span><span class="line">    <span class="c"># name = scrapy.Field()</span>
</span><span class="line">    <span class="k">pass</span>
</span></code></pre></td></tr></table></div></figure>


<p>Let’s add some items that we actually want to collect. For each question the client needs the title and URL. So, update <em>items.py</em> like so:</p>

<figure class="code"><figcaption><span/></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">from</span> <span class="nn">scrapy.item</span> <span class="kn">import</span> <span class="n">Item</span><span class="p">,</span> <span class="n">Field</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">class</span> <span class="nc">StackItem</span><span class="p">(</span><span class="n">Item</span><span class="p">):</span>
</span><span class="line">    <span class="n">title</span> <span class="o">=</span> <span class="n">Field</span><span class="p">()</span>
</span><span class="line">    <span class="n">url</span> <span class="o">=</span> <span class="n">Field</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure>


<a name="Create.the.Spider"/>
<h3>Create the Spider</h3>

<p>Create a file called <em>stack_spider.py</em> in the “spiders” directory. This is where the magic happens – e.g., where we’ll tell Scrapy how to find the <em>exact</em> data we’re looking for. As you can imagine, this is <em>specific</em> to each individual web page that you wish to scrape.</p>

<p>Start by defining a class that inherits from Scrapy’s <code>Spider</code> and then adding attributes as needed:</p>

<figure class="code"><figcaption><span/></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">Spider</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">class</span> <span class="nc">StackSpider</span><span class="p">(</span><span class="n">Spider</span><span class="p">):</span>
</span><span class="line">    <span class="n">name</span> <span class="o">=</span> <span class="s">"stack"</span>
</span><span class="line">    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s">"stackoverflow.com"</span><span class="p">]</span>
</span><span class="line">    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
</span><span class="line">        <span class="s">"http://stackoverflow.com/questions?pagesize=50&amp;sort=newest"</span><span class="p">,</span>
</span><span class="line">    <span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>


<p>The first few variables are self-explanatory (<a href="http://doc.scrapy.org/en/1.0/topics/spiders.html#spider">docs</a>):</p>

<ul>
<li><code>name</code> defines the name of the Spider.</li>
<li><code>allowed_domains</code> contains the base-URLs for the allowed domains for the spider to crawl.</li>
<li><code>start_urls</code> is a list of URLs for the spider to start crawling from. All subsequent URLs will start from the data that the spider downloads from the URLS in <code>start_urls</code>.</li>
</ul>


<a name="XPath.Selectors"/>
<h3>XPath Selectors</h3>

<p>Next, Scrapy uses XPath selectors to extract data from a website. In other words, we can select certain parts of the HTML data based on a given XPath. As stated in Scrapy’s <a href="http://doc.scrapy.org/en/1.0/topics/selectors.html">documentation</a>, “XPath is a language for selecting nodes in XML documents, which can also be used with HTML.”</p>

<p>You can easily find a specific Xpath using Chrome’s Developer Tools. Simply inspect a specific HTML element, copy the XPath, and then tweak (as needed):</p>














<p>Developer Tools also gives you the ability to test XPath selectors in the JavaScript Console by using <code>$x</code> – i.e., <code>$x("//img")</code>:</p>














<p>Again, we basically tell Scrapy where to start looking for information based on a defined XPath. Let’s navigate to the <a href="http://stackoverflow.com/questions?pagesize=50&amp;sort=newest">Stack Overflow</a> site in Chrome and find the XPath selectors.</p>

<p>Right click on the first question and select “Inspect Element”:</p>














<p>Now grab the XPath for the <code>&lt;div class="summary"&gt;</code>, <code>//*[@id="question-summary-27624141"]/div[2]</code>, and then test it out in the JavaScript Console:</p>














<p>As you can tell, it just selects that <em>one</em> question. So we need to alter the XPath to grab <em>all</em> questions. Any ideas? It’s simple: <code>//div[@class="summary"]/h3</code>. What does this mean? Essentially, this XPath states: <em>Grab all <code>&lt;h3&gt;</code> elements that are children of a <code>&lt;div&gt;</code> that has a class of <code>summary</code></em>. Test this XPath out in the JavaScript Console.</p>

<blockquote><p>Notice how we are not using the actual XPath output from Chrome Developer Tools. In most cases, the output is just a helpful aside, which generally points you in the right direction for finding the working XPath.</p></blockquote>

<p>Now let’s update the <em>stack_spider.py</em> script:</p>

<figure class="code"><figcaption><span/></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">Spider</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">scrapy.selector</span> <span class="kn">import</span> <span class="n">Selector</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">class</span> <span class="nc">StackSpider</span><span class="p">(</span><span class="n">Spider</span><span class="p">):</span>
</span><span class="line">    <span class="n">name</span> <span class="o">=</span> <span class="s">"stack"</span>
</span><span class="line">    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s">"stackoverflow.com"</span><span class="p">]</span>
</span><span class="line">    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
</span><span class="line">        <span class="s">"http://stackoverflow.com/questions?pagesize=50&amp;sort=newest"</span><span class="p">,</span>
</span><span class="line">    <span class="p">]</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
</span><span class="line">        <span class="n">questions</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">response</span><span class="p">)</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">'//div[@class="summary"]/h3'</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<a name="Extract.the.Data"/>
<h3>Extract the Data</h3>

<p>We still need to parse and scrape the data we want, which falls within <code>&lt;div class="summary"&gt;&lt;h3&gt;</code>. Again, update <em>stack_spider.py</em> like so:</p>

<figure class="code"><figcaption><span/></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">Spider</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">scrapy.selector</span> <span class="kn">import</span> <span class="n">Selector</span>
</span><span class="line">
</span><span class="line"><span class="kn">from</span> <span class="nn">stack.items</span> <span class="kn">import</span> <span class="n">StackItem</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">class</span> <span class="nc">StackSpider</span><span class="p">(</span><span class="n">Spider</span><span class="p">):</span>
</span><span class="line">    <span class="n">name</span> <span class="o">=</span> <span class="s">"stack"</span>
</span><span class="line">    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s">"stackoverflow.com"</span><span class="p">]</span>
</span><span class="line">    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
</span><span class="line">        <span class="s">"http://stackoverflow.com/questions?pagesize=50&amp;sort=newest"</span><span class="p">,</span>
</span><span class="line">    <span class="p">]</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
</span><span class="line">        <span class="n">questions</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">response</span><span class="p">)</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">'//div[@class="summary"]/h3'</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">        <span class="k">for</span> <span class="n">question</span> <span class="ow">in</span> <span class="n">questions</span><span class="p">:</span>
</span><span class="line">            <span class="n">item</span> <span class="o">=</span> <span class="n">StackItem</span><span class="p">()</span>
</span><span class="line">            <span class="n">item</span><span class="p">[</span><span class="s">'title'</span><span class="p">]</span> <span class="o">=</span> <span class="n">question</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span>
</span><span class="line">                <span class="s">'a[@class="question-hyperlink"]/text()'</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">            <span class="n">item</span><span class="p">[</span><span class="s">'url'</span><span class="p">]</span> <span class="o">=</span> <span class="n">question</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span>
</span><span class="line">                <span class="s">'a[@class="question-hyperlink"]/@href'</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">            <span class="k">yield</span> <span class="n">item</span>
</span></code></pre></td></tr></table></div></figure>


<p>`</p>

<p>We are iterating through the <code>questions</code> and assigning the <code>title</code> and <code>url</code> values from the scraped data. Be sure to test out the XPath selectors in the JavaScript Console within Chrome Developer Tools – e.g., <code>$x('//div[@class="summary"]/h3/a[@class="question-hyperlink"]/text()')</code> and <code>$x('//div[@class="summary"]/h3/a[@class="question-hyperlink"]/@href')</code>.</p>

<a name="Test"/>
<h2>Test</h2>

<p>Ready for the first test? Simply run the following command within the “stack” directory:</p>

<figure class="code"><figcaption><span/></figcaption></figure>


<p>Along with the Scrapy stack trace, you should see 50 question titles and URLs outputted. You can render the output to a JSON file with this little command:</p>

<figure class="code"><figcaption><span/></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"><span class="nv">$ </span>scrapy crawl stack -o items.json -t json
</span></code></pre></td></tr></table></div></figure>


<p>We’ve now implemented our Spider based on our data that we are seeking. Now we need to store the scraped data within MongoDB.</p>

<a name="Store.the.Data.in.MongoDB"/>
<h2>Store the Data in MongoDB</h2>

<p>Each time an item is returned, we want to validate the data and then add it to a Mongo collection.</p>

<p>The initial step is to create the database that we plan to use to save all of our crawled data. Open <em>settings.py</em> and specify the <a href="http://doc.scrapy.org/en/1.0/topics/item-pipeline.html">pipeline</a> and add the database settings:</p>

<figure class="code"><figcaption><span/></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">[</span><span class="s">'stack.pipelines.MongoDBPipeline'</span><span class="p">,</span> <span class="p">]</span>
</span><span class="line">
</span><span class="line"><span class="n">MONGODB_SERVER</span> <span class="o">=</span> <span class="s">"localhost"</span>
</span><span class="line"><span class="n">MONGODB_PORT</span> <span class="o">=</span> <span class="mi">27017</span>
</span><span class="line"><span class="n">MONGODB_DB</span> <span class="o">=</span> <span class="s">"stackoverflow"</span>
</span><span class="line"><span class="n">MONGODB_COLLECTION</span> <span class="o">=</span> <span class="s">"questions"</span>
</span></code></pre></td></tr></table></div></figure>


<a name="Pipeline.Management"/>
<h3>Pipeline Management</h3>

<p>We’ve set up our spider to crawl and parse the HTML, and we’ve set up our database settings. Now we have to connect the two together through a pipeline in <em>pipelines.py</em>.</p>

<p><strong>Connect to Database</strong></p>

<p>First, let’s define a method to actually connect to the database:</p>

<figure class="code"><figcaption><span/></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">pymongo</span>
</span><span class="line">
</span><span class="line"><span class="kn">from</span> <span class="nn">scrapy.conf</span> <span class="kn">import</span> <span class="n">settings</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">class</span> <span class="nc">MongoDBPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="n">connection</span> <span class="o">=</span> <span class="n">pymongo</span><span class="o">.</span><span class="n">MongoClient</span><span class="p">(</span>
</span><span class="line">            <span class="n">settings</span><span class="p">[</span><span class="s">'MONGODB_SERVER'</span><span class="p">],</span>
</span><span class="line">            <span class="n">settings</span><span class="p">[</span><span class="s">'MONGODB_PORT'</span><span class="p">]</span>
</span><span class="line">        <span class="p">)</span>
</span><span class="line">        <span class="n">db</span> <span class="o">=</span> <span class="n">connection</span><span class="p">[</span><span class="n">settings</span><span class="p">[</span><span class="s">'MONGODB_DB'</span><span class="p">]]</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">collection</span> <span class="o">=</span> <span class="n">db</span><span class="p">[</span><span class="n">settings</span><span class="p">[</span><span class="s">'MONGODB_COLLECTION'</span><span class="p">]]</span>
</span></code></pre></td></tr></table></div></figure>


<p>Here, we create a class, <code>MongoDBPipeline()</code>, and we have a constructor function to initialize the class by defining the Mongo settings and then connecting to the database.</p>

<p><strong>Process the Data</strong></p>

<p>Next, we need to define a method to process the parsed data:</p>

<figure class="code"><figcaption><span/></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">pymongo</span>
</span><span class="line">
</span><span class="line"><span class="kn">from</span> <span class="nn">scrapy.conf</span> <span class="kn">import</span> <span class="n">settings</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">scrapy.exceptions</span> <span class="kn">import</span> <span class="n">DropItem</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">log</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">class</span> <span class="nc">MongoDBPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="n">connection</span> <span class="o">=</span> <span class="n">pymongo</span><span class="o">.</span><span class="n">MongoClient</span><span class="p">(</span>
</span><span class="line">            <span class="n">settings</span><span class="p">[</span><span class="s">'MONGODB_SERVER'</span><span class="p">],</span>
</span><span class="line">            <span class="n">settings</span><span class="p">[</span><span class="s">'MONGODB_PORT'</span><span class="p">]</span>
</span><span class="line">        <span class="p">)</span>
</span><span class="line">        <span class="n">db</span> <span class="o">=</span> <span class="n">connection</span><span class="p">[</span><span class="n">settings</span><span class="p">[</span><span class="s">'MONGODB_DB'</span><span class="p">]]</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">collection</span> <span class="o">=</span> <span class="n">db</span><span class="p">[</span><span class="n">settings</span><span class="p">[</span><span class="s">'MONGODB_COLLECTION'</span><span class="p">]]</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
</span><span class="line">        <span class="n">valid</span> <span class="o">=</span> <span class="bp">True</span>
</span><span class="line">        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">item</span><span class="p">:</span>
</span><span class="line">            <span class="k">if</span> <span class="ow">not</span> <span class="n">data</span><span class="p">:</span>
</span><span class="line">                <span class="n">valid</span> <span class="o">=</span> <span class="bp">False</span>
</span><span class="line">                <span class="k">raise</span> <span class="n">DropItem</span><span class="p">(</span><span class="s">"Missing {0}!"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
</span><span class="line">        <span class="k">if</span> <span class="n">valid</span><span class="p">:</span>
</span><span class="line">            <span class="bp">self</span><span class="o">.</span><span class="n">collection</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">item</span><span class="p">))</span>
</span><span class="line">            <span class="n">log</span><span class="o">.</span><span class="n">msg</span><span class="p">(</span><span class="s">"Question added to MongoDB database!"</span><span class="p">,</span>
</span><span class="line">                    <span class="n">level</span><span class="o">=</span><span class="n">log</span><span class="o">.</span><span class="n">DEBUG</span><span class="p">,</span> <span class="n">spider</span><span class="o">=</span><span class="n">spider</span><span class="p">)</span>
</span><span class="line">        <span class="k">return</span> <span class="n">item</span>
</span></code></pre></td></tr></table></div></figure>


<p>We establish a connection to the database, unpack the data, and then save it to the database. Now we can test again!</p>

<a name="Test"/>
<h2>Test</h2>

<p>Again, run the following command within the “stack” directory:</p>

<figure class="code"><figcaption><span/></figcaption></figure>


<blockquote><p><strong>NOTE</strong>: Make sure you have the <a href="http://docs.mongodb.org/v3.0/reference/program/mongod/">Mongo daemon</a> – <code>mongod</code> – running in a different terminal window.</p></blockquote>

<p>Hooray! We have successfully stored our crawled data into the database:</p>














<a name="Conclusion"/>
<h2>Conclusion</h2>

<p>This is a pretty simple example of using Scrapy to crawl and scrape a web page. The actual freelance project required the script to follow the pagination links and scrape each page using the <code>CrawlSpider</code> (<a href="http://doc.scrapy.org/en/1.0/topics/spiders.html#crawlspider">docs</a>), which is super easy to implement. Try implementing this on your own, and leave a comment below with the link to the Github repository for a quick code review. Need help? Start with <a href="https://github.com/realpython/stack-spider/blob/part1/stack/stack/spiders/stack_crawl.py">this script</a>, which is nearly complete. <strong>Then view <a href="https://realpython.com/blog/python/web-scraping-and-crawling-with-scrapy-and-mongodb/">Part 2</a> for the full solution!</strong></p>

<p>You can download the entire source code from the <a href="https://github.com/realpython/stack-spider/releases/tag/v1">Github repository</a>. Comment below with questions. Thanks for Reading!</p>

<p>Happy New Year!</p>

<p>:)</p>

<blockquote><p>Looking for more web scraping? Be sure to check out the <a href="https://realpython.com/courses">Real Python courses</a>. Looking to hire a professional web scraper? Check out <a href="http://www.goscrape.com/">GoScrape</a>.</p></blockquote>
</div>


      </div></body></html>