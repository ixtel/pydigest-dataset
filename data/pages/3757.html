<html><body><div><div class="content_body" data-subscribe="true" data-view="content#body"><p>Discussions criticizing Python often talk about how it is difficult to use Python for multithreaded work, pointing fingers at what is known as the global interpreter lock (affectionately referred to as the “GIL”) that prevents multiple threads of Python code from running simultaneously. Due to this, the threading module doesn’t quite behave the way you would expect it to if you’re not a <a href="https://www.toptal.com/python">Python developer</a> and you are coming from other languages such as C++ or Java. It must be made clear that one can still write code in Python that runs concurrently or in parallel and make a stark difference resulting performance, as long as certain things are taken into consideration. If you haven’t read it yet, I suggest you take a look at Eqbal Quran’s <a href="https://www.toptal.com/ruby/ruby-concurrency-and-parallelism-a-practical-primer">article on concurrency and parallelism in Ruby</a> here on the Toptal blog.</p>

<p>In this Python concurrency tutorial, we will write a small Python script to download the top popular images from Imgur. We will start with a version that downloads images sequentially, or one at a time. As a prerequisite, you will have to register <a href="https://api.imgur.com/oauth2/addclient">an application on Imgur</a>. If you do not have an Imgur account already, please create one first.</p>

<p>The scripts in this tutorial has been tested with Python 3.4.2. With some changes, they should also run with Python 2 - urllib is what has changed the most between these two versions of Python.</p>

<h2 id="getting-started-with-multithreading-in-python">Getting Started with Multithreading in Python</h2>

<p>Let us start by creating a Python module, named “download.py”. This file will contain all the functions necessary to fetch the list of images and download them. We will split these functionalities into three separate functions:</p>

<ul>
  <li>get_links</li>
  <li>download_link</li>
  <li>setup_download_dir</li>
</ul>

<p>The third function, “setup_download_dir”, will be used to create a download destination directory if it doesn’t already exist.</p>

<p>Imgur’s API requires HTTP requests to bear the “Authorization” header with the client ID. You can find this client ID from the dashboard of the application that you have registered on Imgur, and the response will be JSON encoded. We can use Python’s standard JSON library to decode it. Downloading the image is an even simpler task, as all you have to do is fetch the image by its URL and write it to a file.</p>

<p><img src="//assets.toptal.io/uploads/blog/image/952/toptal-blog-image-1426661299127.jpg" alt="Python multithreading"/></p>

<p>This is what the script looks like:</p>

<pre><code class="language-py">import json
import logging
import os
from pathlib import Path
from urllib.request import urlopen, Request

logger = logging.getLogger(__name__)

def get_links(client_id):
   headers = {'Authorization': 'Client-ID {}'.format(client_id)}
   req = Request('https://api.imgur.com/3/gallery/', headers=headers, method='GET')
   with urlopen(req) as resp:
       data = json.loads(resp.readall().decode('utf-8'))
   return map(lambda item: item['link'], data['data'])

def download_link(directory, link):
   logger.info('Downloading %s', link)
   download_path = directory / os.path.basename(link)
   with urlopen(link) as image, download_path.open('wb') as f:
       f.write(image.readall())

def setup_download_dir():
   download_dir = Path('images')
   if not download_dir.exists():
       download_dir.mkdir()
   return download_dir
</code></pre>

<p>Next, we will need to write a module that will use these functions to download the images, one by one. We will name this “single.py”. This will contain the main function of our first, naive version of the Imgur image downloader. The module will retrieve the Imgur client ID in the environment variable “IMGUR_CLIENT_ID”. It will invoke the “setup_download_dir” to create the download destination directory. Finally, it will fetch a list of images using the get_links function, filter out all GIF and album URLs, and then use “download_link” to download and save each of those images to the disk. Here is what “single.py” looks like:</p>

<pre><code class="language-py">import logging
import os
from time import time

from download import setup_download_dir, get_links, download_link

logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logging.getLogger('requests').setLevel(logging.CRITICAL)
logger = logging.getLogger(__name__)

def main():
   ts = time()
   client_id = os.getenv('IMGUR_CLIENT_ID')
   if not client_id:
       raise Exception("Couldn't find IMGUR_CLIENT_ID environment variable!")
   download_dir = setup_download_dir()
   links = [l for l in get_links(client_id) if l.endswith('.jpg')]
   for link in links:
       download_link(download_dir, link)
   print('Took {}s'.format(time() - ts))

if __name__ == '__main__':
   main()
</code></pre>

<p>On my laptop, this script took 19.4 seconds to download 91 images. Please do note that these numbers may vary based on the network you are on. 19.4 seconds isn’t terribly long, but what if we wanted to download more pictures? Perhaps 900 images, instead of 90. With an average of 0.2 seconds per picture, 900 images would take approximately 3 minutes. For 9000 pictures it would take 30 minutes. The good news is that by introducing concurrency or parallelism, we can speed this up dramatically.</p>

<p>All subsequent code examples will only show import statements that are new and specific to those. For convenience, all of these Python scripts can be found in <a href="https://github.com/volker48/python-concurrency">this GitHub repository</a>.</p>

<h2 id="using-threads-for-concurrency-and-parallelism">Using Threads for Concurrency and Parallelism</h2>

<p>Threading is one of the most well known approaches to attaining Python concurrency and parallelism. Threading is a feature usually provided by the operating system. Threads are lighter than processes, and share the same memory space.</p>

<p><img src="//assets.toptal.io/uploads/blog/image/953/toptal-blog-image-1426661478729.jpg" alt="Threading - Python concurrency and parallelism"/></p>

<p>In our Python thread tutorial, we will write a new module to replace “single.py”. This module will create a pool of 8 threads, making a total of 9 threads including the main thread. I chose 8 worker threads, because my computer has 8 CPU cores and one worker thread per core seemed a good number for how many threads to run at once. In practice, this number is chosen much more carefully based on other factors, such as other applications and services running on the same machine.</p>

<p>This is almost the same as the previous one, with the exception that we now have a new class, DownloadWorker, that is a descendent of the Thread class. The run method has been overridden, which runs an infinite loop. On every iteration, it calls “self.queue.get()” to try and fetch an URL to from a thread-safe queue. It blocks until there is an item in the queue for the worker to process. Once the worker receives an item from the queue, it then calls the same “download_link” method that was used in the previous script to download the image to the images directory. After the download is finished, the worker signals the queue that that task is done. This is very important, because the Queue keeps track of how many tasks were enqueued. The call to “queue.join()” would block the main thread forever if the workers did not signal that they completed a task.</p>

<pre><code class="language-py">from queue import Queue
from threading import Thread

class DownloadWorker(Thread):
   def __init__(self, queue):
       Thread.__init__(self)
       self.queue = queue

   def run(self):
       while True:
           # Get the work from the queue and expand the tuple
           directory, link = self.queue.get()
           download_link(directory, link)
           self.queue.task_done()

def main():
   ts = time()
   client_id = os.getenv('IMGUR_CLIENT_ID')
   if not client_id:
       raise Exception("Couldn't find IMGUR_CLIENT_ID environment variable!")
   download_dir = setup_download_dir()
   links = [l for l in get_links(client_id) if l.endswith('.jpg')]
   # Create a queue to communicate with the worker threads
   queue = Queue()
   # Create 8 worker threads
   for x in range(8):
       worker = DownloadWorker(queue)
       # Setting daemon to True will let the main thread exit even though the workers are blocking
       worker.daemon = True
       worker.start()
   # Put the tasks into the queue as a tuple
   for link in links:
       logger.info('Queueing {}'.format(link))
       queue.put((download_dir, link))
   # Causes the main thread to wait for the queue to finish processing all the tasks
   queue.join()
   print('Took {}'.format(time() - ts))
</code></pre>

<p>Running this script on the same machine used earlier results in a download time of 4.1 seconds! Thats 4.7 times faster than the previous example. While this is much faster, it is worth mentioning that only one thread was executing at a time throughout this process due to the GIL. Therefore, this code is concurrent but not parallel. The reason it is still faster is because this is an IO bound task. The processor is hardly breaking a sweat while downloading these images, and the majority of the time is spent waiting for the network. This is why threading can provide a large speed increase. The processor can switch between the threads whenever one of them is ready to do some work. Using the threading module in Python or any other interpreted language with a GIL can actually result in reduced performance. If your code is performing a CPU bound task, such as decompressing gzip files, using the threading module will result in a slower execution time. For CPU bound tasks and truly parallel execution, we can use the multiprocessing module.</p>

<p>While the de facto reference Python implementation - CPython - has a GIL, this is not true of all Python implementations. For example, IronPython, a Python implementation using the .NET framework does not have a GIL, and neither does Jython, the Java based implementation. You can find a list of working Python implementations <a href="https://wiki.python.org/moin/PythonImplementations#Working_Implementations">here</a>.</p>



<h2 id="spawning-multiple-processes">Spawning Multiple Processes</h2>

<p>The multiprocessing module is easier to drop in than the threading module, as we don’t need to add a class like the threading example. The only changes we need to make are in the main function.</p>

<p><img src="//assets.toptal.io/uploads/blog/image/954/toptal-blog-image-1426661499995.jpg" alt="multiprocessing module"/></p>

<p>To use multiple processes we create a multiprocessing Pool. With the map method it provides, we will pass the list of URLs to the pool, which in turn will spawn 8 new processes and use each one to download the images in parallel. This is true parallelism, but it comes with a cost. The entire memory of the script is copied into each subprocess that is spawned. In this simple example it isn’t a big deal, but it can easily become serious overhead for non-trivial programs.</p>

<pre><code class="language-py">from functools import partial
from multiprocessing.pool import Pool

def main():
   ts = time()
   client_id = os.getenv('IMGUR_CLIENT_ID')
   if not client_id:
       raise Exception("Couldn't find IMGUR_CLIENT_ID environment variable!")
   download_dir = setup_download_dir()
   links = [l for l in get_links(client_id) if l.endswith('.jpg')]
   download = partial(download_link, download_dir)
   with Pool(8) as p:
       p.map(download, links)
   print('Took {}s'.format(time() - ts))
</code></pre>

<h2 id="distributing-to-multiple-workers">Distributing to Multiple Workers</h2>

<p>While the threading and multiprocessing modules are great for scripts that are running on your personal computer, what should you do if you want the work to be done on a different machine, or you need to scale up to more than the CPU on one machine can handle? A great use case for this is long-running back-end tasks for web applications. If you have some long running tasks, you don’t want to spin up a bunch of subprocesses or threads on the same machine that need to be running the rest of your application code. This will degrade the performance of your application for all of your users. What would be great is to be able to run these jobs on another machine, or many other machines.</p>

<p>A great Python library for this task is <a href="http://python-rq.org/">RQ</a>, a very simple yet powerful library. You first enqueue a function and its arguments using the library. This <a href="https://docs.python.org/3.4/library/pickle.html">pickles</a> the function call representation, which is then appended to a <a href="http://redis.io/">Redis</a> list. Enqueueing the job is the first step, but will not do anything yet. We also need at least one worker to listen on that job queue.</p>

<p><img src="//assets.toptal.io/uploads/blog/image/959/toptal-blog-image-1426691627082.jpg" alt="RQ python library"/></p>

<p>The first step is to install and run a Redis server on your computer, or have access to a running Redis server. After that, there are only a few small changes made to the existing code. We first create an instance of an RQ Queue and pass it an instance of a Redis server from the <a href="https://github.com/andymccurdy/redis-py">redis-py library</a>. Then, instead of just calling our “download_link” method, we call “q.enqueue(download_link, download_dir, link)”. The enqueue method takes a function as its first argument, then any other arguments or keyword arguments are passed along to that function when the job is actually executed.</p>

<p>One last step we need to do is to start up some workers. RQ provides a handy script to run workers on the default queue. Just run “rqworker” in a terminal window and it will start a worker listening on the default queue. Please make sure your current working directory is the same as where the scripts reside in. If you want to listen to a different queue, you can run “rqworker queue_name” and it will listen to that named queue. The great thing about RQ is that as long as you can connect to Redis, you can run as many workers as you like on as many different machines as you like; therefore, it is very easy to scale up as your application grows. Here is the source for the RQ version:</p>

<pre><code class="language-py">from redis import Redis
from rq import Queue

def main():
   client_id = os.getenv('IMGUR_CLIENT_ID')
   if not client_id:
       raise Exception("Couldn't find IMGUR_CLIENT_ID environment variable!")
   download_dir = setup_download_dir()
   links = [l for l in get_links(client_id) if l.endswith('.jpg')]
   q = Queue(connection=Redis(host='localhost', port=6379))
   for link in links:
       q.enqueue(download_link, download_dir, link)
</code></pre>

<p>However, RQ is not the only Python job queue solution. RQ is easy to use and covers simple use cases extremely well, but if more advanced options are required, other job queue solutions (such as <a href="http://www.celeryproject.org/">Celery</a>) can be used.</p>

<h2 id="conclusion">Conclusion</h2>

<p>If your code is IO bound, both multiprocessing and multithreading in Python will work for you. Multiprocessing is a easier to just drop in than threading, but has a higher memory overhead. If your code is CPU bound, multiprocessing is most likely going to be the better choice - especially if the target machine has multiple cores or CPUs. For web applications, and when you need to scale the work across multiple machines, RQ is going to be better for you.</p>


<div class="blog_author_big_banner is-active" data-ga-action="ClickedOnHireTheAuthor_bottom" data-ga-label="beginners-guide-to-concurrency-and-parallelism-in-python" data-view="content#bottom_banner"><h2>About the author</h2><div class="skill_talent for-blog_post"><div class="skill_talent-main"><a class="link skill_talent-name" href="/resume/marcus-mccurdy">Marcus McCurdy, United States</a><div class="clearfix"><p class="skill_talent-member_since">member since November 10, 2014</p></div><div class="skill_talent-description"><p>Marcus has a Bachelor's in Computer Engineering and a Master's in Computer Science. He is a talented programmer, and excels most at back-end development. However, he is comfortable creating polished products as a full stack developer. </p><a class="link" href="/resume/marcus-mccurdy">[click to continue...]</a></div></div></div></div><p class="template" data-role="subscribe_template" data-template="{&quot;html&quot;:&quot;\u003cdiv class=\&quot;embeddable_form-wrapper\&quot; data-view=\&quot;blog_subscribe#form\&quot;\u003e\u003cform action=\&quot;/blog/subscription\&quot; class=\&quot;embeddable_form for-post\&quot; data-entity=\&quot;blog_subscription\&quot; data-remote=\&quot;\&quot; data-view=\&quot;form#form\&quot; method=\&quot;post\&quot;\u003e\u003cdiv class=\&quot;embeddable_form-step is-email_form is-current\&quot; data-role=\&quot;step\&quot;\u003e\u003cdiv class=\&quot;embeddable_form-row is-label\&quot;\u003e\u003cdiv class=\&quot;embeddable_form-label_title\&quot;\u003eLike what you\u0026#39;re reading?\u003c/div\u003e\u003cdiv class=\&quot;embeddable_form-label\&quot;\u003eGet the latest updates first.\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\&quot;embeddable_form-row form-field is-email_field\&quot;\u003e\u003cinput autocomplete=\&quot;off\&quot; class=\&quot;input is-medium\&quot; data-role=\&quot;email\&quot; name=\&quot;blog_subscription[email]\&quot; placeholder=\&quot;Enter your email address...\&quot; type=\&quot;text\&quot; /\u003e\u003cinput name=\&quot;blog_subscription[vertical]\&quot; type=\&quot;hidden\&quot; value=\&quot;developers\&quot; /\u003e\u003cinput name=\&quot;from_widget\&quot; type=\&quot;hidden\&quot; value=\&quot;true\&quot; /\u003e\u003c/div\u003e\u003cdiv class=\&quot;embeddable_form-row is-submit\&quot;\u003e\u003cinput class=\&quot;button is-green_candy is-default is-full_width\&quot; data-loader-text=\&quot;Subscribing...\&quot; data-role=\&quot;submit\&quot; type=\&quot;submit\&quot; value=\&quot;Get Exclusive Updates\&quot; /\u003e\u003c/div\u003e\u003cdiv class=\&quot;embeddable_form-row is-privacy\&quot;\u003e\u003cdiv class=\&quot;embeddable_form-privacy\&quot;\u003e\u003cdiv class=\&quot;embeddable_form-privacy_icon\&quot;\u003e\u003c/div\u003e\u003cdiv class=\&quot;embeddable_form-privacy_text\&quot;\u003eNo spam. Just great engineering and design posts.\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\&quot;embeddable_form-step is-confirmation\&quot; data-role=\&quot;step\&quot;\u003e\u003cdiv class=\&quot;embeddable_form-row is-label is-success\&quot;\u003e\u003cdiv class=\&quot;embeddable_form-label_title\&quot;\u003eLike what you\u0026#39;re reading?\u003c/div\u003e\u003cdiv class=\&quot;embeddable_form-label\&quot;\u003eGet the latest updates first.\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\&quot;embeddable_form-row is-success\&quot;\u003e\u003cdiv class=\&quot;embeddable_form-label is-header\&quot;\u003eThank you for subscribing!\u003c/div\u003e\u003cdiv class=\&quot;embeddable_form-label\&quot;\u003eYou can edit your subscription preferences \u003ca href='#' data-role='preferences_link'\u003ehere\u003c/a\u003e.\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\&quot;embeddable_form-row is-success\&quot;\u003e\u003cdiv class=\&quot;embeddable_form-done\&quot;\u003e\u003cul class=\&quot;blog_follow_us\&quot;\u003e\u003cul class=\&quot;social_share\&quot;\u003e\u003cli class=\&quot;social_share-item is-facebook\&quot;\u003e\u003cdiv class=\&quot;fb-like\&quot; data-href=\&quot;https://www.toptal.com/blog\&quot; data-layout=\&quot;button_count\&quot; data-send=\&quot;false\&quot; data-show-faces=\&quot;false\&quot; data-width=\&quot;450\&quot;\u003e\u003c/div\u003e\u003c/li\u003e\u003cli class=\&quot;social_share-item is-twitter_follow\&quot;\u003e\u003ca class=\&quot;twitter-follow-button\&quot; data-show-count=\&quot;true\&quot; href=\&quot;https://twitter.com/toptalllc\&quot;\u003eFollow @toptalllc\u003c/a\u003e\u003c/li\u003e\u003cli class=\&quot;social_share-item is-google_plus_follow\&quot;\u003e\u003cdiv class=\&quot;g-follow\&quot; data-annotation=\&quot;bubble\&quot; data-height=\&quot;20\&quot; data-href=\&quot;https://plus.google.com/109028009576318848424\&quot; data-rel=\&quot;publisher\&quot;\u003e\u003c/div\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/form\u003e\u003c/div\u003e&quot;}"/></div></div></body></html>