<html><body><div><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-seq2seq" class="anchor" href="#seq2seq" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Seq2seq</h1>

<p>Sequence to Sequence Learning with Keras</p>

<p><strong>Hi!</strong> You have just found Seq2seq. Seq2seq is a sequence to sequence learning add-on for the python deep learning library <a href="http://www.keras.io">Keras</a>. Using Seq2seq, you can build and train sequence-to-sequence neural network models in Keras. Such models are useful for machine translation, chatbots (see <a href="http://arxiv.org/pdf/1506.05869v1.pdf">[4]</a>), parsers, or whatever that comes to your mind.</p>

<p><a href="https://camo.githubusercontent.com/242210d7d0151cae91107ee63bff364a860db5dd/687474703a2f2f6936342e74696e797069632e636f6d2f333031333674652e706e67" target="_blank"><img src="https://camo.githubusercontent.com/242210d7d0151cae91107ee63bff364a860db5dd/687474703a2f2f6936342e74696e797069632e636f6d2f333031333674652e706e67" alt="seq2seq" data-canonical-src="http://i64.tinypic.com/30136te.png"/></a></p>

<h1><a id="user-content-getting-started" class="anchor" href="#getting-started" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Getting started</h1>

<p>Seq2seq contains modular and reusable layers that you can use to build your own seq2seq models as well as built-in models that work out of the box. Seq2seq models can be compiled as they are or added as layers to a bigger model. Every Seq2seq model has 2 primary layers : the encoder and the  decoder. Generally, the encoder encodes the input  sequence to an internal representation called 'context vector' which is used by the decoder to generate the output sequence. The lengths of input and output sequences can be different, as there is no explicit one on one relation between the input and output sequences. In addition to the encoder and decoder layers, a Seq2seq model may also contain layers such as the left-stack (Stacked LSTMs on the encoder side), the right-stack (Stacked LSTMs on the decoder side), resizers (for shape compatibility between the encoder and the decoder) and dropout layers to avoid overfitting. The source code is heavily documented, so lets go straight to the examples:</p>

<p><strong>A simple Seq2seq model:</strong></p>

<div class="highlight highlight-source-python"><pre><span class="pl-k">import</span> seq2seq
<span class="pl-k">from</span> seq2seq.models <span class="pl-k">import</span> SimpleSeq2seq

model <span class="pl-k">=</span> SimpleSeq2seq(<span class="pl-v">input_dim</span><span class="pl-k">=</span><span class="pl-c1">5</span>, <span class="pl-v">hidden_dim</span><span class="pl-k">=</span><span class="pl-c1">10</span>, <span class="pl-v">output_length</span><span class="pl-k">=</span><span class="pl-c1">8</span>, <span class="pl-v">output_dim</span><span class="pl-k">=</span><span class="pl-c1">8</span>)
model.compile(<span class="pl-v">loss</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>mse<span class="pl-pds">'</span></span>, <span class="pl-v">optimizer</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>rmsprop<span class="pl-pds">'</span></span>)</pre></div>

<p>That's it! You have successfully compiled a minimal Seq2seq model! Next, let's build a 6 layer deep Seq2seq model (3 layers for encoding, 3 layers for decoding).</p>

<p><strong>Deep Seq2seq models:</strong></p>

<div class="highlight highlight-source-python"><pre><span class="pl-k">import</span> seq2seq
<span class="pl-k">from</span> seq2seq.models <span class="pl-k">import</span> SimpleSeq2seq

model <span class="pl-k">=</span> SimpleSeq2seq(<span class="pl-v">input_dim</span><span class="pl-k">=</span><span class="pl-c1">5</span>, <span class="pl-v">hidden_dim</span><span class="pl-k">=</span><span class="pl-c1">10</span>, <span class="pl-v">output_length</span><span class="pl-k">=</span><span class="pl-c1">8</span>, <span class="pl-v">output_dim</span><span class="pl-k">=</span><span class="pl-c1">8</span>, <span class="pl-v">depth</span><span class="pl-k">=</span><span class="pl-c1">3</span>)
model.compile(<span class="pl-v">loss</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>mse<span class="pl-pds">'</span></span>, <span class="pl-v">optimizer</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>rmsprop<span class="pl-pds">'</span></span>)</pre></div>

<p>Notice that we have specified the depth for both encoder and decoder as 3, and your model has a total depth of 3 + 3 = 6. You can also specify different depths for the encoder and the decoder. Example:</p>

<div class="highlight highlight-source-python"><pre><span class="pl-k">import</span> seq2seq
<span class="pl-k">from</span> seq2seq.models <span class="pl-k">import</span> SimpleSeq2seq

model <span class="pl-k">=</span> SimpleSeq2seq(<span class="pl-v">input_dim</span><span class="pl-k">=</span><span class="pl-c1">5</span>, <span class="pl-v">hidden_dim</span><span class="pl-k">=</span><span class="pl-c1">10</span>, <span class="pl-v">output_length</span><span class="pl-k">=</span><span class="pl-c1">8</span>, <span class="pl-v">output_dim</span><span class="pl-k">=</span><span class="pl-c1">20</span>, <span class="pl-v">depth</span><span class="pl-k">=</span>(<span class="pl-c1">4</span>, <span class="pl-c1">5</span>))
model.compile(<span class="pl-v">loss</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>mse<span class="pl-pds">'</span></span>, <span class="pl-v">optimizer</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>rmsprop<span class="pl-pds">'</span></span>)</pre></div>

<p>Notice that the depth is specified as tuple, <code>(4, 5)</code>. Which means your encoder will be 4 layers deep whereas your decoder will be 5 layers deep. And your model will have a total depth of 4 + 5 = 9.</p>

<p><strong>Advanced Seq2seq models:</strong></p>

<p>Until now, you have been using the <code>SimpleSeq2seq</code> model, which is a very minimalistic model. In the actual Seq2seq implementation described in <a href="http://arxiv.org/abs/1409.3215">[1]</a>, the hidden state of the encoder is transferred to decoder. Also, the output of decoder at each timestep becomes the input to the decoder at the next time step. To make things more complicated, the hidden state is propogated throughout the LSTM stack. But you  have no reason to worry, as we have a built-in model that does all that out of the box. Example:</p>

<div class="highlight highlight-source-python"><pre><span class="pl-k">import</span> seq2seq
<span class="pl-k">from</span> seq2seq.models <span class="pl-k">import</span> Seq2seq

model <span class="pl-k">=</span> Seq2seq(<span class="pl-v">batch_input_shape</span><span class="pl-k">=</span>(<span class="pl-c1">16</span>, <span class="pl-c1">7</span>, <span class="pl-c1">5</span>), <span class="pl-v">hidden_dim</span><span class="pl-k">=</span><span class="pl-c1">10</span>, <span class="pl-v">output_length</span><span class="pl-k">=</span><span class="pl-c1">8</span>, <span class="pl-v">output_dim</span><span class="pl-k">=</span><span class="pl-c1">20</span>, <span class="pl-v">depth</span><span class="pl-k">=</span><span class="pl-c1">4</span>)
model.compile(<span class="pl-v">loss</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>mse<span class="pl-pds">'</span></span>, <span class="pl-v">optimizer</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>rmsprop<span class="pl-pds">'</span></span>)</pre></div>

<p>Note that we had to specify the complete input shape, including the samples dimensions. This is because we need a static hidden state(similar to a stateful RNN) for transferring it across layers. By the way, Seq2seq models also support the <code>stateful</code> argument, in case you need it.</p>

<p>You can also experiment with the hidden state propogation turned  off. Simply set the arguments <code>broadcast_state</code> and <code>inner_broadcast_state</code> to <code>False</code>.</p>

<p><strong>Peeky Seq2seq model</strong>:</p>

<p>Let's not stop there. Let's build a model similar to <a href="http://arxiv.org/abs/1406.1078">cho et al 2014</a>, where the decoder gets a 'peek' at the context vector at every timestep.</p>

<p><a href="https://camo.githubusercontent.com/7f690d451036938a51e62feb77149c8bb4be6675/687474703a2f2f6936342e74696e797069632e636f6d2f333032617168692e706e67" target="_blank"><img src="https://camo.githubusercontent.com/7f690d451036938a51e62feb77149c8bb4be6675/687474703a2f2f6936342e74696e797069632e636f6d2f333032617168692e706e67" alt="cho et al 2014" data-canonical-src="http://i64.tinypic.com/302aqhi.png"/></a></p>

<p>To achieve this, simply add the argument <code>peek=True</code>:</p>

<div class="highlight highlight-source-python"><pre><span class="pl-k">import</span> seq2seq
<span class="pl-k">from</span> seq2seq.models <span class="pl-k">import</span> Seq2seq

model <span class="pl-k">=</span> Seq2seq(<span class="pl-v">batch_input_shape</span><span class="pl-k">=</span>(<span class="pl-c1">16</span>, <span class="pl-c1">7</span>, <span class="pl-c1">5</span>), <span class="pl-v">hidden_dim</span><span class="pl-k">=</span><span class="pl-c1">10</span>, <span class="pl-v">output_length</span><span class="pl-k">=</span><span class="pl-c1">8</span>, <span class="pl-v">output_dim</span><span class="pl-k">=</span><span class="pl-c1">20</span>, <span class="pl-v">depth</span><span class="pl-k">=</span><span class="pl-c1">4</span>, <span class="pl-v">peek</span><span class="pl-k">=</span><span class="pl-c1">True</span>)
model.compile(<span class="pl-v">loss</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>mse<span class="pl-pds">'</span></span>, <span class="pl-v">optimizer</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>rmsprop<span class="pl-pds">'</span></span>)</pre></div>

<p><strong>Seq2seq model with attention:</strong></p>

<p><a href="https://camo.githubusercontent.com/0e2e4e5fb2dd47846c2fe027737a5df5e711df1b/687474703a2f2f6936342e74696e797069632e636f6d2f6132727733642e706e67" target="_blank"><img src="https://camo.githubusercontent.com/0e2e4e5fb2dd47846c2fe027737a5df5e711df1b/687474703a2f2f6936342e74696e797069632e636f6d2f6132727733642e706e67" alt="Attention Seq2seq" data-canonical-src="http://i64.tinypic.com/a2rw3d.png"/></a></p>

<p>Let's not stop there either. In all the models described above, there is no allignment between the input sequence elements and the output sequence elements. But for machine translation, learning a soft allignment between the input and output sequences imporves performance.<a href="http://arxiv.org/pdf/1409.0473v6.pdf">[3]</a>. The Seq2seq framework includes a ready made attention model which does the same. Note that in the attention model, there is no hidden state propogation, and a bidirectional LSTM encoder is used by default. Example:</p>

<div class="highlight highlight-source-python"><pre><span class="pl-k">import</span> seq2seq
<span class="pl-k">from</span> seq2seq.models <span class="pl-k">import</span> AttentionSeq2seq

model <span class="pl-k">=</span> AttentionSeq2seq(<span class="pl-v">input_dim</span><span class="pl-k">=</span><span class="pl-c1">5</span>, <span class="pl-v">input_length</span><span class="pl-k">=</span><span class="pl-c1">7</span>, <span class="pl-v">hidden_dim</span><span class="pl-k">=</span><span class="pl-c1">10</span>, <span class="pl-v">output_length</span><span class="pl-k">=</span><span class="pl-c1">8</span>, <span class="pl-v">output_dim</span><span class="pl-k">=</span><span class="pl-c1">20</span>, <span class="pl-v">depth</span><span class="pl-k">=</span><span class="pl-c1">4</span>)
model.compile(<span class="pl-v">loss</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>mse<span class="pl-pds">'</span></span>, <span class="pl-v">optimizer</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>rmsprop<span class="pl-pds">'</span></span>)</pre></div>

<p>As you can see, in the attention model you need not specify the samples dimension as there are no static hidden states involved(But you have to if you are building a stateful Seq2seq model).
Note:  You  can set the argument <code>bidirectional=False</code> if you wish not to use a bidirectional encoder.</p>

<h1><a id="user-content-final-words" class="anchor" href="#final-words" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Final Words</h1>

<p>That's all for now. Hope you love this library. For any questions you might have, create an issue and I will get in touch. You can also contribute to this project by reporting bugs, adding new examples, datasets or models.</p>

<p><strong>Installation:</strong></p>

<p><code>sudo pip install git+ssh://github.com/farizrahman4u/seq2seq.git</code></p>

<p><strong>Requirements:</strong></p>



<p><strong>Working Example:</strong></p>



<p><strong>Papers:</strong></p>


</article>
  </div></body></html>