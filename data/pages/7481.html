<html><body><div><div id="entry" class="usertext">
<p>Catching up on my <a href="http://softwaremaniacs.org/blog/category/ijson-in-rust/en/">Rust learning diaries</a>… Today I'm going to tell you about releaving my Lexer from its Pythonic legacy and what tangible results it produced, beside just being The Right Thing™.</p>
<p><a name="more"/></p>
<h2>Basic idea</h2>
<p>The original lexer yielded three kinds of lexemes:</p>
<ul>
<li>strings enclosed in quotes: <code>"..."</code></li>
<li>multi-character literals and numbers: <code>[a-z0-9eE\.\+-]+</code></li>
<li>single-character lexemes: brackets, braces, commas, colons, etc.</li>
</ul>
<p>Type-wise all of them were <em>strings</em>, and it was the job of the parser to check what kind of lexemes they were: a known literal, something starting with a quote or something parsable as a number… or an error, failing all else.</p>
<p>This made total sense in Python where I, for example, just used a single generalized regexp to parse all non-string lexemes. It allowed for a very simple and readable code and it's in fact the only right way to parse byte buffers in an untyped GC-powered language where dealing with individual bytes introduces too much performance overhead. </p>
<p>In Rust though it simply felt foreign because the lexer already has an intimate understanding of what is it that it parses — something starting with <code>"</code>, or <code>+|-|0..9</code>, or <code>{</code>, … — it has to explicitly check them all anyway. Hence it seemed silly to just drop this intrinsic type information on the floor and clump everything back into strings.</p>
<p>Also I had a suspicion that it should affect performance quite significantly, as I had to allocate memory for and copy all those small string pieces. Lots of allocations and copying is never good!</p>
<h2>Process</h2>
<p>I <a href="https://github.com/isagalaev/ijson-rust/commit/02951fef7525c623b3f">started</a> by introducing a dedicated <code>Lexeme</code> type distinguishing between strings, single-character lexemes and everything else under the umbrella term "scalar" (don't grumble about the name, it was destined to go away in any case):</p>
<pre><code>#[derive(Debug, PartialEq)]
pub enum Lexeme {
    String(String),
    Scalar(String),
    OBrace,
    CBrace,
    OBracket,
    CBracket,
    Comma,
    Colon,
}
</code></pre>
<p>If anything, it made the code uglier as there were now two paradigms sitting in the code side by side: typed values and "scalars" that I had to handle the old way:</p>
<pre><code>match lexeme {
    Lexeme::OBracket =&gt; Event::StartArray,
    Lexeme::OBrace =&gt; Event::StartMap,
    Lexeme::CBracket =&gt; Event::EndArray,
    Lexeme::CBrace =&gt; Event::EndMap,
    Lexeme::String(s) =&gt; Event::String(try!(unescape(s))),

// The Ugliness boundary :-)

    Lexeme::Scalar(ref s) if s == "null" =&gt; Event::Null,
    Lexeme::Scalar(ref s) if s == "true" =&gt; Event::Boolean(true),
    Lexeme::Scalar(ref s) if s == "false" =&gt; Event::Boolean(false),
    Lexeme::Scalar(s) =&gt; {
        Event::Number(try!(s.parse().map_err(|_| Error::Unknown(s))))
    },
    _ =&gt; unreachable!(),
}
</code></pre>
<p>Next, the string un-escaping business has been moved entirely into lexer. Even though it was a pretty much verbatim move of a bunch of code from one module to another, it made it obvious that I'm actually processing escaped characters <em>twice</em>: first simply to correctly find a terminating <code>"</code> and then to decode all those escapes into raw characters. This proved to be a good optimization opportunity later. It never ceases to amaze me how such simple refactorings sometimes give you a much better insight! Do not ignore them.</p>
<p>Finally, I <a href="https://github.com/isagalaev/ijson-rust/commit/e99526c3415c2cb14b36b2e3c276c071dd99558f">split <code>Lexeme::Scalar</code> into honest numbers, booleans and the null</a>. The code got more readable and more idiomatic all over, and there was much rejoicing!</p>
<h2>Bumps along the road</h2>
<p>During all those refactorings I had to constantly fiddle with error definitions (of which <a href="http://softwaremaniacs.org/blog/2015/08/26/ijson-in-rust-errors/en/">I wasn't a fan</a>, to begin with). Changing wrapped error types and types of error parameters — all this really fun stuff, you know… This is the price of using a strongly-typed language: having the knowledge codified in two forms, type declarations and the code itself.</p>
<h2>Performance</h2>
<p>I didn't do this entire exercise just for feeling better about more idiomatic code (though that'd be a reason enough). It was actually the first step in the ongoing performance optimization endeavour that I promised last time.</p>
<p>Cutting right to the chase, this change gave me a <strong>1.5x</strong> performance gain on my 18MB test JSON. Still a far cry though from my reference point, yajl:</p>
<table>
  <tr><td>Before refactoring</td><td>0.290 secs
  </td></tr><tr><td>After refactoring</td><td>0.193 secs
  </td></tr><tr><td>yajl</td><td>0.051 secs
</td></tr></table>

<p>As far as I understand, this gain can be attributed entirely to removing allocations of temporary strings for single-character lexemes. Though I didn't investigate it properly at that point.</p>
<p>See you next time for more optimization fun!

</p></div>
</div></body></html>