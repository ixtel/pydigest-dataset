<html><body><div><div class="post">
  
  <h1 class="post-title">Configuring IPython Notebook Support for PySpark</h1>
  <span class="post-date">01 Feb 2015</span>

  

  <p><a href="https://spark.apache.org/">Apache Spark</a> is a great way for performing
large-scale data processing. Lately, I have begun working with
<a href="https://spark.apache.org/docs/0.9.0/python-programming-guide.html">PySpark</a>, a
way of interfacing with Spark through Python. After a discussion with a
coworker, we were curious whether PySpark could run from within an <a href="http://ipython.org/notebook.html">IPython
Notebook</a>.  It turns out that this is fairly
straightforward by setting up an IPython profile.</p>

<p>Here’s the <code>tl;dr</code> summary:</p>

<ol>
  <li>Install Spark</li>
  <li>Create PySpark profile for IPython</li>
  <li>Some config</li>
  <li>Simple word count example</li>
</ol>

<p>The steps below were successfully executed using Mac OS X 10.10.2 and
<a href="http://brew.sh/">Homebrew</a>. The majority of the steps should be similar for
non-Windows environments. For demonstration purposes, Spark will run in local
mode, but the configuration can be updated to submit code to a cluster.</p>

<p>Many thanks to my coworker <a href="https://twitter.com/stevewampler">Steve Wampler</a> who
did much of the work.</p>

<h2 id="installing-spark">Installing Spark</h2>

<ol>
  <li>Download the <a href="http://spark.apache.org/downloads.html">source for the latest Spark release</a></li>
  <li>Unzip source to <code>~/spark-1.2.0/</code> (or wherever you wish to install Spark)</li>
  <li>From the CLI, type: <code>cd ~/spark-1.2.0/</code></li>
  <li>Install the Scala build tool: <code>brew install sbt</code></li>
  <li>Build Spark:  <code>sbt assembly</code> (Takes a while)</li>
</ol>

<h2 id="create-pyspark-profile-for-ipython">Create PySpark Profile for IPython</h2>

<p>After Spark is installed, let’s start by creating a new IPython profile for PySpark.</p>

<div class="highlight"><pre><code class="language-bash" data-lang="bash">ipython profile create pyspark</code></pre></div>

<p>To avoid port conflicts with other IPython profiles, I updated the default port
to <code>42424</code> within <code>~/.ipython/profile_pyspark/ipython_notebook_config.py</code>:</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">c</span> <span class="o">=</span> <span class="n">get_config</span><span class="p">()</span>

<span class="c"># Simply find this line and change the port value</span>
<span class="n">c</span><span class="o">.</span><span class="n">NotebookApp</span><span class="o">.</span><span class="n">port</span> <span class="o">=</span> <span class="mi">42424</span></code></pre></div>

<p>Set the following environment variables in <code>.bashrc</code> or <code>.bash_profile</code>:</p>

<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># set this to whereever you installed spark</span>
<span class="nb">export </span><span class="nv">SPARK_HOME</span><span class="o">=</span><span class="s2">"$HOME/spark-1.2.0"</span>

<span class="c"># Where you specify options you would normally add after bin/pyspark</span>
<span class="nb">export </span><span class="nv">PYSPARK_SUBMIT_ARGS</span><span class="o">=</span><span class="s2">"--master local[2]"</span></code></pre></div>

<p>Create a file named <code>~/.ipython/profile_pyspark/startup/00-pyspark-setup.py</code> containing the following:</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># Configure the necessary Spark environment</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="n">spark_home</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">'SPARK_HOME'</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">spark_home</span> <span class="o">+</span> <span class="s">"/python"</span><span class="p">)</span>

<span class="c"># Add the py4j to the path.</span>
<span class="c"># You may need to change the version number to match your install</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">spark_home</span><span class="p">,</span> <span class="s">'python/lib/py4j-0.8.2.1-src.zip'</span><span class="p">))</span>

<span class="c"># Initialize PySpark to predefine the SparkContext variable 'sc'</span>
<span class="nb">execfile</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">spark_home</span><span class="p">,</span> <span class="s">'python/pyspark/shell.py'</span><span class="p">))</span></code></pre></div>

<p>Now we are ready to launch a notebook using the PySpark profile</p>

<div class="highlight"><pre><code class="language-bash" data-lang="bash">ipython notebook --profile<span class="o">=</span>pyspark</code></pre></div>

<h2 id="word-count-example">Word Count Example</h2>

<p>Make sure the ipython <code>pyspark</code> profile created a SparkContext by typing <code>sc</code>
within the notebook. You should see output similar to
<code>&lt;pyspark.context.SparkContext at 0x1097e8e90&gt;</code>.</p>

<p>Next, load a text file into a Spark RDD. For example, load the Spark README file:</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">os</span>

<span class="n">spark_home</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">'SPARK_HOME'</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
<span class="n">text_file</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="n">spark_home</span> <span class="o">+</span> <span class="s">"/README.md"</span><span class="p">)</span></code></pre></div>

<p>The word count script below is quite simple. It takes the following steps:</p>

<ol>
  <li>Split each line from the file into words</li>
  <li>Map each word to a tuple containing the word and an initial count of 1</li>
  <li>Sum up the count for each word</li>
</ol>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">word_counts</span> <span class="o">=</span> <span class="n">text_file</span> \
    <span class="o">.</span><span class="n">flatMap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">())</span> \
    <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">word</span><span class="p">:</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> \
    <span class="o">.</span><span class="n">reduceByKey</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span></code></pre></div>

<p>At this point, the word count has not been executed (lazy evaluation). To
actually count the words, execute the pipeline:</p>



<p>Here’s a portion of the output:</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="p">[(</span><span class="s">u'all'</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
 <span class="p">(</span><span class="s">u'when'</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
 <span class="p">(</span><span class="s">u'"local"'</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
 <span class="p">(</span><span class="s">u'including'</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
 <span class="p">(</span><span class="s">u'computation'</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
 <span class="p">(</span><span class="s">u'Spark](#building-spark).'</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
 <span class="p">(</span><span class="s">u'using:'</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
 <span class="p">(</span><span class="s">u'guidance'</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
<span class="o">...</span>
 <span class="p">(</span><span class="s">u'spark://'</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
 <span class="p">(</span><span class="s">u'programs'</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
 <span class="p">(</span><span class="s">u'documentation'</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
 <span class="p">(</span><span class="s">u'It'</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
 <span class="p">(</span><span class="s">u'graphs'</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
 <span class="p">(</span><span class="s">u'./dev/run-tests'</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
 <span class="p">(</span><span class="s">u'first'</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
 <span class="p">(</span><span class="s">u'latest'</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span></code></pre></div>

</div>

</div></body></html>