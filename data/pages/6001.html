<html><body><div><div class="section">
              <h1>langid 1.1.5</h1>

              


<p>langid.py is a standalone Language Identification (LangID) tool.</p><p>








================</p><p>``langid.py`` readme</p><p>================</p><p>Introduction</p><p>------------</p><p>``langid.py`` is a standalone Language Identification (LangID) tool.</p><p>The design principles are as follows:</p><p>1. Fast</p><p>2. Pre-trained over a large number of languages (currently 97)</p><p>3. Not sensitive to domain-specific features (e.g. HTML/XML markup)</p><p>4. Single .py file with minimal dependencies</p><p>5. Deployable as a web service</p><p>All that is required to run ``langid.py`` is &gt;= Python 2.7 and numpy.  </p><p>The main script ``langid/langid.py`` is cross-compatible with both Python2 and</p><p>Python3, but the accompanying training tools are still Python2-only.</p><p>``langid.py`` is WSGI-compliant.  ``langid.py`` will use ``fapws3`` as a web server if </p><p>available, and default to ``wsgiref.simple_server`` otherwise.</p><p>``langid.py`` comes pre-trained on 97 languages (ISO 639-1 codes given):</p><p>    af, am, an, ar, as, az, be, bg, bn, br, </p><p>    bs, ca, cs, cy, da, de, dz, el, en, eo, </p><p>    es, et, eu, fa, fi, fo, fr, ga, gl, gu, </p><p>    he, hi, hr, ht, hu, hy, id, is, it, ja, </p><p>    jv, ka, kk, km, kn, ko, ku, ky, la, lb, </p><p>    lo, lt, lv, mg, mk, ml, mn, mr, ms, mt, </p><p>    nb, ne, nl, nn, no, oc, or, pa, pl, ps, </p><p>    pt, qu, ro, ru, rw, se, si, sk, sl, sq, </p><p>    sr, sv, sw, ta, te, th, tl, tr, ug, uk, </p><p>    ur, vi, vo, wa, xh, zh, zu</p><p>The training data was drawn from 5 different sources:</p><p>* JRC-Acquis </p><p>* ClueWeb 09</p><p>* Wikipedia</p><p>* Reuters RCV2</p><p>* Debian i18n</p><p>Usage</p><p>-----</p><p>    langid.py [options]</p><p>Options:</p><p>  -h, --help            show this help message and exit</p><p>  -s, --serve           launch web service</p><p>  --host=HOST           host/ip to bind to</p><p>  --port=PORT           port to listen on</p><p>  -v                    increase verbosity (repeat for greater effect)</p><p>  -m MODEL              load model from file</p><p>  -l LANGS, --langs=LANGS</p><p>                        comma-separated set of target ISO639 language codes</p><p>                        (e.g en,de)</p><p>  -r, --remote          auto-detect IP address for remote access</p><p>  -b, --batch           specify a list of files on the command line</p><p>  --demo                launch an in-browser demo application</p><p>  -d, --dist            show full distribution over languages</p><p>  -u URL, --url=URL     langid of URL</p><p>  --line                process pipes line-by-line rather than as a document</p><p>  -n, --normalize       normalize confidence scores to probability values</p><p>The simplest way to use ``langid.py`` is as a command-line tool, and you can </p><p>invoke using ``python langid.py``. If you installed ``langid.py`` as a Python </p><p>module (e.g. via ``pip install langid``), you can invoke ``langid`` instead of </p><p>``python langid.py`` (the two are equivalent).  This will cause a prompt to </p><p>display. Enter text to identify, and hit enter::</p><p>  &gt;&gt;&gt; This is a test</p><p>  ('en', 0.99999999099035441)</p><p>  &gt;&gt;&gt; Questa e una prova</p><p>  ('it', 0.98569847366134222)</p><p>``langid.py`` can also detect when the input is redirected (only tested under Linux), and in this</p><p>case will process until EOF rather than until newline like in interactive mode::</p><p>  python langid.py &lt; readme.rst </p><p>  ('en', 1.0)</p><p>The value returned is the probability estimate for the language. Calculating </p><p>the exact probability estimate is not actually necessary for classification, </p><p>and can be disabled for a slight performance boost. More details are provided</p><p>in the section on `Probability Normalization`.</p><p>You can also use ``langid.py`` as a Python library::</p><p>  # python</p><p>  Python 2.7.2+ (default, Oct  4 2011, 20:06:09) </p><p>  [GCC 4.6.1] on linux2</p><p>  Type "help", "copyright", "credits" or "license" for more information.</p><p>  &gt;&gt;&gt; import langid</p><p>  &gt;&gt;&gt; langid.classify("This is a test")</p><p>  ('en', 0.99999999099035441)  </p><p>Finally, ``langid.py`` can use Python's built-in ``wsgiref.simple_server`` (or ``fapws3`` if available) to</p><p>provide language identification as a web service. To do this, launch ``python langid.py -s``, and</p><p>access http://localhost:9008/detect . The web service supports GET, POST and PUT. If GET is performed</p><p>with no data, a simple HTML forms interface is displayed.</p><p>The response is generated in JSON, here is an example::</p><p>  {"responseData": {"confidence": 0.99999999099035441, "language": "en"}, "responseDetails": null, "responseStatus": 200}</p><p>A utility such as curl can be used to access the web service::</p><p>  # curl -d "q=This is a test" localhost:9008/detect</p><p>  {"responseData": {"confidence": 0.99999999099035441, "language": "en"}, "responseDetails": null, "responseStatus": 200}</p><p>You can also use HTTP PUT::</p><p>  # curl -T readme.rst localhost:9008/detect</p><p>    % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current</p><p>                                 Dload  Upload   Total   Spent    Left  Speed</p><p>  100  2871  100   119  100  2752    117   2723  0:00:01  0:00:01 --:--:--  2727</p><p>  {"responseData": {"confidence": 1.0, "language": "en"}, "responseDetails": null, "responseStatus": 200}</p><p>If no "q=XXX" key-value pair is present in the HTTP POST payload, ``langid.py`` will interpret the entire</p><p>file as a single query. This allows for redirection via curl::</p><p>  # echo "This is a test" | curl -d @- localhost:9008/detect</p><p>  {"responseData": {"confidence": 0.99999999099035441, "language": "en"}, "responseDetails": null, "responseStatus": 200}</p><p>``langid.py`` will attempt to discover the host IP address automatically. Often, this is set to localhost(127.0.1.1), even </p><p>though the machine has a different external IP address. ``langid.py`` can attempt to automatically discover the external</p><p>IP address. To enable this functionality, start ``langid.py`` with the ``-r`` flag.</p><p>``langid.py`` supports constraining of the output language set using the ``-l`` flag and a comma-separated list of ISO639-1 </p><p>language codes::</p><p>  # python langid.py -l it,fr</p><p>  &gt;&gt;&gt; Io non parlo italiano</p><p>  ('it', 0.99999999988965627)</p><p>  &gt;&gt;&gt; Je ne parle pas français</p><p>  ('fr', 1.0)</p><p>  &gt;&gt;&gt; I don't speak english</p><p>  ('it', 0.92210605672341062)</p><p>When using ``langid.py`` as a library, the set_languages method can be used to constrain the language set::</p><p>  python                      </p><p>  Python 2.7.2+ (default, Oct  4 2011, 20:06:09) </p><p>  [GCC 4.6.1] on linux2</p><p>  Type "help", "copyright", "credits" or "license" for more information.</p><p>  &gt;&gt;&gt; import langid</p><p>  &gt;&gt;&gt; langid.classify("I do not speak english")</p><p>  ('en', 0.57133487679900674)</p><p>  &gt;&gt;&gt; langid.set_languages(['de','fr','it'])</p><p>  &gt;&gt;&gt; langid.classify("I do not speak english")</p><p>  ('it', 0.99999835791478453)</p><p>  &gt;&gt;&gt; langid.set_languages(['en','it'])</p><p>  &gt;&gt;&gt; langid.classify("I do not speak english")</p><p>  ('en', 0.99176190378750373)</p><p>Batch Mode</p><p>----------</p><p>``langid.py`` supports batch mode processing, which can be invoked with the ``-b`` flag.</p><p>In this mode, ``langid.py`` reads a list of paths to files to classify as arguments.</p><p>If no arguments are supplied, ``langid.py`` reads the list of paths from ``stdin``,</p><p>this is useful for using ``langid.py`` with UNIX utilities such as ``find``.</p><p>In batch mode, ``langid.py`` uses ``multiprocessing`` to invoke multiple instances of</p><p>the classifier, utilizing all available CPUs to classify documents in parallel. </p><p>.. Probability Normalization</p><p>Probability Normalization</p><p>-------------------------</p><p>The probabilistic model implemented by ``langid.py`` involves the multiplication of a</p><p>large number of probabilities. For computational reasons, the actual calculations are</p><p>implemented in the log-probability space (a common numerical technique for dealing with</p><p>vanishingly small probabilities). One side-effect of this is that it is not necessary to</p><p>compute a full probability in order to determine the most probable language in a set</p><p>of candidate languages. However, users sometimes find it helpful to have a "confidence"</p><p>score for the probability prediction. Thus, ``langid.py`` implements a re-normalization</p><p>that produces an output in the 0-1 range.</p><p>For command-line usages of ``langid.py``, the default behaviour is to disable</p><p>probability normalization. It can be enabled by passing the ``-n`` flag. For</p><p>library use, the default behaviour is to enable it. To disable it, the user</p><p>must instantiate their own ``LanguageIdentifier``. An example of such usage is as follows::  </p><p>  &gt;&gt; from langid.langid import LanguageIdentifier, model</p><p>  &gt;&gt; identifier = LanguageIdentifier.from_modelstring(model, norm_probs=False)</p><p>  &gt;&gt; identifier.classify("This is a test")</p><p>  ('en', -54.41310358047485)</p><p>Training a model</p><p>----------------</p><p>We provide a full set of training tools to train a model for ``langid.py`` </p><p>on user-supplied data.  The system is parallelized to fully utilize modern </p><p>multiprocessor machines, using a sharding technique similar to MapReduce to </p><p>allow parallelization while running in constant memory.</p><p>The full training can be performed using the tool ``train.py``. For </p><p>research purposes, the process has been broken down into indiviual steps, </p><p>and command-line drivers for each step are provided. This allows the user </p><p>to inspect the intermediates produced, and also allows for some parameter </p><p>tuning without repeating some of the more expensive steps in the </p><p>computation. By far the most expensive step is the computation of </p><p>information gain, which will make up more than 90% of the total computation </p><p>time.</p><p>The tools are:</p><p>1. index.py  - index a corpus. Produce a list of file, corpus, language pairs.</p><p>2. tokenize.py - take an index and tokenize the corresponding files</p><p>3. DFfeatureselect.py - choose features by document frequency</p><p>4. IGweight.py - compute the IG weights for language and for domain</p><p>5. LDfeatureselect.py - take the IG weights and use them to select a feature set</p><p>6. scanner.py - build a scanner on the basis of a feature set</p><p>7. NBtrain.py - learn NB parameters using an indexed corpus and a scanner</p><p>The tools can be found in ``langid/train`` subfolder. </p><p>Each tool can be called with ``--help`` as the only parameter to provide an overview of the </p><p>functionality. </p><p>To train a model, we require multiple corpora of monolingual documents. Each document should </p><p>be a single file, and each file should be in a 2-deep folder hierarchy, with language nested </p><p>within domain. For example, we may have a number of English files:</p><p>    ./corpus/domain1/en/File1.txt</p><p>    ./corpus/domainX/en/001-file.xml</p><p>To use default settings, very few parameters need to be provided. Given a corpus in the format</p><p>described above at ``./corpus``, the following is an example set of invocations that would</p><p>result in a model being trained, with a brief description of what each step </p><p>does.</p><p>To build a list of training documents::</p><p>    python index.py ./corpus</p><p>This will create a directory ``corpus.model``, and produces a list of paths to documents in the</p><p>corpus, with their associated language and domain.</p><p>We then tokenize the files using the default byte n-gram tokenizer::</p><p>    python tokenize.py corpus.model</p><p>This runs each file through the tokenizer, tabulating the frequency of each token according</p><p>to language and domain. This information is distributed into buckets according to a hash</p><p>of the token, such that all the counts for any given token will be in the same bucket.</p><p>The next step is to identify the most frequent tokens by document </p><p>frequency::</p><p>    python DFfeatureselect.py corpus.model</p><p>This sums up the frequency counts per token in each bucket, and produces a list of the highest-df</p><p>tokens for use in the IG calculation stage. Note that this implementation of DFfeatureselect</p><p>assumes byte n-gram tokenization, and will thus select a fixed number of features per ngram order.</p><p>If tokenization is replaced with a word-based tokenizer, this should be replaced accordingly.</p><p>We then compute the IG weights of each of the top features by DF. This is computed separately</p><p>for domain and for language::</p><p>    python IGweight.py -d corpus.model</p><p>    python IGweight.py -lb corpus.model</p><p>Based on the IG weights, we compute the LD score for each token::</p><p>    python LDfeatureselect.py corpus.model</p><p>This produces the final list of LD features to use for building the NB model.</p><p>We then assemble the scanner::</p><p>    python scanner.py corpus.model</p><p>The scanner is a compiled DFA over the set of features that can be used to </p><p>count the number of times each of the features occurs in a document in a </p><p>single pass over the document. This DFA is built using Aho-Corasick string </p><p>matching.</p><p>Finally, we learn the actual Naive Bayes parameters::</p><p>    python NBtrain.py corpus.model</p><p>This performs a second pass over the entire corpus, tokenizing it with the scanner from the previous</p><p>step, and computing the Naive Bayes parameters P(C) and p(t|C). It then compiles the parameters</p><p>and the scanner into a model compatible with ``langid.py``. </p><p>In this example, the final model will be at the following path::</p><p>  ./corpus.model/model</p><p>This model can then be used in ``langid.py`` by invoking it with the ``-m`` command-line option as </p><p>follows:</p><p>    python langid.py -m ./corpus.model/model</p><p>It is also possible to edit ``langid.py`` directly to embed the new model string.</p><p>Read more</p><p>---------</p><p>``langid.py`` is based on our published research. [1] describes the LD feature selection technique in detail,</p><p>and [2] provides more detail about the module ``langid.py`` itself. [3] compares the speed of ``langid.py``</p><p>to Google's Chrome CLD2, as well as my own pure-C implementation and the authors' implementation on specialized</p><p>hardware.</p><p>[1] Lui, Marco and Timothy Baldwin (2011) Cross-domain Feature Selection for Language Identification, </p><p>In Proceedings of the Fifth International Joint Conference on Natural Language Processing (IJCNLP 2011), </p><p>Chiang Mai, Thailand, pp. 553—561. Available from http://www.aclweb.org/anthology/I11-1062</p><p>[2] Lui, Marco and Timothy Baldwin (2012) langid.py: An Off-the-shelf Language Identification Tool, </p><p>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012), </p><p>Demo Session, Jeju, Republic of Korea. Available from www.aclweb.org/anthology/P12-3005</p><p>[3] Kenneth Heafield and Rohan Kshirsagar and Santiago Barona (2015) Language Identification and Modeling in Specialized Hardware,</p><p>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint </p><p>Conference on Natural Language Processing (Volume 2: Short Papers).</p><p>Available from http://aclweb.org/anthology/P15-2063</p><p>Contact</p><p>-------</p><p>Marco Lui &lt;saffsd@gmail.com&gt; http://www.csse.unimelb.edu.au/~mlui</p><p>I appreciate any feedback, and I'm particularly interested in hearing about </p><p>places where ``langid.py`` is being used. I would love to know more about </p><p>situations where you have found that ``langid.py`` works well, and about</p><p>any shortcomings you may have found.</p><p>Acknowledgements</p><p>----------------</p><p>Thanks to aitzol for help with packaging ``langid.py`` for PyPI.</p><p>Thanks to pquentin for suggestions and improvements to packaging.</p><p>Related Implementations</p><p>-----------------------</p><p>Dawid Weiss has ported ``langid.py`` to Java, with a particular focus on</p><p>speed and memory use. Available from https://github.com/carrotsearch/langid-java</p><p>I have written a Pure-C version of ``langid.py``, which an external evaluation (see `Read more`)</p><p>has found to be up to 20x as fast as the pure Python implementation here. </p><p>Available from https://github.com/saffsd/langid.c</p><p>I have also written a JavaScript version of ``langid.py`` which runs entirely in the browser.</p><p>Available from https://github.com/saffsd/langid.js</p><p>Changelog</p><p>---------</p><p>v1.0: </p><p>  * Initial release</p><p>v1.1:</p><p>  * Reorganized internals to implement a LanguageIdentifier class</p><p>v1.1.2:</p><p>  * Added a 'langid' entry point</p><p>v1.1.3:</p><p>  * Made `classify` and `rank` return Python data types rather than numpy ones</p><p>v1.1.4:</p><p>  * Added set_languages to __init__.py, fixing #10 (and properly fixing #8)</p><p>v1.1.5:</p><p>  * remove dev tag</p><p>  * add PyPi classifiers, fixing #34 (thanks to pquentin)

</p><a name="downloads"> </a>


<ul class="nodot">
  <li><strong>Downloads (All Versions):</strong></li>
  <li>
    <span>0</span> downloads in the last day
  </li>
  <li>
    <span>2088</span> downloads in the last week
  </li>
  <li>
    <span>22860</span> downloads in the last month
  </li>
</ul>









            </div>


          </div></body></html>