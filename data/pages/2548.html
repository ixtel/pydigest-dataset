<html><body><div><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-scrapy-random-user-agent" class="anchor" href="#scrapy-random-user-agent" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Scrapy Random User-Agent</h1>
<p>Does your scrapy spider get identified and blocked by servers because
you use the default user-agent or a generic one?</p>
<p>Use this <code>random_useragent</code> module and set a random user-agent for
every request. You are limited only by the number of different
user-agents you set in a text file.</p>
<a name="user-content-installing"/>
<h2><a id="user-content-installing" class="anchor" href="#installing" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Installing</h2>
<p>Installing it is pretty simple.</p>
<div class="highlight highlight-source-python"><pre>pip install scrapy<span class="pl-k">-</span>random<span class="pl-k">-</span>useragent</pre></div>
<a name="user-content-usage"/>
<h2><a id="user-content-usage" class="anchor" href="#usage" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" role="img" version="1.1" viewbox="0 0 16 16"><path d="M4 9h1v1h-1c-1.5 0-3-1.69-3-3.5s1.55-3.5 3-3.5h4c1.45 0 3 1.69 3 3.5 0 1.41-0.91 2.72-2 3.25v-1.16c0.58-0.45 1-1.27 1-2.09 0-1.28-1.02-2.5-2-2.5H4c-0.98 0-2 1.22-2 2.5s1 2.5 2 2.5z m9-3h-1v1h1c1 0 2 1.22 2 2.5s-1.02 2.5-2 2.5H9c-0.98 0-2-1.22-2-2.5 0-0.83 0.42-1.64 1-2.09v-1.16c-1.09 0.53-2 1.84-2 3.25 0 1.81 1.55 3.5 3 3.5h4c1.45 0 3-1.69 3-3.5s-1.5-3.5-3-3.5z"/></svg></a>Usage</h2>
<p>In your <code>settings.py</code> file, update the <code>DOWNLOADER_MIDDLEWARES</code>
variable like this.</p>
<div class="highlight highlight-source-python"><pre><span class="pl-c1">DOWNLOADER_MIDDLEWARES</span> <span class="pl-k">=</span> {
    <span class="pl-s"><span class="pl-pds">'</span>scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware<span class="pl-pds">'</span></span>: <span class="pl-c1">None</span>,
    <span class="pl-s"><span class="pl-pds">'</span>random_useragent.RandomUserAgentMiddleware<span class="pl-pds">'</span></span>: <span class="pl-c1">400</span>
}</pre></div>
<p>This disables the default <code>UserAgentMiddleware</code> and enables the
<code>RandomUserAgentMiddleware</code>.</p>
<p>Then, create a new variable <code>USER_AGENT_LIST</code> with the path to your
text file which has the list of all user-agents
(one user-agent per line).</p>
<div class="highlight highlight-source-python"><pre><span class="pl-c1">USER_AGENT_LIST</span> <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>/path/to/useragents.txt<span class="pl-pds">"</span></span></pre></div>
<p>Now all the requests from your crawler will have a random user-agent
picked from the text file.</p>

</article>
  </div></body></html>