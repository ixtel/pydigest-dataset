<html><body><div><div class="content html_format"><p>
      Не так давно мы начали пару проектов, в которых необходима оптическая система с каналом дальности, и решили для этого использовать Kinect v2. Поскольку проекты реализуются на Python, то для начала нужно было заставить работать Kinect из Python, а затем откалибровать его, так как Kinect из коробки вносит некоторые геометрические искажения в кадры и дает сантиметровые ошибки в определении глубины.</p>
<p>
До этого я никогда не имел дела ни с компьютерным зрением, ни с OpenCV, ни с Kinect. Исчерпывающую инструкцию, как со всем этим хозяйством работать, мне найти тоже не удалось, так что в итоге пришлось порядком повозиться. И я решил, что будет не лишним систематизировать полученный опыт в этой статье. Быть может, она окажется небесполезной для какого-нибудь страждущего,</p><s> а еще нам нужна популярная статья для галочки в отчетности</s><p>.

</p><img src="https://habrastorage.org/files/fa0/4ae/2d7/fa04ae2d7d414303b1cd224d7416b8f2.png" alt="image"/>
<a name="habracut"/>
<u>Минимальные системные требования</u><p>: Windows 8 и выше, Kinect SDK 2.0, USB 3.0. 
</p><p>
Таблица I. Характеристики Kinect v2:
</p><table>
<tr>
<td>Разрешение RGB камеры, пикс.</td>
<td>1920 x 1080</td>
</tr>
<tr>
<td>Разрешение инфракрасной (ИК) камеры, пикс.</td>
<td>512 x 424</td>
</tr>
<tr>
<td>Углы обзора RGB камеры, º</td>
<td>84.1 x 53.8</td>
</tr>
<tr>
<td>Углы обзора ИК камеры, º</td>
<td>70.6 x 60.0</td>
</tr>
<tr>
<td>Диапазон измерений дальности, м.</td>
<td>0.6 — 8.0<sup>1</sup> </td>
</tr>
<tr>
<td>Частота съемки RGB камеры, Гц</td>
<td>30</td>
</tr>
<tr>
<td>Частота съемки ИК камеры, Гц</td>
<td>30</td>
</tr>
</table>
<div class="spoiler"><b class="spoiler_title">1</b><div class="spoiler_text"><p>Информация различается от источника к источнику. </p><a href="https://dev.windows.com/en-us/kinect/hardware">Тут</a><p> говорят о 0.5–4.5 м., по факту я получал ~0.6-8.0 м. 
</p></div></div><p>
Таким образом, передо мной стояли следующие задачи:

</p><ol>
<li>завести Kinect на Python;</li>
<li>откалибровать RGB и ИК камеры;</li>
<li>реализовать возможность совмещения кадров RGB и ИК;</li>
<li>откалибровать канал глубины.</li>
</ol><p>
А теперь подробно остановимся на каждом пункте.

</p><h4><b>1. Kinect v2 и Python</b></h4><p>
Как я уже говорил, до этого я с компьютерным зрением дел не имел, но до меня доходили слухи, что без библиотеки OpenCV тут никуда. А поскольку в ней есть целый </p><a href="http://docs.opencv.org/3.0.0/d9/d0c/group__calib3d.html">модуль </a><p>для калибровки камер, то первым делом я собрал OpenCV с поддержкой Python 3 под Windows 8.1. Тут не обошлось без некоторой мороки, обычно сопровождающей сборку open-sourсe проектов на Windows, но все прошло без особых сюрпризов и в целом в рамках </p><a href="http://docs.opencv.org/master/d5/de5/tutorial_py_setup_in_windows.html#gsc.tab=0">инструкции </a><p>от разработчиков. </p><p>
С Kinect-ом же пришлось повозиться несколько подольше. Официальный SDK поддерживает интерфейсы только для C#, С++ и JavaScript. Если зайти с другой стороны, то можно увидеть, что OpenCV </p><a href="http://docs.opencv.org/3.0-rc1/d7/df5/tutorial_ug_highgui.html">поддерживает </a><p> ввод с 3D камер, но камера должна быть совместима с библиотекой OpenNI. OpenNI поддерживает Kinect, а вот сравнительно недавний Kinect v2 — нет. Впрочем, добрые люди написали </p><a href="https://github.com/kaorun55/OpenNI2-Kinect2Driver">драйвер </a><p>для Kinect v2 под OpenNI. Он даже работает и позволяет полюбоваться на видео с каналов устройства в NiViewer, но при использовании с OpenCV вылетает с ошибкой. Впрочем, другие добрые люди написали</p><a href="https://github.com/kinect/PyKinect2"> Python-обертку</a><p> над официальным SDK. На ней я и остановился.

</p><h4><b>2. Калибровка камер</b></h4><p>
Камеры не идеальны, искажают картинку и нуждаются в калибровке. Чтобы использовать Kinect для измерений, было бы неплохо устранить эти геометрические искажения как на RGB камере, так и на датчике глубины. Поскольку ИК камера является одновременно и приемником датчика глубины, то мы можем использовать ИК кадры для калибровки, а затем результаты калибровки использовать для устранения искажений с кадров глубины.
</p><p>
Калибровка камеры осуществляется с целью узнать внутренние параметры камеры, а именно — матрицу камеры и коэффициенты дисторсии.
 
</p><img src="https://habrastorage.org/files/43f/3ff/9e5/43f3ff9e56d94ba0bf206ec38ce737a2.png" alt="image"/>
<p>
Матрицей камеры называется матрица вида:

</p><img src="https://habrastorage.org/files/61d/95f/272/61d95f27296b4a80b058799edc53756d.png"/><p> где
</p><p>
(</p><i>с<sub>u</sub>, c<sub>v</sub></i><p>) — координаты принципиальной точки (точки пересечения оптической оси с плоскостью изображения, в идеальной камере находиться точно в центре изображения, в реальных немного смещена от центра);

</p><i>f<sub>u</sub>, f<sub>v</sub></i><p> — фокусное расстояние </p><i>f</i><p>, измеренное в ширине и высоте пикселя.
</p><p>
Существуют два основных вида дисторсии: радиальная дисторсия и тангенциальная дисторсия.

</p><u>Радиальная дисторсия</u><p> — искажение изображения в результате неидеальности параболической формы линзы. Искажения, вызванные радиальной дисторсией, равны 0 в оптическом центре сенсора и возрастают к краям. Как правило, радиальная дисторсия вносит наибольший вклад в искажение изображения.

</p><u>Тангенциальная дисторсия</u><p> — искажения изображения, вызванные погрешностями в установки линзы параллельно плоскости изображения.

</p><img src="https://habrastorage.org/files/a2d/34e/f76/a2d34ef76130433586b5106c179e6669.png"/>
<p>
Для устранение дисторсии координаты пикселей можно пересчитать с помощью следующего </p><a href="http://docs.opencv.org/2.4/doc/tutorials/calib3d/camera_calibration/camera_calibration.html">уравнения</a><p>:

</p><img src="https://habrastorage.org/files/ffd/b2a/718/ffdb2a718a1b4702930f4af387515157.png"/>
<p>
где (</p><i>u,v</i><p>) — первоначальное расположение пикселя,</p><p>
(</p><i>u<sub>corrected</sub>,v<sub>corrected</sub></i><p>) — расположение пикселя после устранения геометрических искажений,
</p><i>k<sub>1</sub>, k<sub>2</sub>, k<sub>3</sub></i><p> — коэффициенты радиальной дисторсии,
</p><i>p<sub>1</sub>, p<sub>2</sub> </i><p> — коэффициенты тангенциальной дисторсии,
</p><i> r<sup>2</sup>=u<sup>2</sup>+v<sup>2</sup>.</i>
<p>
Точность измерения параметров камеры (коэффициенты дисторсии, матрица камеры) определяется средней величиной ошибки перепроэцирования (</p><i>ReEr, Reprojection Error</i><p>). </p><i>ReEr </i><p>— расстояние (в пикселях) между проекцией </p><i>P'</i><p> на плоскость изображения точки </p><i>P</i><p> на поверхности объекта, и проекцией </p><i>P''</i><p> этой же точки </p><i>P</i><p>, построенной после устранения дисторсии с использованием параметров камеры.

</p><img src="https://habrastorage.org/files/83f/15b/255/83f15b25525f48fcbf3e917689a67e1f.png" alt="image"/>
<p>
Стандартная процедура калибровки камеры состоит из следующих шагов:
</p><p>
1) cделать 20-30 фотографий с разными положениями </p><s>объекта с известной геометрией</s><p> шахматной доски;

</p><img src="https://habrastorage.org/files/a77/4f0/58d/a774f058dd01405e9391df228a2acea4.png" alt="image"/>
<p>
2) определить ключевые точки объекта на изображении;

</p><pre><code class="python">found, corners = cv2.findChessboardCorners(img, #изображение
					    PATTERN_SIZE,#сколько ключевых точек, в нашем случае 6x8 
					    flags)#параметры поиска точек
</code></pre>
<img src="https://habrastorage.org/files/856/8f8/e09/8568f8e092794e36a902296d0337f931.png"/>
<p>
3) найти такие коэффициенты дисторсии которые минимизирует </p><i>ReEr</i><p>.

</p><pre><code class="python">ReEr, camera_matrix, dist_coefs, rvecs, tvecs = cv2.calibrateCamera(obj_points,#координаты ключевых точек в системе координат объекта 
                                                                    #(х', y', z'=0)
                                                                    img_points,#в системе координат изображения (u,v)
                                                                    (w, h),#размер изображения
                                                                    None,#можно использовать уже известную матрицу камеры
                                                                    None, #можно использовать уже известные коэффициенты дисторсии
                                                                    criteria = criteria,#критерии окончания минимизации ReEr
		                                                    flags = flags)#какие коэффициенты дисторсии мы хотим получить
</code></pre><p>
В нашем случае, для RGB камеры среднее значение </p><i>ReEr </i><p>составило 0.3 пикселя, а для ИК камеры — 0.15. Результаты устранения дисторсии:

</p><pre><code class="python">img = cv2.undistort(img, camera_matrix, dist_coefs)
</code></pre>
<img src="https://habrastorage.org/files/39d/1f7/746/39d1f774669f498aa5d667c9f76e530b.png"/>

<h4><b>3. Совмещение кадров с двух камер</b></h4>

<img src="https://habrastorage.org/files/1a9/330/541/1a93305411f348a8881a1a87b0139b3a.png"/>
<p>
Для того чтобы получить для пикселя как глубину (Z координату), так и цвет, для начала необходимо перейти из пиксельных координат на кадре глубины в трехмерные координаты ИК камеры [2]:

</p><img src="https://habrastorage.org/files/eef/3cb/3b3/eef3cb3b37764d6f9f0b8014c84896a7.png"/>
<p>
где (</p><i>x<sub>1</sub>,y<sub>1</sub>,z<sub>1</sub></i><p>) — координаты точки в системе координат ИК камеры,
</p><i>z<sub>1</sub></i><p> — результат возвращаемый датчиком глубины,</p><p>
(</p><i>u<sub>1</sub>,v<sub>1</sub></i><p>) — координаты пикселя на кадре глубины,
</p><i>c<sub>1,u</sub>, c<sub>1,v</sub></i><p> — координаты оптического центра ИК камеры, 
</p><i>f<sub>1,u</sub>, f<sub>1,v</sub></i><p> — проекции фокусного расстояния ИК камеры.
</p><p>
Затем нужно перейти из системы координат ИК камеры к системе координат RGB камеры. Для этого требуется переместить начало координат с помощью вектора переноса</p><i> T</i><p> и повернуть систему координат с помощью матрицы вращения </p><i>R</i><p>:

</p><img src="https://habrastorage.org/files/139/483/292/1394832929d648e891f4920a958a9cb8.png"/>
<p>
После чего нужно перейти из трехмерной системы координат RGB камеры к пиксельным координатам RGB кадра:

</p><img src="https://habrastorage.org/files/305/db9/532/305db953214d446d88f25bd712f3233f.png"/>
<p>
Таким образом, после всех этих преобразований, мы можем получить для пикселя (</p><i>u<sub>1</sub>, v<sub>1</sub></i><p>) кадра глубины значение цвета соответствующего пикселя RGB кадра (</p><i>u<sub>2</sub>, v<sub>2</sub></i><p>).

</p><img src="https://habrastorage.org/files/04c/17e/1b5/04c17e1b551b4361a3d0b0e30754dc55.png"/>
<p>
Как видно на результирующей картинке, изображение местами двоится. Такой же эффект можно </p><a href="http://www.bryancook.net/2014/03/mapping-between-kinect-color-and-depth.html">наблюдать </a><p>и при использовании класса </p><i>CoordinateMapper </i><p>из официального SDK. Впрочем, если на изображении нас интересует только человек, то можно воспользоваться </p><i>bodyIndexFrame </i><p>(поток Kinect, позволяющий узнать, какие пиксели относятся к человеку, а какие к фону) для выделения области интереса и устранения двоения.

</p><img src="https://habrastorage.org/files/0f4/477/36e/0f447736ebc747fbbca89ffbd0288ec9.png"/>
<p>
Для определения матрицы вращения </p><i>R</i><p> и вектора переноса </p><i>T</i><p> необходимо провести совместную калибровку двух камер. Для этого нужно сделать 20-30 фотографий объекта с известной геометрий в различных положениях как RGB, так и ИК камерой, лучше при этом не держать объект в руках, чтобы исключить возможность его смещения между снятием кадров разными камерами. Затем нужно воспользоваться функцией </p><i>stereoCalibrate </i><p>из библиотеки OpenCV. Данная функция определяет позицию каждой из камер относительно калибровочного объекта, а затем находит такое преобразование из системы координат первой камеры в систему координат второй камеры, которое обеспечивает минимизацию ReEr.

</p><pre><code class="python">retval, cameraMatrix1, distCoeffs1, cameraMatrix2, distCoeffs2, R, T, E, F = cv2.stereoCalibrate(pattern_points, #координаты ключевых 
                                                          #точек в системе координат объекта (х', y', z'=0)
                                                          ir_img_points,#в системе координат ИК камеры (u1, v1)
                                                          rgb_img_points, #в системе координат RGB камеры (u2, v2)
                                                          irCamera['camera_matrix'],#матрица камеры ИК (брать из calibrateCamera),
                                                          irCamera['dist_coefs'], #коэф. дис. ИК камеры (брать из calibrateCamera)
                                                          rgbCamera['camera_matrix'], #матрица RGB камеры (брать из calibrateCamera)             
                                                          rgbCamera['dist_coefs'], #коэф. дис. RGB камеры (брать из calibrateCamera)
                                                          image_size) #размер изображения ИК камеры (в пикселях)
</code></pre><p>
И в итоге мы получили </p><i>ReEr </i><p> = 0.23.

</p><h4><b>4. Калибровка канала глубины</b></h4><p>
Датчик глубины Kinect возвращает глубину (именно глубину, т.е. Z-координату, а не расстояние) в мм. Но насколько точны эти значения? Судя по публикации [2], ошибка может cоставлять 0.5-3 см в зависимости от дистанции, так что есть смысл провести калибровку канала глубины.
</p><p>
Эта процедура заключается в том, чтобы найти систематическую ошибку Kinect (разницу между эталонной глубиной и глубиной, выдаваемой сенсором) в зависимости от расстояния до объекта. А для этого необходимо знать эталонную глубину. Наиболее очевидный путь — расположить плоский объект параллельно плоскости камеры и измерить расстояние до него линейкой. Постепенно сдвигая объект и делая серию измерений на каждом расстоянии, можно найти среднюю ошибку для каждой из дистанций. Но, во-первых, это не очень удобно, во-вторых, найти идеально плоский объект относительно больших размеров и обеспечить параллельность его расположения относительно плоскости камеры сложнее, чем может показаться на первый взгляд. Поэтому в качестве эталона, относительно которого будет рассчитываться ошибка, мы решили взять глубину, определяемую по известной геометрии объекта.
</p><p>
Зная геометрию объекта (например размеры клеток шахматной доски) и расположив его строго параллельно плоскости камеры можно определить глубину до него следующим образом:

</p><img src="https://habrastorage.org/files/1f7/52e/e05/1f752ee05f5647298c67cf614d899e9c.png"/>
<p>
где </p><i>f</i><p> — фокусное расстояние,
</p><i>d</i><p> — расстояние между проекциями ключевых точек на матрице камеры,
</p><i>D</i><p> — расстояние между ключевыми точками объекта,
</p><i>Z</i><p> — расстояние от центра проекции камеры до объекта.

</p><img src="https://habrastorage.org/files/720/301/c40/720301c40b1d4e839ba8512f31d2d2aa.png"/>
<p>
В случае если объект расположен не строго параллельно, а под некоторым углом к плоскости камеры, глубину можно определить на основе решения задачи Perspective-n-Point (PnP) [3]. Решению этой проблемы посвящен ряд алгоритмов, реализованных в библиотеке OpenCV, которые позволяют найти преобразование |</p><i>R, T</i><p>| между системой координат калибровочного объекта и системой координат камеры, а значит, и определить глубину с точностью до параметров камеры. 

</p><pre><code class="python">retval, R, T = cv2.solvePnP(obj_points[:, [0, 5, 42, 47]],#крайние точки в координатах объекта
			    img_points[:, [0, 5, 42, 47]], #крайние точки в координатах изображения
			    rgbCameraMatrix,#матрица камеры
                            rgbDistortion,#коэффициенты дисторсии
                            flags= cv2.SOLVEPNP_UPNP)#метод решения PnP

R, jacobian = cv2.Rodrigues(R)#переходим от вектора вращения к матрице вращения
for j in range(0, numberOfPoints): # цикл по ключевым точкам
    point = numpy.dot(rgb_obj_points[j], R.T) + T.T # Важно! В документации нигде об этом не сказано, 
    #но по итогам экспериментов с модельными изображениями, выяснилось, что нужно транспонировать матрицу вращения
    computedDistance[j] = point[0][2] * 1000 # Z-координата в мм
</code></pre><p>
Для калибровки канала глубины мы произвели серию съемок калибровочного объекта на расстояниях ~0.7-2.6 м с шагом ~7 cм. Калибровочный объект располагался в центре кадра параллельно плоскости камеры, на сколько это возможно сделать «на глазок». На каждом расстоянии делался один снимок RGB камерой и 100 снимков датчиком глубины. Данные с датчика усреднялись, а расстояние, определенное по геометрии объекта на основе RGB кадра, принималось за эталон. Средняя ошибка в определении глубины датчиком Kinect на данной дистанции определилась следующем образом:

</p><img src="https://habrastorage.org/files/cb7/d88/416/cb7d884169bf400eb10f717a81192e96.png"/>
<p>
где </p><i>z <sub>i</sub><sup>RGB</sup> </i><p> — расстояние до i-й ключевой точки по геометрии,
 </p><i>z <sub>i</sub><sup>depth</sup> </i><p> — усредненное по 100 кадрам расстояние до i-й ключевой точки по данным датчика глубины,</p><p>
N — количество ключевых точек на объекте (в нашем случае 48).
</p><p>
Затем мы получили функцию ошибки от расстояния путем интерполяции полученных результатов.

</p><img src="https://habrastorage.org/files/b9a/913/275/b9a9132758964ba2a2601188f4dfd4d5.png"/>
<p>
На рисунке ниже показано распределение ошибок до и после коррекции на калибровочных кадрах. Всего было сделано 120000 измерений (25 дистанций, 100 кадров глубины на каждой, 48 ключевых точек на объекте). Ошибка до коррекции составила 17±9.95 мм (среднее ± стандартное отклонение), после — 0.45±8.16 мм.

</p><img src="https://habrastorage.org/files/1f5/d46/c29/1f5d46c294e14e6da0c94bb15b87de0e.png"/>
<p>
Затем было сделано 25 тестовых кадров (RGB и глубина) калибровочного объекта в различных положениях. Всего 1200 измерений (25 кадров, 48 ключевых точек на каждом). Ошибка до коррекции составила 7.41±6.32 мм (среднее ± стандартное отклонение), после — 3.12±5.50 мм. На рисунке ниже представлено распределение ошибок до и после коррекции на тестовых кадрах.

</p><img src="https://habrastorage.org/files/798/ed0/1f9/798ed01f9f1b4d61aa7befd869ff59f9.png"/>

<h4><b>Заключение</b></h4><p>
Таким образом, мы устранили геометрические искажения RGB камеры и датчика глубины, научились совмещать кадры и улучшили точность определения глубины. Код этого проекта можно найти </p><a href="https://github.com/tataraidze/KinectV2">тут</a><p>. Надеюсь, он окажется небесполезным.

</p><i>Исследование выполнено за счет гранта Российского научного фонда (проект №15-19-30012) </i>

<h4><b>Список источников</b></h4><p>
1. Kramer J. Hacking the Kinect / Apress. 2012. P. 130</p><p>
2. Lachat E. et al. First Experiences With Kinect V2 Sensor for Close Range 3D Modelling // International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences. 2015. </p><p>
3. Gao X.S. et al. Complete solution classification for the perspective-three-point problem // IEEE Transactions on Pattern Analysis and Machine Intelligence. Vol. 25. N 8. 2003. P. 930-943.
      </p><p class="clear"/>
    </div>

    
  </div></body></html>