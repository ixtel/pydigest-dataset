<html><body><div><div class="clear padding-top-small">
                        <p><img src="/content/images/2015/08/Screen-Shot-2015-08-03-at-6-46-32-PM.jpg" align="right"/></p>

<p>Domino recently added support for GPU instances. To celebrate this release, I will show you how to:</p>

<ul>
<li>Configure the Python library Theano to use the GPU for computation. </li>
<li>Build and train neural networks in Python.</li>
</ul>

<p>Using the GPU, I'll show that we can train deep belief networks up to 15x faster than using just the CPU, cutting training time down from hours to minutes. You can see my code, experiments, and results <a href="https://app.dominodatalab.com/LeJit/GPU_Example">on Domino</a>.</p>

<h1 id="whyaregpususeful">Why are GPUs useful?</h1>

<p>When you think of high-performance graphics cards, data science may not be the first thing that comes to mind. However, computer graphics and data science have one important thing in common: matrices!</p>

<p>Images, videos, and other graphics are represented as matrices, and when you perform a certain operation, such as a camera rotation or a zoom in effect, all you are doing is applying some mathematical transformation to a matrix.</p>

<p><img src="http://www.pling.org.uk/cs/cgvimg/transformations.png" alt="Graphics Matrix transformation"/></p>

<p align="center"><small><em>Source: <a href="http://www.pling.org.uk/cs/cgvimg/transformations.png">http://www.pling.org.uk/cs/cgvimg/transformations.png</a></em></small></p>  

<p>What this means is that GPUs, compared to CPUs (Central Processing Unit), are more specialized at performing matrix operations and other advanced mathematical transformations. In some cases, we see a 10x speedup in an algorithm when it runs on the GPU.</p>

<p><img src="http://http.developer.nvidia.com/GPUGems2/elementLinks/45_finance_04.jpg" alt="GPU vs CPU"/></p>

<p align="center"><small><em>Source: <a href="http://http.developer.nvidia.com/GPUGems2/gpugems2_chapter45.html">http://http.developer.nvidia.com/GPUGems2/gpugems2_chapter45.html</a></em></small></p>  

<p>GPU-based computation have been employed in a wide variety of scientific applications, from genomic to <a href="http://devblogs.nvidia.com/parallelforall/gpus-accelerate-epidemic-forecasting/">epidemiology</a></p>

<p>Recently, there has been a rise in GPU-accelerated algorithms in machine learning thanks to the rising popularity of deep learning algorithms. Deep Learning is a collection of algorithms for training neural network-based models for various problems in machine learning. Deep Learning algorithms involve computationally intensive methods, such as convolutions, Fourier Transforms, and other matrix-based operations which GPUs are well-suited for computing. The computationally intensive functions, which make up about 5% of the code, are run on the GPU, and the remaining code is run on the CPU. <br/>
<img src="http://www.nvidia.com/docs/IO/143716/how-gpu-acceleration-works.png" alt="GPU example"/></p>

<p align="center"><small><em>Source: <a href="http://www.nvidia.com/docs/IO/143716/how-gpu-acceleration-works.png">http://www.nvidia.com/docs/IO/143716/how-gpu-acceleration-works.png</a></em></small></p>

<p>With the recent advances in GPU performance and support for GPUs in common libraries, I recommend anyone interested in deep learning get ahold of a GPU.</p>

<p>Now that I have thoroughly motivated the use of GPUs, let's see how they can be used to train neural networks in Python.</p>

<h1 id="deeplearninginpython">Deep Learning in Python</h1>

<p>The most popular library in Python to implement neural networks is <a href="http://deeplearning.net/software/theano/">Theano</a>. However, Theano is not strictly a neural network library, but rather a Python library that makes it possible to implement a wide variety of mathematical abstractions. Because of this, Theano has a high learning curve, so I will be using two neural network libraries built on top of Theano that have a more gentle learning curve.</p>

<p>The first library is <a href="https://github.com/Lasagne/Lasagne">Lasagne</a>. This library provides a nice abstraction that allows you to construct each layer of the neural network, and then stack the layers on top of each other to construct the full model. While this is nicer than Theano, constructing each layer and then appending them on top of one another becomes tedious, so we'll be using the <a href="https://github.com/dnouri/nolearn">Nolearn library</a> which provides a <a href="http://scikit-learn.org/stable/">Scikit-Learn</a> style API over Lasagne to easily construct neural networks with multiple layers.</p>

<p>Because these libraries do not come default with Domino's hardware, you need to create a <em>requirements.txt</em> with the following text:</p>

<pre><code class="language-prettyprint lang-py">-r https://raw.githubusercontent.com/dnouri/nolearn/master/requirements.txt
git+https://github.com/dnouri/nolearn.git@master#egg=nolearn==0.7.git  
</code></pre>

<h3 id="settinguptheano">Setting up Theano</h3>

<p>Now, before we can import Lasagne and Nolearn, we need to <a href="http://deeplearning.net/software/theano/library/config.html">configure Theano, so that it can utilize the GPU hardware</a>. To do this, we create a <em>.theanorc</em> file in our project directory with the following contents:</p>

<pre><code class="language-prettyprint lang-py ">[global]
device = gpu  
floatX = float32

[nvcc]
fastmath = True  
</code></pre>

<p>The <em>.theanorc</em> file must be placed in the home directory. On your local machine this could be done manually, but we cannot access the home directory of Domino's machine, so we will move the file to the home directory using the <a href="https://app.dominodatalab.com/LeJit/GPU_Example/view/setup_GPU.py">following code</a>:</p>

<pre><code class="language-prettyprint lang-py">import os  
import shutil

destfile = "/home/ubuntu/.theanorc"  
open(destfile, 'a').close()  
shutil.copyfile(".theanorc", destfile)  
</code></pre>

<p>The above code creates an empty <em>.theanorc</em> file in the home directory and then copy the contents of the <em>.theanorc</em> file in our project directory into the file in the home directory.</p>

<p>After changing the hardware tier to GPU, we can test to see if Theano detects the GPU using the <a href="https://app.dominodatalab.com/LeJit/GPU_Example/view/gpu_test.py">test code</a> provided in Theano's documentation.</p>

<pre><code class="language-prettyprint lang-py">import os  
import shutil

destfile = "/home/ubuntu/.theanorc"  
open(destfile, 'a').close()  
shutil.copyfile(".theanorc", destfile)

from theano import function, config, shared, sandbox  
import theano.tensor as T  
import numpy  
import time

vlen = 10 * 30 * 768  # 10 x #cores x # threads per core  
iters = 1000

rng = numpy.random.RandomState(22)  
x = shared(numpy.asarray(rng.rand(vlen), config.floatX))  
f = function([], T.exp(x))  
print f.maker.fgraph.toposort()  
t0 = time.time()  
for i in xrange(iters):  
    r = f()
t1 = time.time()  
print 'Looping %d times took' % iters, t1 - t0, 'seconds'  
print 'Result is', r  
if numpy.any([isinstance(x.op, T.Elemwise) for x in f.maker.fgraph.toposort()]):  
    print 'Used the cpu'
else:  
    print 'Used the gpu'
</code></pre>

<p>If Theano detects the GPU, the above function should take about 0.7 seconds to run and will print 'Used the gpu'. Otherwise, it will take 2.6 seconds to run and print 'Used the cpu'. If it outputs this, then you forgot to change the hardware tier to GPU.</p>

<h3 id="thedataset">The Dataset</h3>

<p>For this project, we'll be using the <a href="http://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10 image dataset</a> containing 60,000 32x32 colored images from 10 different classes. <br/>
<img src="http://i.imgur.com/al4cmJF.png" alt="CIFAR10"/></p>

<p>Fortunately, the data come in a <a href="https://en.wikipedia.org/wiki/Pickle_(Python)">pickled</a> format, so we can load the data using helper functions to load each file into NumPy arrays to produce a training set (Xtr), training labels (Ytr), testing set (Xte), and testing labels (Yte). Credit for the <a href="https://app.dominodatalab.com/LeJit/GPU_Example/view/data_load.py">following code</a> goes to <a href="http://cs231n.stanford.edu/">Stanford's CS231n</a> course staff.</p>

<pre><code class="language-prettyprint lang-py">import cPickle as pickle  
import numpy as np  
import os

def load_CIFAR_file(filename):  
    '''Load a single file of CIFAR'''
    with open(filename, 'rb') as f:
        datadict= pickle.load(f)
        X = datadict['data']
        Y = datadict['labels']
        X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype('float32')
        Y = np.array(Y).astype('int32')
        return X, Y


def load_CIFAR10(directory):  
    '''Load all of CIFAR'''
    xs = []
    ys = []
    for k in range(1,6):
        f = os.path.join(directory, "data_batch_%d" % k)
        X, Y = load_CIFAR_file(f)
        xs.append(X)
        ys.append(Y)
    Xtr = np.concatenate(xs)
    Ytr = np.concatenate(ys)
    Xte, Yte = load_CIFAR_file(os.path.join(directory, 'test_batch'))
    return Xtr, Ytr, Xte, Yte
</code></pre>

<h3 id="multilayeredperceptron">Multi-Layered Perceptron</h3>

<p>A multi-layered perceptron is one of the most simple neural network models. The model consists of an input layer for the data, a hidden layer to apply some mathematical transformation, and an output layer to produce a label (either categorical for classification or continuous for regression).</p>

<p><img src="http://dms.irb.hr/tutorial/images/ann.jpg" alt="MLP"/></p>

<p align="center"><small><em>Source: <a href="http://dms.irb.hr/tutorial/tut_nnets_short.php">http://dms.irb.hr/tutorial/tut_nnets_short.php</a> </em></small></p>

<p>Before we can use the training data, we need to grayscale it and flatten it into a two-dimensional matrix. In addition, we will divide each value by 255 and subtract 0.5. When we grayscale the image, we convert each (R,G,B) tuple in a float value between 0 and 255. By dividing by 255, we normalize the grayscale value to the interval [0,1]. Next, we subtract 0.5 to map the values to the interval [-0.5, 0.5]. Now, each image is represented by a 1024-dimensional array where each value is between -0.5 and 0.5. It's common practice to standardize your input features to the interval [-1, 1] when training classification networks. </p>

<pre><code class="language-prettyprint lang-py">X_train_flat = np.dot(X_train[...,:3], [0.299, 0.587, 0.114]).reshape(X_train.shape[0],-1).astype(np.float32)  
X_train_flat = (X_train_flat/255.0)-0.5  
X_test_flat = np.dot(X_test[...,:3], [0.299, 0.587, 0.114]).reshape(X_test.shape[0],-1).astype(np.float32)  
X_test_flat = (X_test_flat/255.0)-.5  
</code></pre>

<p>Using Nolearn's API, we can <a href="https://app.dominodatalab.com/LeJit/GPU_Example/view/MultiLayerPerceptron.py">easily create a multi-layered perceptron</a> with an input, hidden, and output layer. The <em>hidden_num_units = 100</em> means our hidden layer has 100 neurons, and the <em>output_num_units = 10</em> means our output layer has 10 neurons, one for each of the label. Before outputting, the network applies a <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax function</a> to determine the most probable label. If  The network is trained for 50 epochs and with <em>verbose = 1</em>, the model prints out the result of each training epoch and how long the epoch took.</p>

<pre><code class="language-prettyprint lang-py">net1 = NeuralNet(  
    layers = [
        ('input', layers.InputLayer),
        ('hidden', layers.DenseLayer),
        ('output', layers.DenseLayer),
        ],
        #layers parameters:
        input_shape = (None, 1024),
        hidden_num_units = 100,
        output_nonlinearity = softmax,
        output_num_units = 10,

        #optimization parameters:
        update = nesterov_momentum,
        update_learning_rate = 0.01,
        update_momentum = 0.9,

        regression = False,
        max_epochs = 50,
        verbose = 1,
        )
</code></pre>

<p>As a side remark, this API makes it easy to build deep networks. If we wanted to add a second hidden layer, all we would have to do is add it to the <em>layers</em> parameter and specify how many units in the new layer.</p>

<pre><code class="language-prettyprint lang-py">net1 = NeuralNet(  
    layers = [
        ('input', layers.InputLayer),
        ('hidden1', layers.DenseLayer),
        ('hidden2', layers.DenseLayer), #Added Layer Here
        ('output', layers.DenseLayer),
        ],
        #layers parameters:
        input_shape = (None, 1024),
        hidden1_num_units = 100,
        hidden2_num_units = 100, #Added Layer Params Here
</code></pre>

<p>Now, as I mentioned earlier about Nolearn's Scikit-Learn style API, we can fit the Neural Network with the <em>fit</em> method.</p>

<pre><code class="language-prettyprint lang-py">net1.fit(X_train_flat, y_train)  
</code></pre>

<p>When the network is trained on the GPU hardware, we see each training epoch usually takes 0.5 seconds.</p>

<p><img src="http://i.imgur.com/rgc9LLW.png" alt="MLP_GPU"/></p>

<p>On the other hand, when Domino's hardware is set to XX-Large (32 core, 60 GB RAM), each training epoch takes usually takes 1.3 seconds.</p>

<p><img src="http://i.imgur.com/Vmi2ykp.png" alt="MLP_CPU"/></p>

<p>By training this network on the GPU, we see roughly a 3x speedup in training the network. As expected, both the GPU-trained network and the CPU-trained network yield similar results. Both yield about a similar validation accuracy of 41% and similar training loss. </p>

<p>We can test the network on the testing data:</p>

<pre><code class="language-prettyprint lang-py">y_pred1 = net1.predict(X_test_flat)  
print "The accuracy of this network is: %0.2f" % (y_pred1 == y_test).mean()  
</code></pre>

<p>And we achieve an accuracy of 41% on the testing data.</p>

<h3 id="convolutionalnetworks">Convolutional Networks</h3>

<p>A convolutional neural network is a more complex neural network architecture in which neurons in a layer are connected to a subset of neurons from the previous layer. As a result, convolutions are used to pool the outputs from each subset. <br/>
<img src="http://colah.github.io/posts/2014-07-Conv-Nets-Modular/img/Conv2-9x5-Conv2Conv2.png" alt="Convolutional NN"/></p>

<p align="center"><small><em>Source: <a href="http://colah.github.io/posts/2014-07-Conv-Nets-Modular/">http://colah.github.io/posts/2014-07-Conv-Nets-Modular/</a></em></small></p>

<p>Convolutional Neural Networks are popular in industry and <a href="http://jeffreydf.github.io/diabetic-retinopathy-detection/">Kaggle Competitions</a> due to their flexibility to learn different problems and scalability.</p>

<p>Once again, before we can build our convolutional neural network, we have to first grayscale and transform the data. This time we will keep the images in their 32x32 shape. I have modified the order of the rows of the matrix, so each image is now represented as (color, x, y). Once again, I divide the features by 255, and subtract by 0.5 to map the features within the interval (-1, 1). </p>

<pre><code class="language-prettyprint lang-py">X_train_2d = np.dot(X_train[...,:3], [0.299, 0.587, 0.114]).reshape(-1,1,32,32).astype(np.float32)  
X_train_2d = (X_train_2d/255.0)-0.5  
X_test_2d = np.dot(X_test[...,:3], [0.299, 0.587, 0.114]).reshape(-1,1,32,32).astype(np.float32)  
X_train_2d = (X_train_2d/255.0)-0.5  
</code></pre>

<p>Now we can construct the Convolutional Neural Network. The Network consists of the input layer, 3 convolution layers, 3 2x2 pooling layers, a 200-neuron hidden layer, and finally the output layer. </p>

<pre><code class="language-prettyprint lang-py">net2 = NeuralNet(  
    layers = [
        ('input', layers.InputLayer),
        ('conv1', layers.Conv2DLayer),
        ('pool1', layers.MaxPool2DLayer),
        ('conv2', layers.Conv2DLayer),
        ('pool2', layers.MaxPool2DLayer),
        ('conv3', layers.Conv2DLayer),
        ('pool3', layers.MaxPool2DLayer),
        ("hidden4", layers.DenseLayer),
        ("output", layers.DenseLayer),
        ],
        #layer parameters:
        input_shape = (None, 1, 32, 32),
        conv1_num_filters = 16, conv1_filter_size = (3, 3), pool1_pool_size = (2,2),
        conv2_num_filters = 32, conv2_filter_size = (2, 2) , pool2_pool_size =  (2,2),
        conv3_num_filters = 64, conv3_filter_size = (2, 2), pool3_pool_size = (2,2),
        hidden4_num_units = 200,
        output_nonlinearity = softmax,
        output_num_units = 10,

        #optimization parameters:
        update = nesterov_momentum,
        update_learning_rate = 0.015,
        update_momentum = 0.9,
        regression = False,
        max_epochs = 5,
        verbose = 1,
        )
</code></pre>

<p>Once again, we can fit the model using the <em>fit</em> method.</p>

<pre><code class="language-prettyprint lang-py">net2.fit(X_train_2d, y_train)  
</code></pre>

<p>Compared to the Multi-Layer Perceptron, the Convolutional Neural Network will take longer to train. With the GPU enabled, most training epochs take 12.8 seconds to complete. However, the Convolutional Neural Network achieves a validation loss of about 63%, beating the Multi-Layered Perceptron's validation loss of 40%. So by incorporating the convolution and pooling layers, we can improve our accuracy by 20%.</p>

<p><img src="http://i.imgur.com/qV5dOO9.png" alt="CNN_GPU"/></p>

<p>With only the CPU on Domino's XX-Large Hardware tier, each training epoch takes about 177 seconds, or close to 3 minutes, to complete. Thus, by training using the GPU, we see about a 15x speedup in the training time.</p>

<p><img src="http://i.imgur.com/iF3vOi7.png" alt="CNN_CPU"/></p>

<p>Once again, we see similar results for the Convolutional Neural Network trained on the CPU compared to the Convolutional Neural Network trained on the GPU, with similar validation accuracies and training losses.</p>

<p>When we test the convolutional neural network on the testing data, we achieve an accuracy of 61%.  </p>

<pre><code class="language-prettyprint lang-py">y_pred2 = net2.predict(X_test_2d)  
print "The accuracy of this network is: %0.2f" % (y_pred2 == y_test).mean()  
</code></pre>

<p>All the code for building the convolutional neural network can be found in <a href="https://app.dominodatalab.com/LeJit/GPU_Example/view/ConvolutionNN.py">this file</a>.</p>

<p>As you can see, using the GPU to train deep neural networks results in a speed up in runtime, ranging from 3x faster to 15x faster in this project. In industry and academia, we often use multiple GPUs as this cuts the training runtime of deep networks down from weeks to days.  </p>

<h1 id="additionalresources">Additional Resources</h1>

<p>As always, the <a href="https://app.dominodatalab.com/LeJit/GPU_Example/browse?">code for this project is publicly available on Domino</a>.</p>

<p><a href="http://www.nvidia.com/page/home.html">NVIDIA</a> has recently launched a <a href="https://developer.nvidia.com/deep-learning-courses">free online course on GPUs and deep learning</a>, so those of who are interested in learning more about GPU-accelerated deep learning can check this course out!</p>

<p>Additionally, <a href="https://www.udacity.com/">Udacity</a> has a free online course in <a href="https://www.udacity.com/course/intro-to-parallel-programming--cs344">CUDA Programming and Parallel Programming</a> for those who are interested in general GPU and CUDA programming.</p>

<p>If you are more interested in convolutional neural networks, the <em>Neural Networks and Deep Learning</em> ebook recently released a <a href="http://neuralnetworksanddeeplearning.com/chap6.html">a chapter on convolutional neural network</a>.</p>

<p>I would like to extend a special acknowledgement to Reddit user sdsfs23fs for their criticism and correcting incorrect statements I had in my original version of this blog post. In addition, this Reddit user provided me with code for the image pre-processing step that greatly improves the accuracy of the neural networks.</p>

<p><em>Edit</em>: I have updated the requirements.txt file for this project to overcome an error caused by one of the Python libraries. The new requirements.txt is listed above in the <em>Deep Learning in Python</em> section.</p>
                      </div>

                </div></body></html>